{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Federated PyTorch TinyImageNet Tutorial"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This notebook is an example of Transfer Learning \n",
    "\n",
    "Custom DataLoader is used with OpenFL Python API"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Install dependencies if not already installed\n",
    "!pip install torch torchvision\n",
    "%load_ext autoreload\n",
    "%autoreload 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import glob\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "from PIL import Image\n",
    "\n",
    "import numpy as np\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import torch.optim as optim\n",
    "\n",
    "import torchvision\n",
    "from torchvision import transforms as T\n",
    "\n",
    "import openfl.native as fx\n",
    "from openfl.federated import FederatedModel, FederatedDataSet\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "After importing the required packages, the next step is setting up our openfl workspace. To do this, simply run the `fx.init()` command as follows:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Setup default workspace, logging, etc.\n",
    "fx.init('torch_cnn_mnist')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now we are ready to define our dataset and model to perform federated learning on. The dataset should be composed of a numpy arrayWe start with a simple fully connected model that is trained on the MNIST dataset. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Download the data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!wget --no-clobber http://cs231n.stanford.edu/tiny-imagenet-200.zip\n",
    "!unzip -n tiny-imagenet-200.zip\n",
    "TINY_IMAGENET_ROOT = './tiny-imagenet-200/'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Describe the dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class TinyImageNet(Dataset):\n",
    "    \"\"\"\n",
    "    Contains 200 classes for training. Each class has 500 images. \n",
    "    Parameters\n",
    "    ----------\n",
    "    root: string\n",
    "        Root directory including `train` and `val` subdirectories.\n",
    "    split: string\n",
    "        Indicating which split to return as a data set.\n",
    "        Valid option: [`train`, `val`]\n",
    "    transform: torchvision.transforms\n",
    "        A (series) of valid transformation(s).\n",
    "    collabs: int\n",
    "        How many dataset shards will be needed, minimum 1\n",
    "    shard_num: int\n",
    "        Current shard number, starting from 0\n",
    "    \"\"\"\n",
    "    def __init__(self, root, split='train', collabs=1, shard_num=0, transform=None, target_transform=None):\n",
    "        assert collabs > shard_num, \"Incorrect shard number\"\n",
    "        NUM_IMAGES_PER_CLASS = 500\n",
    "        self.root = os.path.expanduser(root)\n",
    "        self.transform = transform\n",
    "        self.target_transform = target_transform\n",
    "        self.split_dir = os.path.join(self.root, split)\n",
    "        self.image_paths = sorted(glob.iglob(os.path.join(self.split_dir, '**', '*.JPEG'), recursive=True))\n",
    "        # DO the SHARDING\n",
    "        if split == 'train':\n",
    "            self.image_paths = self.image_paths[shard_num::collabs]\n",
    "        \n",
    "        self.labels = {}  # fname - label number mapping\n",
    "\n",
    "        # build class label - number mapping\n",
    "        with open(os.path.join(self.root, 'wnids.txt'), 'r') as fp:\n",
    "            self.label_texts = sorted([text.strip() for text in fp.readlines()])\n",
    "        self.label_text_to_number = {text: i for i, text in enumerate(self.label_texts)}\n",
    "\n",
    "        if split == 'train':\n",
    "            for label_text, i in self.label_text_to_number.items():\n",
    "                for cnt in range(NUM_IMAGES_PER_CLASS):\n",
    "                    self.labels[f'{label_text}_{cnt}.JPEG'] = i\n",
    "        elif split == 'val':\n",
    "            with open(os.path.join(self.split_dir, 'val_annotations.txt'), 'r') as fp:\n",
    "                for line in fp.readlines():\n",
    "                    terms = line.split('\\t')\n",
    "                    file_name, label_text = terms[0], terms[1]\n",
    "                    self.labels[file_name] = self.label_text_to_number[label_text]\n",
    "                    \n",
    "    \n",
    "    def __len__(self):\n",
    "        return len(self.image_paths)\n",
    "\n",
    "    def __getitem__(self, index):\n",
    "        file_path = self.image_paths[index]\n",
    "        label = self.labels[os.path.basename(file_path)]\n",
    "        label = self.target_transform(label) if self.target_transform else label\n",
    "        return self.read_image(file_path), label\n",
    "\n",
    "    def read_image(self, path):\n",
    "        img = Image.open(path)\n",
    "        return self.transform(img) if self.transform else img\n",
    "\n",
    "def one_hot(labels, classes):\n",
    "    return np.eye(classes)[labels]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "normalize = T.Normalize(mean=[0.485, 0.456, 0.406],\n",
    "                                 std=[0.229, 0.224, 0.225])\n",
    "\n",
    "augmentation = T.RandomApply([\n",
    "    T.RandomHorizontalFlip(),\n",
    "    T.RandomRotation(10),\n",
    "    T.RandomResizedCrop(64)], p=.8)\n",
    "\n",
    "training_transform = T.Compose([\n",
    "    T.Lambda(lambda x: x.convert(\"RGB\")),\n",
    "    augmentation,\n",
    "    T.ToTensor(),\n",
    "    normalize])\n",
    "\n",
    "valid_transform = T.Compose([\n",
    "    T.Lambda(lambda x: x.convert(\"RGB\")),\n",
    "    T.ToTensor(),\n",
    "    normalize])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Implement Federated dataset\n",
    "We have to implement `split` method"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class TinyImagenetDataloader(FederatedDataSet):\n",
    "    def __init__(self,collaborator_count, rank, batch_size, **kwargs):\n",
    "        \"\"\"Instantiate the data object\n",
    "        Args:\n",
    "            data_path: The file path to the data\n",
    "            batch_size: The batch size of the data loader\n",
    "            **kwargs: Additional arguments, passed to super init and load_mnist_shard\n",
    "        \"\"\"\n",
    "        super().__init__([],[],[],[],batch_size, **kwargs)\n",
    "    \n",
    "        self.fed_size = int(collaborator_count)\n",
    "        self.rank = int(rank)\n",
    "        \n",
    "        self.batch_size = batch_size\n",
    "        self.shuffle = kwargs.setdefault('shuffle', True)\n",
    "\n",
    "        self.training_set = TinyImageNet(TINY_IMAGENET_ROOT, 'train', transform=training_transform, \\\n",
    "                collabs=self.fed_size, shard_num=self.rank)\n",
    "        self.valid_set = TinyImageNet(TINY_IMAGENET_ROOT, 'val', transform=valid_transform, \\\n",
    "                                      target_transform=lambda target: one_hot(target, 200))\n",
    "\n",
    "        self.train_loader = self.get_train_loader()\n",
    "        \n",
    "        self.val_loader = self.get_valid_loader()\n",
    "\n",
    "        self.num_classes = 200\n",
    "\n",
    "\n",
    "    def get_valid_loader(self, num_batches=None):\n",
    "        return DataLoader(self.valid_set, batch_size=self.batch_size*2, num_workers=6)\n",
    "\n",
    "    def get_train_loader(self, num_batches=None):\n",
    "        return DataLoader(self.training_set, batch_size=self.batch_size, shuffle=self.shuffle, num_workers=4)\n",
    "\n",
    "    def get_train_data_size(self):\n",
    "        return len(self.training_set)\n",
    "\n",
    "    def get_valid_data_size(self):\n",
    "        return len(self.valid_set)\n",
    "\n",
    "    def get_feature_shape(self):\n",
    "        return self.valid_set[0][0].shape\n",
    "    \n",
    "    def split(self, num_collaborators, shuffle=True, equally=True):\n",
    "        return [TinyImagenetDataloader(num_collaborators, collab_rank, self.batch_size, shuffle=shuffle) \\\n",
    "        for collab_rank in range(num_collaborators)]\n",
    "        \n",
    "fl_data = TinyImagenetDataloader(collaborator_count=1, rank=0, batch_size=32)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Define model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Net(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(Net, self).__init__()\n",
    "        self.model = torchvision.models.mobilenet_v2(pretrained=True)\n",
    "        self.model.requires_grad_(False)\n",
    "        self.model.classifier[1] = torch.nn.Linear(in_features=1280, \\\n",
    "                        out_features=fl_data.num_classes, bias=True)\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.model.forward(x)\n",
    "        return x\n",
    "\n",
    "    \n",
    "optimizer = lambda x: optim.Adam(x, lr=1e-4)\n",
    "\n",
    "def cross_entropy(output, target):\n",
    "    \"\"\"Binary cross-entropy metric\n",
    "    \"\"\"\n",
    "    return F.cross_entropy(input=output,target=target)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Create a federated model using the pytorch class, lambda optimizer function, and loss function\n",
    "fl_model = FederatedModel(build_model=Net,optimizer=optimizer,loss_fn=cross_entropy, \\\n",
    "                        data_loader=fl_data)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The `FederatedModel` object is a wrapper around your Keras, Tensorflow or PyTorch model that makes it compatible with openfl. It provides built in federated training and validation functions that we will see used below. Using it's `setup` function, collaborator models and datasets can be automatically defined for the experiment. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "collaborator_models = fl_model.setup(num_collaborators=2)\n",
    "collaborators = {'one':collaborator_models[0],'two':collaborator_models[1]}#, 'three':collaborator_models[2]}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Original TinyImageNet dataset\n",
    "print(f'Original training data size: {len(fl_data.training_set)}')\n",
    "print(f'Original validation data size: {len(fl_data.valid_set)}\\n')\n",
    "\n",
    "#Collaborator one's data\n",
    "print(f'Collaborator one\\'s training data size: {len(collaborator_models[0].data_loader.training_set)}')\n",
    "print(f'Collaborator one\\'s validation data size: {len(collaborator_models[0].data_loader.valid_set)}\\n')\n",
    "\n",
    "#Collaborator two's data\n",
    "print(f'Collaborator two\\'s training data size: {len(collaborator_models[1].data_loader.training_set)}')\n",
    "print(f'Collaborator two\\'s validation data size: {len(collaborator_models[1].data_loader.valid_set)}\\n')\n",
    "\n",
    "#Collaborator three's data\n",
    "#print(f'Collaborator three\\'s training data size: {len(collaborator_models[2].data_loader.X_train)}')\n",
    "#print(f'Collaborator three\\'s validation data size: {len(collaborator_models[2].data_loader.X_valid)}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "#Run experiment, return trained FederatedModel\n",
    "final_fl_model = fx.run_experiment(collaborators,{'aggregator.settings.rounds_to_train':10})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Save final model\n",
    "final_fl_model.save_native('final_model.pth')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}

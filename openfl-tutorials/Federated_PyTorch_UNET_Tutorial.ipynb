{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Federated PyTorch UNET Tutorial"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Install dependencies if not already installed\n",
    "!pip install torch"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "First of all we need to set up our OpenFL workspace. To do this, simply run the `fx.init()` command as follows:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import openfl.native as fx\n",
    "\n",
    "# Setup default workspace, logging, etc. Install additional requirements\n",
    "fx.init('torch_unet_kvasir')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import installed modules\n",
    "import PIL\n",
    "import json\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import torch.optim as optim\n",
    "import numpy as np\n",
    "from skimage import io\n",
    "from torchvision import transforms as tsf\n",
    "import matplotlib.pyplot as plt\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "\n",
    "from os import listdir\n",
    "\n",
    "from openfl.federated import FederatedModel, FederatedDataSet\n",
    "from openfl.utilities import TensorKey\n",
    "from openfl.utilities import validate_file_hash"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Download Kvasir dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!wget 'https://datasets.simula.no/downloads/hyper-kvasir/hyper-kvasir-segmented-images.zip' -O kvasir.zip\n",
    "ZIP_SHA384 = ('66cd659d0e8afd8c83408174'\n",
    "            '1ade2b75dada8d4648b816f2533c8748b1658efa3d49e205415d4116faade2c5810e241e')\n",
    "validate_file_hash('./kvasir.zip', ZIP_SHA384)\n",
    "!unzip -n kvasir.zip -d ./data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now we are ready to define our dataset and model to perform federated learning on."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "DATA_PATH = './data/segmented-images/'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def read_data(image_path, mask_path):\n",
    "    \"\"\"\n",
    "    Read image and mask from disk.\n",
    "    \"\"\"\n",
    "    img = io.imread(image_path)\n",
    "    assert(img.shape[2] == 3)\n",
    "    mask = io.imread(mask_path)\n",
    "    return (img, mask[:, :, 0].astype(np.uint8))\n",
    "\n",
    "\n",
    "class KvasirDataset(Dataset):\n",
    "    \"\"\"\n",
    "    Kvasir dataset contains 1000 images for all collaborators.\n",
    "    Args:\n",
    "        data_path: path to dataset on disk\n",
    "        collaborator_count: total number of collaborators\n",
    "        collaborator_num: number of current collaborator\n",
    "        is_validation: validation option\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self, data_path, collaborator_count, collaborator_num, is_validation):\n",
    "        self.images_path = './data/segmented-images/images/'\n",
    "        self.masks_path = './data/segmented-images/masks/'\n",
    "        self.images_names = [\n",
    "            img_name\n",
    "            for img_name in sorted(listdir(self.images_path))\n",
    "            if len(img_name) > 3 and img_name[-3:] == 'jpg'\n",
    "        ]\n",
    "\n",
    "        self.images_names = self.images_names[collaborator_num:: collaborator_count]\n",
    "        self.is_validation = is_validation\n",
    "        assert(len(self.images_names) > 8)\n",
    "        validation_size = len(self.images_names) // 8\n",
    "        if is_validation:\n",
    "            self.images_names = self.images_names[-validation_size:]\n",
    "        else:\n",
    "            self.images_names = self.images_names[: -validation_size]\n",
    "\n",
    "        self.img_trans = tsf.Compose([\n",
    "            tsf.ToPILImage(),\n",
    "            tsf.Resize((332, 332)),\n",
    "            tsf.ToTensor(),\n",
    "            tsf.Normalize(mean=[0.5, 0.5, 0.5], std=[0.5, 0.5, 0.5])])\n",
    "        self.mask_trans = tsf.Compose([\n",
    "            tsf.ToPILImage(),\n",
    "            tsf.Resize((332, 332), interpolation=PIL.Image.NEAREST),\n",
    "            tsf.ToTensor()])\n",
    "\n",
    "    def __getitem__(self, index):\n",
    "        name = self.images_names[index]\n",
    "        img, mask = read_data(self.images_path + name, self.masks_path + name)\n",
    "        img = self.img_trans(img).numpy()\n",
    "        mask = self.mask_trans(mask).numpy()\n",
    "        return img, mask\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.images_names)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Here we redefine `FederatedDataSet` methods, if we don't want to use default batch generator from `FederatedDataSet`. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class KvasirFederatedDataset(FederatedDataSet):\n",
    "    def __init__(self, collaborator_count=1, collaborator_num=0, batch_size=1, **kwargs):\n",
    "        \"\"\"Instantiate the data object\n",
    "        Args:\n",
    "            collaborator_count: total number of collaborators\n",
    "            collaborator_num: number of current collaborator\n",
    "            batch_size:  the batch size of the data loader\n",
    "            **kwargs: additional arguments, passed to super init\n",
    "        \"\"\"\n",
    "        super().__init__([], [], [], [], batch_size, num_classes=2, **kwargs)\n",
    "\n",
    "        self.collaborator_num = int(collaborator_num)\n",
    "\n",
    "        self.batch_size = batch_size\n",
    "\n",
    "        self.training_set = KvasirDataset(\n",
    "            DATA_PATH, collaborator_count, collaborator_num, is_validation=False\n",
    "        )\n",
    "        self.valid_set = KvasirDataset(\n",
    "            DATA_PATH, collaborator_count, collaborator_num, is_validation=True\n",
    "        )\n",
    "\n",
    "        self.train_loader = self.get_train_loader()\n",
    "        self.val_loader = self.get_valid_loader()\n",
    "\n",
    "    def get_valid_loader(self, num_batches=None):\n",
    "        return DataLoader(self.valid_set, num_workers=8, batch_size=self.batch_size)\n",
    "\n",
    "    def get_train_loader(self, num_batches=None):\n",
    "        return DataLoader(\n",
    "            self.training_set, num_workers=8, batch_size=self.batch_size, shuffle=True\n",
    "        )\n",
    "\n",
    "    def get_train_data_size(self):\n",
    "        return len(self.training_set)\n",
    "\n",
    "    def get_valid_data_size(self):\n",
    "        return len(self.valid_set)\n",
    "\n",
    "    def get_feature_shape(self):\n",
    "        return self.valid_set[0][0].shape\n",
    "\n",
    "    def split(self, collaborator_count, shuffle=True, equally=True):\n",
    "        return [\n",
    "            KvasirFederatedDataset(collaborator_count,\n",
    "                           collaborator_num, self.batch_size)\n",
    "            for collaborator_num in range(collaborator_count)\n",
    "        ]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Our Unet model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def soft_dice_loss(output, target):\n",
    "    num = target.size(0)\n",
    "    m1 = output.view(num, -1)\n",
    "    m2 = target.view(num, -1)\n",
    "    intersection = m1 * m2\n",
    "    score = 2.0 * (intersection.sum(1) + 1) / (m1.sum(1) + m2.sum(1) + 1)\n",
    "    score = 1 - score.sum() / num\n",
    "    return score\n",
    "\n",
    "\n",
    "def soft_dice_coef(output, target):\n",
    "    num = target.size(0)\n",
    "    m1 = output.view(num, -1)\n",
    "    m2 = target.view(num, -1)\n",
    "    intersection = m1 * m2\n",
    "    score = 2.0 * (intersection.sum(1) + 1) / (m1.sum(1) + m2.sum(1) + 1)\n",
    "    return score.sum()\n",
    "\n",
    "\n",
    "class DoubleConv(nn.Module):\n",
    "    def __init__(self, in_ch, out_ch):\n",
    "        super(DoubleConv, self).__init__()\n",
    "        self.in_ch = in_ch\n",
    "        self.out_ch = out_ch\n",
    "        self.conv = nn.Sequential(\n",
    "            nn.Conv2d(in_ch, out_ch, 3, padding=1),\n",
    "            nn.BatchNorm2d(out_ch),\n",
    "            nn.ReLU(inplace=True),\n",
    "            nn.Conv2d(out_ch, out_ch, 3, padding=1),\n",
    "            nn.BatchNorm2d(out_ch),\n",
    "            nn.ReLU(inplace=True),\n",
    "        )\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.conv(x)\n",
    "        return x\n",
    "\n",
    "\n",
    "class Down(nn.Module):\n",
    "    def __init__(self, in_ch, out_ch):\n",
    "        super(Down, self).__init__()\n",
    "        self.mpconv = nn.Sequential(\n",
    "            nn.MaxPool2d(2),\n",
    "            DoubleConv(in_ch, out_ch)\n",
    "        )\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.mpconv(x)\n",
    "        return x\n",
    "\n",
    "\n",
    "class Up(nn.Module):\n",
    "    def __init__(self, in_ch, out_ch, bilinear=False):\n",
    "        super(Up, self).__init__()\n",
    "        self.in_ch = in_ch\n",
    "        self.out_ch = out_ch\n",
    "        if bilinear:\n",
    "            self.Up = nn.Upsample(\n",
    "                scale_factor=2,\n",
    "                mode=\"bilinear\",\n",
    "                align_corners=True\n",
    "            )\n",
    "        else:\n",
    "            self.Up = nn.ConvTranspose2d(in_ch, in_ch // 2, 2, stride=2)\n",
    "        self.conv = DoubleConv(in_ch, out_ch)\n",
    "\n",
    "    def forward(self, x1, x2):\n",
    "        x1 = self.Up(x1)\n",
    "        diffY = x2.size()[2] - x1.size()[2]\n",
    "        diffX = x2.size()[3] - x1.size()[3]\n",
    "\n",
    "        x1 = F.pad(x1, (diffX // 2, diffX - diffX //\n",
    "                        2, diffY // 2, diffY - diffY // 2))\n",
    "\n",
    "        x = torch.cat([x2, x1], dim=1)\n",
    "        x = self.conv(x)\n",
    "        return x\n",
    "\n",
    "\n",
    "class UNet(nn.Module):\n",
    "    def __init__(self, n_channels=3, n_classes=1):\n",
    "        super().__init__()\n",
    "        self.inc = DoubleConv(n_channels, 64)\n",
    "        self.down1 = Down(64, 128)\n",
    "        self.down2 = Down(128, 256)\n",
    "        self.down3 = Down(256, 512)\n",
    "        self.down4 = Down(512, 1024)\n",
    "        self.up1 = Up(1024, 512)\n",
    "        self.up2 = Up(512, 256)\n",
    "        self.up3 = Up(256, 128)\n",
    "        self.up4 = Up(128, 64)\n",
    "        self.outc = nn.Conv2d(64, n_classes, 1)\n",
    "\n",
    "    def forward(self, x):\n",
    "        x1 = self.inc(x)\n",
    "        x2 = self.down1(x1)\n",
    "        x3 = self.down2(x2)\n",
    "        x4 = self.down3(x3)\n",
    "        x5 = self.down4(x4)\n",
    "        x = self.up1(x5, x4)\n",
    "        x = self.up2(x, x3)\n",
    "        x = self.up3(x, x2)\n",
    "        x = self.up4(x, x1)\n",
    "        x = self.outc(x)\n",
    "        x = torch.sigmoid(x)\n",
    "        return x\n",
    "\n",
    "    def validate(\n",
    "        self, col_name, round_num, input_tensor_dict, use_tqdm=False, **kwargs\n",
    "    ):\n",
    "        \"\"\" Validate. Redifine function from PyTorchTaskRunner, to use our validation\"\"\"\n",
    "        self.rebuild_model(round_num, input_tensor_dict, validation=True)\n",
    "        self.eval()\n",
    "        self.to(self.device)\n",
    "        val_score = 0\n",
    "        total_samples = 0\n",
    "\n",
    "        loader = self.data_loader.get_valid_loader()\n",
    "        if use_tqdm:\n",
    "            loader = tqdm.tqdm(loader, desc=\"validate\")\n",
    "\n",
    "        with torch.no_grad():\n",
    "            for data, target in loader:\n",
    "                samples = target.shape[0]\n",
    "                total_samples += samples\n",
    "                data, target = (\n",
    "                    torch.tensor(data).to(self.device),\n",
    "                    torch.tensor(target).to(self.device),\n",
    "                )\n",
    "                output = self(data)\n",
    "                # get the index of the max log-probability\n",
    "                val = soft_dice_coef(output, target)\n",
    "                val_score += val.sum().cpu().numpy()\n",
    "\n",
    "        origin = col_name\n",
    "        suffix = \"validate\"\n",
    "        if kwargs[\"apply\"] == \"local\":\n",
    "            suffix += \"_local\"\n",
    "        else:\n",
    "            suffix += \"_agg\"\n",
    "        tags = (\"metric\", suffix)\n",
    "        output_tensor_dict = {\n",
    "            TensorKey(\"dice_coef\", origin, round_num, True, tags): np.array(\n",
    "                val_score / total_samples\n",
    "            )\n",
    "        }\n",
    "        return output_tensor_dict, {}\n",
    "\n",
    "\n",
    "def optimizer(x): return optim.Adam(x, lr=1e-3)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Create `KvasirFederatedDataset`, federated datasets for collaborators will be created in `split()` method of this object"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fl_data = KvasirFederatedDataset(batch_size=6)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The `FederatedModel` object is a wrapper around your Keras, Tensorflow or PyTorch model that makes it compatible with OpenFL. It provides built-in federated training function which will be used while training. Using its `setup` function, collaborator models and datasets can be automatically obtained for the experiment. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create a federated model using the pytorch class, optimizer function, and loss function\n",
    "fl_model = FederatedModel(build_model=UNet, optimizer=optimizer,\n",
    "                          loss_fn=soft_dice_loss, data_loader=fl_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "collaborator_models = fl_model.setup(num_collaborators=2)\n",
    "collaborators = {'one': collaborator_models[0], 'two': collaborator_models[1]}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can see the current FL plan values by running the `fx.get_plan()` function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get the current values of the FL plan. Each of these can be overridden\n",
    "print(fx.get_plan())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now we are ready to run our experiment. If we want to pass in custom FL plan settings, we can easily do that with the `override_config` parameter"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Run experiment, return trained FederatedModel\n",
    "final_fl_model = fx.run_experiment(\n",
    "    collaborators, override_config={'aggregator.settings.rounds_to_train': 30})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Save final model\n",
    "final_fl_model.save_native('final_pytorch_model')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's visually evaluate the results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "collaborator = collaborator_models[0]\n",
    "loader = collaborator.runner.data_loader.get_valid_loader()\n",
    "model = final_fl_model.model\n",
    "model.eval()\n",
    "device = final_fl_model.runner.device\n",
    "model.to(device)\n",
    "with torch.no_grad():\n",
    "    for batch, _ in zip(loader, range(5)):\n",
    "        preds = model(batch[0].to(device))\n",
    "        for image, pred, target in zip(batch[0], preds, batch[1]):\n",
    "            plt.figure(figsize=(10, 10))\n",
    "            plt.subplot(131)\n",
    "            plt.imshow(image.permute(1, 2, 0).data.cpu().numpy() * 0.5 + 0.5)\n",
    "            plt.title(\"img\")\n",
    "            plt.subplot(132)\n",
    "            plt.imshow(pred[0].data.cpu().numpy())\n",
    "            plt.title(\"pred\")\n",
    "            plt.subplot(133)\n",
    "            plt.imshow(target[0].data.cpu().numpy())\n",
    "            plt.title(\"targ\")\n",
    "            plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.8.10 64-bit",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "name": "python",
   "version": "3.8.10"
  },
  "vscode": {
   "interpreter": {
    "hash": "916dbcbb3f70747c44a77c7bcd40155683ae19c65e1c03b4aa3499c5328201f1"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}

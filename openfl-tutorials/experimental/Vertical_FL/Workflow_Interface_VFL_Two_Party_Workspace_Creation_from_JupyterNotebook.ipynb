{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "0d7f65ec-d3d8-4c91-99a4-277c160cb33b",
   "metadata": {},
   "source": [
    "# Workflow Interface VFL Two Party: Workspace Creation from Jupyter Notebook"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d83a6c7f-4816-472a-9a46-ed0a2ddec8ef",
   "metadata": {},
   "source": [
    "This tutorial demonstrates the methodology to convert a Federated Learning experiment developed in Jupyter Notebook into a Workspace that can be deployed using Aggregator Based Workflow\n",
    "\n",
    "OpenFL experimental Workflow Interface enables the user to simulate a Federated Learning experiment using **LocalRuntime**. Once the simulation is ready, the methodology described in this tutorial enables the user to convert this experiment into an OpenFL workspace that can be deployed using the Aggregator-Based-Workflow\n",
    "\n",
    "##### High Level Overview of Methodology\n",
    "1. User annotates the relevant cells of the Jupyter notebook with `#| export` directive\n",
    "2. We then Leverage `nbdev` functionality to export these annotated cells of Jupyter notebook into a Python script\n",
    "3. Utilize OpenFL experimental module `WorkspaceExport` to convert the Python script into a OpenFL workspace\n",
    "4. User can utilize the experimental `fx` commands to deploy and run the federation seamlessly\n",
    "\n",
    "\n",
    "The methodology is described using an existing [OpenFL Two Party VFL Tutorial](https://github.com/securefederatedai/openfl/blob/develop/openfl-tutorials/experimental/Vertical_FL/Workflow_Interface_VFL_Two_Party.ipynb). Let's get started !"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "054e58b4-1ecf-475e-ac5d-3b972ee25431",
   "metadata": {},
   "source": [
    "## Getting Started"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3d918e19-90ac-4ab3-a678-0b2d94debaac",
   "metadata": {},
   "source": [
    "Initially, we start by specifying the module where cells marked with the `#| export` directive will be automatically exported. \n",
    "\n",
    "In the following cell, `#| default_exp experiment `indicates that the exported file will be named 'experiment'. This name can be modified based on user's requirement & preferences"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0e35ccea-bb36-4d73-8dcf-a34e4d84908b",
   "metadata": {},
   "outputs": [],
   "source": [
    "#| default_exp experiment"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d65f17c2-a772-4f62-848e-9ba6ad1ab128",
   "metadata": {},
   "source": [
    "We start by installing OpenFL and dependencies of the workflow interface \n",
    "> These dependencies are required to be exported and become the requirements for the Federated Learning Workspace "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6867c928-430d-4710-ac2a-4f4e2c86ab0f",
   "metadata": {},
   "outputs": [],
   "source": [
    "#| export\n",
    "\n",
    "!pip install git+https://github.com/intel/openfl.git\n",
    "!pip install -r ../requirements_workflow_interface.txt\n",
    "!pip install torch\n",
    "!pip install torchvision"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7cbcb941-93b8-4427-85ae-0c17439a81d7",
   "metadata": {},
   "source": [
    "We now define our dataloaders, model, optimizer, and some helper functions like we would for any other deep learning experiment \n",
    "\n",
    "> This cell and all the subsequent cells are important ingredients of the Federated Learning experiment and therefore annotated with the `#| export` directive"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "difficult-madrid",
   "metadata": {},
   "outputs": [],
   "source": [
    "#| export\n",
    "\n",
    "from copy import deepcopy\n",
    "import numpy as np\n",
    "import torch\n",
    "import torchvision\n",
    "from time import time\n",
    "from torchvision import datasets, transforms\n",
    "from torch import nn, optim\n",
    "\n",
    "from openfl.experimental.interface import FLSpec, Aggregator, Collaborator\n",
    "from openfl.experimental.runtime import LocalRuntime\n",
    "from openfl.experimental.placement import aggregator, collaborator\n",
    "\n",
    "# Data preprocessing\n",
    "transform = transforms.Compose([transforms.ToTensor(),\n",
    "                                transforms.Normalize((0.5,), (0.5,)),\n",
    "                                ])\n",
    "trainset = datasets.MNIST('mnist', download=True,\n",
    "                          train=True, transform=transform)\n",
    "trainloader = torch.utils.data.DataLoader(\n",
    "    trainset, batch_size=2048, shuffle=False)\n",
    "\n",
    "testset = datasets.MNIST('mnist', download=True,\n",
    "                         train=False, transform=transform)\n",
    "testloader = torch.utils.data.DataLoader(testset, batch_size=64, shuffle=False)\n",
    "\n",
    "torch.manual_seed(0)  # Define our model segments\n",
    "input_size = 784\n",
    "hidden_sizes = [128, 640]\n",
    "output_size = 10\n",
    "\n",
    "label_model = nn.Sequential(\n",
    "    nn.Linear(hidden_sizes[1], output_size),\n",
    "    nn.LogSoftmax(dim=1)\n",
    ")\n",
    "\n",
    "label_model_optimizer = optim.SGD(label_model.parameters(), lr=0.03)\n",
    "\n",
    "data_model = nn.Sequential(\n",
    "    nn.Linear(input_size, hidden_sizes[0]),\n",
    "    nn.ReLU(),\n",
    "    nn.Linear(hidden_sizes[0], hidden_sizes[1]),\n",
    "    nn.ReLU(),\n",
    ")\n",
    "\n",
    "data_model_optimizer = optim.SGD(data_model.parameters(), lr=0.03)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b4cae5c4-6b9c-4dc1-bde0-29e4f90bf414",
   "metadata": {},
   "source": [
    "Now we define the workflow for Vertical Federated Learning"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "forward-world",
   "metadata": {},
   "outputs": [],
   "source": [
    "#| export\n",
    "\n",
    "class VerticalTwoPartyFlow(FLSpec):\n",
    "\n",
    "    def __init__(self, total_rounds, batch_num=0):\n",
    "        super().__init__()\n",
    "        self.batch_num = batch_num\n",
    "        self.total_rounds = total_rounds\n",
    "        self.round = 0\n",
    "        \n",
    "\n",
    "    @aggregator\n",
    "    def start(self):\n",
    "        if self.batch_num == 0:\n",
    "            print(f'Starting round {self.round}')\n",
    "            self.data_remaining=True\n",
    "            self.collaborators = self.runtime.collaborators\n",
    "        else:\n",
    "            print(f'Batch_num = {self.batch_num}')\n",
    "        # 1) Zero the gradients\n",
    "        self.label_model_optimizer.zero_grad()\n",
    "        self.next(self.data_model_forward_pass, foreach='collaborators')\n",
    "\n",
    "\n",
    "    @collaborator\n",
    "    def data_model_forward_pass(self):\n",
    "        self.data_model_output_local = ''\n",
    "        for idx, (images, _) in enumerate(self.trainloader):\n",
    "            if idx < self.batch_num:\n",
    "                continue\n",
    "            self.data_model_optimizer.zero_grad()\n",
    "            images = images.view(images.shape[0], -1)\n",
    "            model_output = self.data_model(images)\n",
    "            self.data_model_output_local = model_output\n",
    "            self.data_model_output = model_output.detach().requires_grad_()\n",
    "            break\n",
    "        self.next(self.label_model_forward_pass)\n",
    "                  #exclude=['data_model_output_local'])\n",
    "\n",
    "    @aggregator\n",
    "    def label_model_forward_pass(self, inputs):\n",
    "        criterion = nn.NLLLoss()\n",
    "        self.grad_to_local = []\n",
    "        total_loss = 0\n",
    "        self.data_remaining = False\n",
    "        for idx, (_, labels) in enumerate(self.trainloader):\n",
    "            if idx < self.batch_num:\n",
    "                continue\n",
    "            self.data_remaining = True\n",
    "            pred = self.label_model(inputs[0].data_model_output)\n",
    "            loss = criterion(pred, labels)\n",
    "            loss.backward()\n",
    "            self.grad_to_local = inputs[0].data_model_output.grad.clone()\n",
    "            self.label_model_optimizer.step()\n",
    "            total_loss += loss\n",
    "            break\n",
    "        print(f'Total loss = {total_loss}')  # / len(self.trainloader)}')\n",
    "        self.next(self.data_model_backprop, foreach='collaborators')\n",
    "\n",
    "    @collaborator\n",
    "    def data_model_backprop(self):\n",
    "        if self.data_remaining:\n",
    "            self.data_model_optimizer = optim.SGD(self.data_model.parameters(), lr=0.03)\n",
    "            self.data_model_optimizer.zero_grad()\n",
    "            self.data_model_output_local.backward(self.grad_to_local)\n",
    "            self.data_model_optimizer.step()\n",
    "        self.next(self.join)\n",
    "\n",
    "    @aggregator\n",
    "    def join(self, inputs):\n",
    "        print(f'Join batch_num = {self.batch_num}')\n",
    "        self.batch_num += 1\n",
    "        self.next(self.check_round_completion)\n",
    "\n",
    "    @aggregator\n",
    "    def check_round_completion(self):\n",
    "        if self.round == self.total_rounds:\n",
    "            self.next(self.end)\n",
    "        else:\n",
    "            if self.data_remaining:\n",
    "                print(f'Continuing training loop: batch_num = {self.batch_num}')\n",
    "                self.next(self.start)\n",
    "            else:\n",
    "                print('Start next round')\n",
    "                self.round += 1\n",
    "                self.batch_num = 0\n",
    "                self.next(self.start)\n",
    "\n",
    "    @aggregator\n",
    "    def end(self):\n",
    "        print(f'This is the end of the flow')\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5806d963-60a8-49be-bafe-0b8d2e027eb6",
   "metadata": {},
   "source": [
    "We now initialize private attributes of the aggregator and collaborator, simulation parameters (seed, batch-sizes, optimizer parameters) and create the `LocalRuntime`\n",
    "\n",
    "> NOTE: The aggregator based workflow is case sensitive. Therefore, the collaborator names should be registered in lowercase only."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "59aff1fc",
   "metadata": {},
   "outputs": [],
   "source": [
    "#| export\n",
    "\n",
    "# Setup participants\n",
    "aggregator = Aggregator()\n",
    "\n",
    "def callable_to_initialize_aggregator_private_attributes(train_loader,label_model,label_model_optimizer):\n",
    "        return {\"trainloader\": train_loader,\n",
    "                \"label_model\" : label_model,\n",
    "                \"label_model_optimizer\":label_model_optimizer\n",
    "                }  \n",
    "\n",
    "# Setup aggregator private attributes via callable function\n",
    "aggregator = Aggregator(\n",
    "    name=\"agg\",\n",
    "    private_attributes_callable=callable_to_initialize_aggregator_private_attributes,\n",
    "    train_loader = trainloader,\n",
    "    label_model=label_model,\n",
    "    label_model_optimizer=label_model_optimizer\n",
    ")\n",
    "\n",
    "# Setup collaborators private attributes via callable function\n",
    "collaborator_names = ['Portland']\n",
    "\n",
    "def callable_to_initialize_collaborator_private_attributes(index,data_model,data_model_optimizer,train_loader):\n",
    "    return {\n",
    "        \"data_model\": data_model,\n",
    "        \"data_model_optimizer\": data_model_optimizer,\n",
    "        \"trainloader\" : deepcopy(train_loader)\n",
    "    }\n",
    "\n",
    "collaborators = []\n",
    "for idx, collaborator_name in enumerate(collaborator_names):\n",
    "        collaborators.append(\n",
    "            Collaborator(\n",
    "                name=collaborator_name,\n",
    "                private_attributes_callable=callable_to_initialize_collaborator_private_attributes,\n",
    "                index=idx,\n",
    "                data_model = data_model,\n",
    "                data_model_optimizer = data_model_optimizer,\n",
    "                train_loader = trainloader\n",
    "            )\n",
    "        )\n",
    "\n",
    "local_runtime = LocalRuntime(\n",
    "    aggregator=aggregator, collaborators=collaborators, backend='single_process')\n",
    "print(f'Local runtime collaborators = {local_runtime.collaborators}')\n",
    "\n",
    "\n",
    "total_rounds = 5\n",
    "vflow = VerticalTwoPartyFlow(total_rounds=total_rounds)\n",
    "vflow.runtime = local_runtime\n",
    "# vflow.run()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d5b37d8e-e271-4d72-b8eb-9c357927ebff",
   "metadata": {},
   "source": [
    "## Workspace creation"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3777b993-3d8f-404e-aa92-e1ad6f497d41",
   "metadata": {},
   "source": [
    "The following cells convert the Jupyter notebook into a Python script and create a Template Workspace that can be utilized by Aggregator based Workflow\n",
    "> NOTE: Only Notebook cells that were marked with `#| export` directive shall be included in this Python script\n",
    "\n",
    "We first import `WorkspaceExport` module and execute `WorkspaceExport.export()` that converts the notebook and generates the template workspace. User is required to specify: \n",
    "1. `notebook_path`: path of the Jupyter notebook that is required to be converted\n",
    "2. `output_workspace`: path where the converted workspace is stored"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "statutory-prime",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "from openfl.experimental.workspace_export import WorkspaceExport\n",
    "\n",
    "WorkspaceExport.export(\n",
    "    notebook_path='./Workflow_Interface_VFL_Two_Party_Workspace_Creation_from_JupyterNotebook.ipynb',\n",
    "    output_workspace=f\"/home/{os.environ['USER']}/generated-workspace\"\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9b789fa6-4c77-4eb0-84c6-bc91a87d86a3",
   "metadata": {},
   "source": [
    "## Workspace usage\n",
    "\n",
    "The workspace crated above can be used by the Aggregator based workflow by using the `fx` commands in the following manner"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "92c96636-57f0-474d-aac8-158a80e08f0d",
   "metadata": {},
   "source": [
    "**Workspace Activation and Creation**\n",
    "1. Activate the experimental aggregator-based workflow:\n",
    "\n",
    "    `fx experimental activate`\n",
    "\n",
    "   This will create an 'experimental' directory under ~/.openfl/\n",
    "3. Create a workspace using the custom template:\n",
    "\n",
    "    `fx workspace create --prefix workspace_path --custom_template /home/$USER/generated-workspace`\n",
    "4. Change to the workspace directory:\n",
    "\n",
    "    `cd workspace_path`\n",
    "\n",
    "**Workspace Initialization and Certification**\n",
    "1. Initialize the FL plan and auto-populate the fully qualified domain name (FQDN) of the aggregator node:\n",
    "\n",
    "    `fx plan initialize`\n",
    "2. Certify the workspace:\n",
    "\n",
    "    `fx workspace certify`\n",
    "    \n",
    "**Aggregator Setup and Workspace Export**\n",
    "1. Run the aggregator certificate creation command:\n",
    "\n",
    "    `fx aggregator generate-cert-request`\n",
    "\n",
    "    `fx aggregator certify`\n",
    "2. Export the workspace for collaboration:\n",
    "\n",
    "    `fx workspace export`\n",
    "    \n",
    "**Collaborator Node Setup**\n",
    "\n",
    "***On the Collaborator Node:***\n",
    "\n",
    "1. Copy the workspace archive from the aggregator node to the collaborator nodes. Import the workspace archive:\n",
    "\n",
    "    `fx workspace import --archive WORKSPACE.zip`\n",
    "   \n",
    "    `cd workspace_path`\n",
    "3. Generate a collaborator certificate request:\n",
    "\n",
    "    `fx collaborator generate-cert-request -n {COL_LABEL}`\n",
    "\n",
    "***On the Aggregator Node (Certificate Authority):***\n",
    "\n",
    "3. Sign the Collaborator Certificate Signing Request (CSR) Package from collaborator nodes:\n",
    "\n",
    "    `fx collaborator certify --request-pkg /PATH/TO/col_{COL_LABEL}_to_agg_cert_request.zip`\n",
    "\n",
    "***On the Collaborator Node:***\n",
    "\n",
    "4. Import the signed certificate and certificate chain into the workspace:\n",
    "\n",
    "    `fx collaborator certify --import /PATH/TO/agg_to_col_{COL_LABEL}_signed_cert.zip`\n",
    "    \n",
    "**Final Workspace Activation**\n",
    "***On the Aggregator Node:***\n",
    "\n",
    "1. Start the Aggregator:\n",
    "\n",
    "    `fx aggregator start`\n",
    "    \n",
    "    The Aggregator is now running and waiting for Collaborators to connect.\n",
    "\n",
    "***On the Collaborator Nodes:***\n",
    "\n",
    "2. Run the Collaborator:\n",
    "\n",
    "    `fx collaborator start -n {COL_LABEL}`\n",
    "\n",
    "**Workspace Deactivation**\n",
    "1. To deactivate the experimental aggregator-based workflow and switch back to original aggregator-based workflow:\n",
    "\n",
    "    `fx experimental deactivate`\n",
    "\n",
    "   This will remove the 'experimental' directory under ~/.openfl/"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "v_o",
   "language": "python",
   "name": "v_o"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}

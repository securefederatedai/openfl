{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "14821d97",
   "metadata": {},
   "source": [
    "# Workflow Interface 102 - Held out aggregator validation\n",
    "[![Open In Colab](https://colab.research.google.com/assets/colab-badge.svg)](https://colab.research.google.com/github/intel/openfl/blob/develop/openfl-tutorials/experimental/Workflow_Interface_102_Aggregator_Validation.ipynb)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "bd059520",
   "metadata": {},
   "source": [
    "In this tutorial, we build on the ideas from the [first](https://github.com/securefederatedai/openfl/blob/develop/openfl-tutorials/experimental/Workflow_Interface_101_MNIST.ipynb) quick start notebook, and demonstrate how to perform validation on the aggregator after training."
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "fc8e35da",
   "metadata": {},
   "source": [
    "# Getting Started"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "4dbb89b6",
   "metadata": {},
   "source": [
    "First we start by installing the necessary dependencies for the workflow interface"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f7f98600",
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip install git+https://github.com/securefederatedai/openfl.git\n",
    "!pip install -r workflow_interface_requirements.txt\n",
    "!pip install torch\n",
    "!pip install torchvision\n",
    "\n",
    "# Uncomment this if running in Google Colab\n",
    "#!pip install -r https://raw.githubusercontent.com/intel/openfl/develop/openfl-tutorials/experimental/workflow_interface_requirements.txt\n",
    "#import os\n",
    "#os.environ[\"USERNAME\"] = \"colab\""
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "7237eac4",
   "metadata": {},
   "source": [
    "We begin with the quintessential example of a small pytorch CNN model trained on the MNIST dataset. Let's start define our dataloaders, model, optimizer, and some helper functions like we would for any other deep learning experiment"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7e85e030",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import torch.optim as optim\n",
    "import torch\n",
    "import torchvision\n",
    "import numpy as np\n",
    "\n",
    "n_epochs = 3\n",
    "batch_size_train = 64\n",
    "batch_size_test = 1000\n",
    "learning_rate = 0.01\n",
    "momentum = 0.5\n",
    "log_interval = 10\n",
    "\n",
    "random_seed = 1\n",
    "torch.backends.cudnn.enabled = False\n",
    "torch.manual_seed(random_seed)\n",
    "\n",
    "mnist_train = torchvision.datasets.MNIST('files/', train=True, download=True,\n",
    "                             transform=torchvision.transforms.Compose([\n",
    "                               torchvision.transforms.ToTensor(),\n",
    "                               torchvision.transforms.Normalize(\n",
    "                                 (0.1307,), (0.3081,))\n",
    "                             ]))\n",
    "\n",
    "mnist_test = torchvision.datasets.MNIST('files/', train=False, download=True,\n",
    "                             transform=torchvision.transforms.Compose([\n",
    "                               torchvision.transforms.ToTensor(),\n",
    "                               torchvision.transforms.Normalize(\n",
    "                                 (0.1307,), (0.3081,))\n",
    "                             ]))\n",
    "\n",
    "class Net(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(Net, self).__init__()\n",
    "        self.conv1 = nn.Conv2d(1, 10, kernel_size=5)\n",
    "        self.conv2 = nn.Conv2d(10, 20, kernel_size=5)\n",
    "        self.conv2_drop = nn.Dropout2d()\n",
    "        self.fc1 = nn.Linear(320, 50)\n",
    "        self.fc2 = nn.Linear(50, 10)\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = F.relu(F.max_pool2d(self.conv1(x), 2))\n",
    "        x = F.relu(F.max_pool2d(self.conv2_drop(self.conv2(x)), 2))\n",
    "        x = x.view(-1, 320)\n",
    "        x = F.relu(self.fc1(x))\n",
    "        x = F.dropout(x, training=self.training)\n",
    "        x = self.fc2(x)\n",
    "        return F.log_softmax(x)\n",
    "    \n",
    "def inference(network,test_loader):\n",
    "    network.eval()\n",
    "    test_loss = 0\n",
    "    correct = 0\n",
    "    with torch.no_grad():\n",
    "      for data, target in test_loader:\n",
    "        output = network(data)\n",
    "        test_loss += F.nll_loss(output, target, size_average=False).item()\n",
    "        pred = output.data.max(1, keepdim=True)[1]\n",
    "        correct += pred.eq(target.data.view_as(pred)).sum()\n",
    "    test_loss /= len(test_loader.dataset)\n",
    "    print('\\nTest set: Avg. loss: {:.4f}, Accuracy: {}/{} ({:.0f}%)\\n'.format(\n",
    "      test_loss, correct, len(test_loader.dataset),\n",
    "      100. * correct / len(test_loader.dataset)))\n",
    "    accuracy = float(correct / len(test_loader.dataset))\n",
    "    return accuracy"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "cd268911",
   "metadata": {},
   "source": [
    "Next we import the `FLSpec`, `LocalRuntime`, and placement decorators.\n",
    "\n",
    "- `FLSpec` – Defines the flow specification. User defined flows are subclasses of this.\n",
    "- `Runtime` – Defines where the flow runs, infrastructure for task transitions (how information gets sent). The `LocalRuntime` runs the flow on a single node.\n",
    "- `aggregator/collaborator` - placement decorators that define where the task will be assigned"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "precise-studio",
   "metadata": {},
   "outputs": [],
   "source": [
    "from copy import deepcopy\n",
    "\n",
    "from openfl.experimental.interface import FLSpec, Aggregator, Collaborator\n",
    "from openfl.experimental.runtime import LocalRuntime\n",
    "from openfl.experimental.placement import aggregator, collaborator\n",
    "\n",
    "\n",
    "def FedAvg(models, weights=None):\n",
    "    new_model = models[0]\n",
    "    state_dicts = [model.state_dict() for model in models]\n",
    "    state_dict = new_model.state_dict()\n",
    "    for key in models[1].state_dict():\n",
    "        state_dict[key] = torch.from_numpy(np.average([state[key].numpy() for state in state_dicts],\n",
    "                                                      axis=0, \n",
    "                                                      weights=weights))\n",
    "    new_model.load_state_dict(state_dict)\n",
    "    return new_model\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "b2e45614",
   "metadata": {
    "scrolled": true
   },
   "source": [
    "Now we come to the updated flow definition. Here we use the same tasks as the [quickstart](https://github.com/psfoley/openfl/blob/experimental-workflow-interface/openfl-tutorials/experimental/Workflow_Interface_MNIST.ipynb), but give the aggregator a `test_loader` as a private attribute. The aggregator will do a forward pass on each of the aggregator's models using it's validation data, and weight the highest accuracy model higher than others.  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "difficult-madrid",
   "metadata": {},
   "outputs": [],
   "source": [
    "class AggregatorValidationFlow(FLSpec):\n",
    "\n",
    "    def __init__(self, model = None, optimizer = None, rounds=3, **kwargs):\n",
    "        super().__init__(**kwargs)\n",
    "        if model is not None:\n",
    "            self.model = model\n",
    "            self.optimizer = optimizer\n",
    "        else:\n",
    "            self.model = Net()\n",
    "            self.optimizer = optim.SGD(self.model.parameters(), lr=learning_rate,\n",
    "                                   momentum=momentum)\n",
    "        self.rounds = rounds\n",
    "\n",
    "    @aggregator\n",
    "    def start(self):\n",
    "        print(f'Performing initialization for model')\n",
    "        self.collaborators = self.runtime.collaborators\n",
    "        self.private = 10\n",
    "        self.current_round = 0\n",
    "        self.next(self.aggregated_model_validation,foreach='collaborators',exclude=['private'])\n",
    "\n",
    "    @collaborator\n",
    "    def aggregated_model_validation(self):\n",
    "        print(f'Performing aggregated model validation for collaborator {self.input}')\n",
    "        self.agg_validation_score = inference(self.model,self.test_loader)\n",
    "        print(f'{self.input} value of {self.agg_validation_score}')\n",
    "        self.next(self.train)\n",
    "\n",
    "    @collaborator\n",
    "    def train(self):\n",
    "        self.model.train()\n",
    "        self.optimizer = optim.SGD(self.model.parameters(), lr=learning_rate,\n",
    "                                   momentum=momentum)\n",
    "        train_losses = []\n",
    "        for batch_idx, (data, target) in enumerate(self.train_loader):\n",
    "          self.optimizer.zero_grad()\n",
    "          output = self.model(data)\n",
    "          loss = F.nll_loss(output, target)\n",
    "          loss.backward()\n",
    "          self.optimizer.step()\n",
    "          if batch_idx % log_interval == 0:\n",
    "            print('Train Epoch: 1 [{}/{} ({:.0f}%)]\\tLoss: {:.6f}'.format(\n",
    "               batch_idx * len(data), len(self.train_loader.dataset),\n",
    "              100. * batch_idx / len(self.train_loader), loss.item()))\n",
    "            self.loss = loss.item()\n",
    "            torch.save(self.model.state_dict(), 'model.pth')\n",
    "            torch.save(self.optimizer.state_dict(), 'optimizer.pth')\n",
    "        self.training_completed = True\n",
    "        self.next(self.local_model_validation)\n",
    "\n",
    "    @collaborator\n",
    "    def local_model_validation(self):\n",
    "        self.local_validation_score = inference(self.model,self.test_loader)\n",
    "        print(f'Doing local model validation for collaborator {self.input}: {self.local_validation_score}')\n",
    "        self.next(self.join, exclude=['training_completed'])\n",
    "\n",
    "    @aggregator\n",
    "    def join(self,inputs):\n",
    "        self.average_loss = sum(input.loss for input in inputs)/len(inputs)\n",
    "        self.aggregated_model_accuracy = sum(input.agg_validation_score for input in inputs)/len(inputs)\n",
    "        self.local_model_accuracy = sum(input.local_validation_score for input in inputs)/len(inputs)\n",
    "        print(f'Average aggregated model validation values = {self.aggregated_model_accuracy}')\n",
    "        print(f'Average training loss = {self.average_loss}')\n",
    "        print(f'Average local model validation values = {self.local_model_accuracy}')\n",
    "        \n",
    "        highest_accuracy = 0\n",
    "        highest_accuracy_model_idx = -1\n",
    "        for idx,col in enumerate(inputs):\n",
    "            accuracy_for_held_out_agg_data = inference(col.model,self.test_loader)\n",
    "            if accuracy_for_held_out_agg_data > highest_accuracy:\n",
    "                highest_accuracy = accuracy_for_held_out_agg_data\n",
    "                highest_accuracy_model_idx = idx\n",
    "        \n",
    "        relative_model_weights = len(inputs)*[1]\n",
    "        # Give highest accuracy model (on held out aggregator data) 2x the importance\n",
    "        relative_model_weights[highest_accuracy_model_idx] = 2\n",
    "        print(f'Aggregator validation score: {highest_accuracy}')\n",
    "        print(f'Highest accuracy model sent from {inputs[highest_accuracy_model_idx].input}. Receiving 2x weight in updated model')\n",
    "        self.model = FedAvg([input.model for input in inputs],weights=relative_model_weights)\n",
    "        self.optimizer = [input.optimizer for input in inputs][0]\n",
    "        self.current_round += 1\n",
    "        if self.current_round < self.rounds:\n",
    "            self.next(self.aggregated_model_validation, foreach='collaborators', exclude=['private'])\n",
    "        else:\n",
    "            self.next(self.end)\n",
    "        \n",
    "    @aggregator\n",
    "    def end(self):\n",
    "        print(f'This is the end of the flow')  "
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "7a133f9f",
   "metadata": {},
   "source": [
    "You'll notice in the `FederatedFlow` definition above that there were certain attributes that the flow was not initialized with, namely the `train_loader` and `test_loader` for each of the collaborators. Each participant has it's own set of private attributes which can be set using callback function while instantiating the participant. The callback function returns the private attributes (`train_loader` & `test_loader`) in form of a dictionary where the key is the attribute name, and the value is the object that will be made accessible to that participant's task\n",
    "\n",
    "Below, we segment shards of the MNIST dataset for **four collaborators**: `Portland`, `Seattle`, `Chandler`, and `Portland`. Each has their own slice of the dataset that is accessible through the `train_loader` and `test_loader` attributes, which are set using the `callable_to_initialize_collaborator_private_attributes` callable function. Note that the private attributes are flexible, and you can choose to pass in a completely different type of object to any of the collaborators or aggregator (with an arbitrary name). These private attributes will always be filtered out of the current state when transfering from collaborator to aggregator, or vice versa.\n",
    "\n",
    "Private attributes can be set using callback function while instantiating the participant. Parameters required by the callback function are specified as arguments while instantiating the participant. In this example callback function, `callable_to_initialize_collaborator_private_attributes`, returns the private attributes `train_loader` and `test_loader` of the collaborator. Callback function, `callable_to_initialize_aggregator_private_attributes`, returns the private attribute `test_loader` of the Aggregator."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "forward-world",
   "metadata": {},
   "outputs": [],
   "source": [
    "collaborator_names = ['Portland', 'Seattle', 'Chandler', 'Bangalore']\n",
    "\n",
    "def callable_to_initialize_aggregator_private_attributes(n_collaborators, test_dataset, batch_size_train):\n",
    "    aggregator_test = deepcopy(test_dataset)\n",
    "    aggregator_test.targets = test_dataset.targets[n_collaborators::n_collaborators+1]\n",
    "    aggregator_test.data = test_dataset.data[n_collaborators::n_collaborators+1]\n",
    "    return {\n",
    "        'test_loader': torch.utils.data.DataLoader(aggregator_test,batch_size=batch_size_train, shuffle=True)\n",
    "    }\n",
    "\n",
    "# Setup Aggregator private attributes via callable function\n",
    "aggregator = Aggregator(\n",
    "    name=\"agg\",\n",
    "    private_attributes_callable=callable_to_initialize_aggregator_private_attributes,\n",
    "    n_collaborators=len(collaborator_names),\n",
    "    test_dataset=mnist_test, batch_size_train=batch_size_train\n",
    ")\n",
    "\n",
    "# Setup collaborators private attributes via callable function\n",
    "def callable_to_initialize_collaborator_private_attributes(index, n_collaborators, train_dataset, test_dataset, batch_size_train):\n",
    "    local_train = deepcopy(train_dataset)\n",
    "    local_test = deepcopy(test_dataset)\n",
    "    local_train.data = train_dataset.data[index::n_collaborators]\n",
    "    local_train.targets = train_dataset.targets[index::n_collaborators]\n",
    "    local_test.data = test_dataset.data[index::n_collaborators]\n",
    "    local_test.targets = test_dataset.targets[index::n_collaborators]\n",
    "    \n",
    "    return {\n",
    "        'train_loader': torch.utils.data.DataLoader(local_train,batch_size=batch_size_train, shuffle=True),\n",
    "        'test_loader': torch.utils.data.DataLoader(local_test,batch_size=batch_size_train, shuffle=True)\n",
    "    }\n",
    "\n",
    "collaborators=[]\n",
    "for idx, collaborator_name in enumerate(collaborator_names):\n",
    "    collaborators.append(\n",
    "        Collaborator(\n",
    "            name=collaborator_name, num_cpus=0, num_gpus=0,\n",
    "            private_attributes_callable=callable_to_initialize_collaborator_private_attributes,\n",
    "            index=idx, n_collaborators=len(collaborator_names),\n",
    "            train_dataset=mnist_train, test_dataset=mnist_test, batch_size_train=batch_size_train,\n",
    "        )\n",
    "    )\n",
    "\n",
    "local_runtime = LocalRuntime(aggregator=aggregator, collaborators=collaborators, backend='ray')\n",
    "print(f'Local runtime collaborators = {local_runtime.collaborators}')"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "0525eaa9",
   "metadata": {},
   "source": [
    "Now that we have our flow and runtime defined, let's run the experiment! "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d581d96a",
   "metadata": {},
   "outputs": [],
   "source": [
    "model = None\n",
    "best_model = None\n",
    "optimizer = None\n",
    "flflow = AggregatorValidationFlow(model, optimizer)\n",
    "flflow.runtime = local_runtime\n",
    "flflow.run()"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "8b9f8d25",
   "metadata": {},
   "source": [
    "# Congratulations!\n",
    "Now that you've completed your this notebook, see some of the more advanced things you can do in our [other tutorials](broken_link), including:\n",
    "\n",
    "- Using the LocalRuntime Ray Backend for dedicated GPU access\n",
    "- Vertical Federated Learning\n",
    "- Model Watermarking\n",
    "- Differential Privacy\n",
    "- And More!"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "new_test_env",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.19"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}

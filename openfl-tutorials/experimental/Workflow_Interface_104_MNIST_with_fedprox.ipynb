{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "14821d97",
   "metadata": {},
   "source": [
    "# Workflow Interface 104: MNIST with Fedprox\n",
    "[![Open In Colab](https://colab.research.google.com/assets/colab-badge.svg)](https://colab.research.google.com/github/intel/openfl/blob/develop/openfl-tutorials/experimental/Workflow_Interface_104_MNIST_with_fedprox.ipynb)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bd059520",
   "metadata": {},
   "source": [
    "In this tutorial, we demonstate how to use FedProx aggregation algorithm to tackle data heterogeneity in federated setup.\n",
    "\n",
    "- Fedprox is a generalization and reparameterization of FedAvg\n",
    "- Demonstrates more stable and accurate convergence compared to FedAvg for non-iid datasets.\n",
    "- It uses a proximal term in its calculations to help improve stability.\n",
    "- Fedprox paper: https://arxiv.org/pdf/1812.06127.pdf"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fc8e35da",
   "metadata": {},
   "source": [
    "# Getting Started"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4dbb89b6",
   "metadata": {},
   "source": [
    "First we start by installing the necessary dependencies for the workflow interface"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f7f98600",
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip install git+https://github.com/intel/openfl.git\n",
    "!pip install -r requirements_workflow_interface.txt\n",
    "\n",
    "# Uncomment this if running in Google Colab\n",
    "#!pip install -r https://raw.githubusercontent.com/intel/openfl/develop/openfl-tutorials/experimental/requirements_workflow_interface.txt\n",
    "#import os\n",
    "#os.environ[\"USERNAME\"] = \"colab\""
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7237eac4",
   "metadata": {},
   "source": [
    "We begin with the quintessential example of a small pytorch CNN model trained on the MNIST dataset. Let's start define our dataloaders, model, optimizer, and some helper functions like we would for any other deep learning experiment"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7e85e030",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import torch.optim as optim\n",
    "import torch\n",
    "import torchvision\n",
    "import numpy as np\n",
    "\n",
    "batch_size_train = 64\n",
    "batch_size_test = 1000\n",
    "\n",
    "\n",
    "random_seed = 1\n",
    "torch.backends.cudnn.enabled = False\n",
    "torch.manual_seed(random_seed)\n",
    "\n",
    "mnist_train = torchvision.datasets.MNIST('files/', train=True, download=False,\n",
    "                             transform=torchvision.transforms.Compose([\n",
    "                               torchvision.transforms.ToTensor(),\n",
    "                               torchvision.transforms.Normalize(\n",
    "                                 (0.1307,), (0.3081,))\n",
    "                             ]))\n",
    "\n",
    "mnist_test = torchvision.datasets.MNIST('files/', train=False, download=False,\n",
    "                             transform=torchvision.transforms.Compose([\n",
    "                               torchvision.transforms.ToTensor(),\n",
    "                               torchvision.transforms.Normalize(\n",
    "                                 (0.1307,), (0.3081,))\n",
    "                             ]))\n",
    "\n",
    "class Net(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(Net, self).__init__()\n",
    "        self.conv1 = nn.Conv2d(1, 10, kernel_size=5)\n",
    "        self.conv2 = nn.Conv2d(10, 20, kernel_size=5)\n",
    "        self.conv2_drop = nn.Dropout2d()\n",
    "        self.fc1 = nn.Linear(320, 50)\n",
    "        self.fc2 = nn.Linear(50, 10)\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = F.relu(F.max_pool2d(self.conv1(x), 2))\n",
    "        x = F.relu(F.max_pool2d(self.conv2_drop(self.conv2(x)), 2))\n",
    "        x = x.view(-1, 320)\n",
    "        x = F.relu(self.fc1(x))\n",
    "        x = F.dropout(x, training=self.training)\n",
    "        x = self.fc2(x)\n",
    "        return F.log_softmax(x)\n",
    "\n",
    "def compute_loss_and_acc(network,test_loader):\n",
    "    network.eval()\n",
    "    test_loss = 0\n",
    "    correct = 0\n",
    "    with torch.no_grad():\n",
    "      for data, target in test_loader:\n",
    "        output = network(data)\n",
    "        test_loss += F.nll_loss(output, target, size_average=False).item()\n",
    "        pred = output.data.max(1, keepdim=True)[1]\n",
    "        correct += pred.eq(target.data.view_as(pred)).sum()\n",
    "    test_loss /= len(test_loader.dataset)\n",
    "    print('\\nTest set: Avg. loss: {:.4f}, Accuracy: {}/{} ({:.0f}%)\\n'.format(\n",
    "      test_loss, correct, len(test_loader.dataset),\n",
    "      100. * correct / len(test_loader.dataset)))\n",
    "    accuracy = float(correct / len(test_loader.dataset))\n",
    "    return accuracy, test_loss, correct"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cd268911",
   "metadata": {},
   "source": [
    "Next we import the `FLSpec`, `LocalRuntime`, and placement decorators.\n",
    "\n",
    "- `FLSpec` – Defines the flow specification. User defined flows are subclasses of this.\n",
    "- `Runtime` – Defines where the flow runs, infrastructure for task transitions (how information gets sent). The `LocalRuntime` runs the flow on a single node.\n",
    "- `aggregator/collaborator` - placement decorators that define where the task will be assigned"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "precise-studio",
   "metadata": {},
   "outputs": [],
   "source": [
    "from copy import deepcopy\n",
    "\n",
    "from openfl.experimental.interface import FLSpec, Aggregator, Collaborator\n",
    "from openfl.experimental.runtime import LocalRuntime\n",
    "from openfl.experimental.placement import aggregator, collaborator\n",
    "\n",
    "# Import the framework adapter plugin, Fedprox optimizer and aggregation function\n",
    "from openfl.plugins.frameworks_adapters.pytorch_adapter import FrameworkAdapterPlugin as fa\n",
    "from openfl.experimental.interface.aggregation_functions.fedprox import FedProxAgg\n",
    "from openfl.utilities.optimizers.torch import FedProxOptimizer\n",
    "\n",
    "from collections import OrderedDict\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "8e406db6",
   "metadata": {
    "scrolled": true
   },
   "source": [
    "Now we come to the flow definition. The OpenFL Workflow Interface adopts the conventions set by Metaflow, that every workflow begins with `start` and concludes with the `end` task. The aggregator begins with an optionally passed in model and optimizer. The aggregator begins the flow with the `start` task, where the list of collaborators is extracted from the runtime (`self.collaborators = self.runtime.collaborators`) and is then used as the list of participants to run the task listed in `self.next`, `aggregated_model_validation`. The model, optimizer, and anything that is not explicitly excluded from the next function will be passed from the `start` function on the aggregator to the `aggregated_model_validation` task on the collaborator. Where the tasks run is determined by the placement decorator that precedes each task definition (`@aggregator` or `@collaborator`). Once each of the collaborators (defined in the runtime) complete the `aggregated_model_validation` task, they pass their current state onto the `train` task, from `train` to `local_model_validation`, and then finally to `join` at the aggregator. It is in `join` that an average is taken of the model weights, and the next round can begin.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "difficult-madrid",
   "metadata": {},
   "outputs": [],
   "source": [
    "class FedProxFlow(FLSpec):\n",
    "\n",
    "    def __init__(self, model=None, optimizer=None, n_selected_collaborators=10, n_rounds=10, **kwargs):\n",
    "        super(FedProxFlow, self).__init__(**kwargs)\n",
    "        self.round_number = 1\n",
    "        self.n_selected_collaborators = n_selected_collaborators\n",
    "        self.n_rounds = n_rounds\n",
    "        self.loss_and_acc = {\"Train Loss\": [], \"Test Accuracy\": []}\n",
    "        if model is not None:\n",
    "            self.model = model\n",
    "            self.optimizer = optimizer\n",
    "        else:\n",
    "            self.model = Net()\n",
    "            self.optimizer = FedProxOptimizer(\n",
    "                self.model.parameters(), lr=learning_rate, mu=mu, weight_decay=weight_decay)\n",
    "\n",
    "        self.agg_func = FedProxAgg()\n",
    "\n",
    "    @aggregator\n",
    "    def start(self):\n",
    "        \"\"\"\n",
    "        Start of the flow. Call compute_loss_and_accuracy step for each collaborator\n",
    "        \"\"\"\n",
    "        print(f'\\nStarting round number {self.round_number} .... \\n')\n",
    "        self.collaborators = self.runtime.collaborators\n",
    "        self.next(self.compute_loss_and_accuracy, foreach='collaborators')\n",
    "\n",
    "    @collaborator\n",
    "    def compute_loss_and_accuracy(self):\n",
    "        \"\"\"\n",
    "        Compute training accuracy, training loss, aggregated validation accuracy,\n",
    "        aggregated validation loss, \n",
    "        \"\"\"\n",
    "        # Compute Train Loss and Train Acc\n",
    "        self.training_accuracy, self.training_loss, _, = compute_loss_and_acc(\n",
    "            self.model, self.train_loader)\n",
    "        \n",
    "        # Compute Test Loss and Test Acc\n",
    "        self.agg_validation_score, self.agg_validation_loss, test_correct = compute_loss_and_acc(\n",
    "            self.model, self.test_loader)\n",
    "\n",
    "        self.train_dataset_length = len(self.train_loader.dataset)\n",
    "        self.test_dataset_length = len(self.test_loader.dataset)\n",
    "\n",
    "        print(\n",
    "            \"<Collab: {:<5}> | Train Round: {:<5} : Train Loss {:<.6f}, Test Acc: {:<.6f} [{}/{}]\".format(\n",
    "                self.input,\n",
    "                self.round_number,\n",
    "                self.training_loss,\n",
    "                self.agg_validation_score, \n",
    "                test_correct, \n",
    "                self.test_dataset_length\n",
    "            )\n",
    "        )\n",
    "\n",
    "        self.next(self.gather_results_and_take_weighted_average)\n",
    "\n",
    "    @aggregator\n",
    "    def gather_results_and_take_weighted_average(self, inputs):\n",
    "        \"\"\"\n",
    "        Gather results of all collaborators computed in previous \n",
    "        step.\n",
    "        Compute train and test weightes, and compute weighted average of \n",
    "        aggregated training loss, and aggregated test accuracy\n",
    "        \"\"\"\n",
    "        # Calculate train_weights and test_weights\n",
    "        train_datasize, test_datasize = [], []\n",
    "        for input_ in inputs:\n",
    "            train_datasize.append(input_.train_dataset_length)\n",
    "            test_datasize.append(input_.test_dataset_length)\n",
    "\n",
    "        self.train_weights, self.test_weights = [], []\n",
    "        for input_ in inputs:\n",
    "            self.train_weights.append(input_.train_dataset_length / sum(train_datasize))\n",
    "            self.test_weights.append(input_.test_dataset_length / sum(test_datasize))\n",
    "\n",
    "        aggregated_model_accuracy_list, aggregated_model_loss_list = [], []\n",
    "        for input_ in inputs:\n",
    "            aggregated_model_loss_list.append(input_.training_loss)\n",
    "            aggregated_model_accuracy_list.append(input_.agg_validation_score)\n",
    "\n",
    "        self.aggregated_model_training_loss, self.aggregated_model_test_accuracy = self.agg_func.aggregate_metrics(\n",
    "            [aggregated_model_loss_list, aggregated_model_accuracy_list], [self.train_weights, self.test_weights])\n",
    "        \n",
    "        # Store experiment results\n",
    "        self.loss_and_acc[\"Train Loss\"].append(self.aggregated_model_training_loss)\n",
    "        self.loss_and_acc[\"Test Accuracy\"].append(self.aggregated_model_test_accuracy)\n",
    "\n",
    "        print(\n",
    "            \"<Agg> | Train Round: {:<5} : Agg Train Loss {:<.6f}, Agg Test Acc: {:<.6f}\".format(\n",
    "                self.round_number,\n",
    "                self.aggregated_model_training_loss,\n",
    "                self.aggregated_model_test_accuracy\n",
    "            )\n",
    "        )\n",
    "\n",
    "        self.next(self.select_collaborators)\n",
    "\n",
    "    @aggregator\n",
    "    def select_collaborators(self):\n",
    "        \"\"\"\n",
    "        Randomly select n_selected_collaborators collaborator\n",
    "        \"\"\"\n",
    "        np.random.seed(self.round_number)\n",
    "        self.selected_collaborator_indices = np.random.choice(range(len(self.collaborators)), \\\n",
    "            self.n_selected_collaborators, replace=False)\n",
    "        self.selected_collaborators = [self.collaborators[idx] for idx in self.selected_collaborator_indices]\n",
    "\n",
    "        self.next(self.train_selected_collaborators, foreach=\"selected_collaborators\")\n",
    "\n",
    "\n",
    "    @collaborator\n",
    "    def train_selected_collaborators(self):\n",
    "        \"\"\"\n",
    "        Train selected collaborators\n",
    "        \"\"\"\n",
    "        self.model.train(mode=True)\n",
    "\n",
    "        self.train_dataset_length = len(self.train_loader.dataset)\n",
    "\n",
    "        # Rebuild the optimizer with global model parameters\n",
    "        self.optimizer = FedProxOptimizer(\n",
    "            self.model.parameters(), lr=learning_rate, mu=mu, weight_decay=weight_decay)\n",
    "        # Set global model parameters as old weights to enable computation of proximal term\n",
    "        self.optimizer.set_old_weights([p.clone().detach() for p in self.model.parameters()])\n",
    "\n",
    "        for epoch in range(local_epoch):\n",
    "            train_loss = []\n",
    "            correct = 0\n",
    "            for data, target in self.train_loader:\n",
    "                self.optimizer.zero_grad()\n",
    "                output = self.model(data)\n",
    "                loss = F.nll_loss(output, target)\n",
    "                loss.backward()\n",
    "                self.optimizer.step()\n",
    "                pred = output.data.max(1, keepdim=True)[1]\n",
    "                correct += pred.eq(target.data.view_as(pred)).sum()\n",
    "                train_loss.append(loss.item())\n",
    "            training_accuracy = float(correct / self.train_dataset_length)\n",
    "            training_loss = np.mean(train_loss)\n",
    "            print(\n",
    "                \"<Collab: {:<5}> | Train Round: {:<5} | Local Epoch: {:<3}: FedProx Optimization Train Loss {:<.6f}, Train Acc: {:<.6f} [{}/{}]\".format(\n",
    "                    self.input,\n",
    "                    self.round_number,\n",
    "                    epoch,\n",
    "                    training_loss,\n",
    "                    training_accuracy,\n",
    "                    correct, \n",
    "                    len(self.train_loader.dataset)\n",
    "                )\n",
    "            )\n",
    "\n",
    "        self.next(self.join)\n",
    "    \n",
    "    @aggregator\n",
    "    def join(self, inputs):\n",
    "        \"\"\"\n",
    "        Compute train dataset, and take weighted average of model.\n",
    "        \"\"\"\n",
    "        train_datasize = sum([input_.train_dataset_length for input_ in inputs])\n",
    "        \n",
    "        train_tensors, train_weights = [], [] \n",
    "        for input_ in inputs:\n",
    "            train_weights.append(input_.train_dataset_length / train_datasize)\n",
    "            train_tensors.append([v for k,v in (fa.get_tensor_dict(input_.model)).items()])\n",
    "            keys_list=[k for k,v in (fa.get_tensor_dict(input_.model)).items()]\n",
    "\n",
    "        avg_tensors = self.agg_func.aggregate_models(train_tensors, train_weights)\n",
    "        state_dict = dict(zip(keys_list, avg_tensors))\n",
    "        fa.set_tensor_dict(self.model, state_dict)\n",
    "        \n",
    "        self.next(self.internal_loop)\n",
    "\n",
    "    @aggregator\n",
    "    def internal_loop(self):\n",
    "        \"\"\"\n",
    "        Check if training is finished for `self.n_rounds`\n",
    "        if finished move to end step. Otherwise, go back to start\n",
    "        step for next round of training.\n",
    "        \"\"\"\n",
    "        if self.round_number < self.n_rounds:\n",
    "            self.round_number += 1\n",
    "            self.next(self.start)\n",
    "        else:\n",
    "            self.next(self.end)\n",
    "\n",
    "    @aggregator\n",
    "    def end(self):\n",
    "        \"\"\"\n",
    "        This is the 'end' step.\n",
    "        \"\"\"\n",
    "        self.round_number += 1\n",
    "        print('This is end of the flow')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2aabf61e",
   "metadata": {},
   "source": [
    "You'll notice in the `FederatedFlow` definition above that there were certain attributes that the flow was not initialized with, namely the `train_loader` and `test_loader` for each of the collaborators. These are **private_attributes** that are exposed only throught he runtime. Each participant has it's own set of private attributes: a dictionary where the key is the attribute name, and the value is the object that will be made accessible through that participant's task. \n",
    "\n",
    "Below, we segment shards of the MNIST dataset for **seven collaborators**: Portland, Seattle, Chandler, Bangalore, Guadalajara, Santa Clara and San Jose. Each has their own slice of the dataset that's accessible via the `train_loader` or `test_loader` attribute. Note that the private attributes are flexible, and you can choose to pass in a completely different type of object to any of the collaborators or aggregator (with an arbitrary name). These private attributes will always be filtered out of the current state when transfering from collaborator to aggregator, or vice versa.  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "forward-world",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Setup participants\n",
    "aggregator = Aggregator()\n",
    "aggregator.private_attributes = {}\n",
    "\n",
    "# Setup collaborators with private attributes\n",
    "collaborator_names = ['Portland', 'Seattle', 'Chandler','Bangalore', 'Guadalajara', 'Santa Clara', 'San Jose']\n",
    "collaborators = [Collaborator(name=name) for name in collaborator_names]\n",
    "# Keep a list of collaborator weights. The weights are decided by the number of samples for each collaborator\n",
    "collaborators_weights_dict = {}\n",
    "\n",
    "for idx, collaborator in enumerate(collaborators):\n",
    "    local_train = deepcopy(mnist_train)\n",
    "    local_test = deepcopy(mnist_test)\n",
    "    local_train.data = mnist_train.data[idx::len(collaborators)]\n",
    "    local_train.targets = mnist_train.targets[idx::len(collaborators)]\n",
    "    local_test.data = mnist_test.data[idx::len(collaborators)]\n",
    "    local_test.targets = mnist_test.targets[idx::len(collaborators)]\n",
    "    collaborator.private_attributes = {\n",
    "            'train_loader': torch.utils.data.DataLoader(local_train,batch_size=batch_size_train, shuffle=True),\n",
    "            'test_loader': torch.utils.data.DataLoader(local_test,batch_size=batch_size_train, shuffle=True)\n",
    "    }\n",
    "    collaborators_weights_dict[collaborator] = len(local_train.data)\n",
    "\n",
    "for col in collaborators_weights_dict:\n",
    "    collaborators_weights_dict[col] /= len(mnist_train.data)\n",
    "\n",
    "if len(collaborators_weights_dict) != 0:\n",
    "        assert np.abs(1.0 - sum(collaborators_weights_dict.values())) < 0.01, (\n",
    "            f'Collaborator weights do not sum to 1.0: {collaborators_weights_dict}'\n",
    "        )\n",
    "\n",
    "local_runtime = LocalRuntime(aggregator=aggregator, collaborators=collaborators, backend='single_process')\n",
    "print(f'Local runtime collaborators = {local_runtime.collaborators}')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "278ad46b",
   "metadata": {},
   "source": [
    "Now that we have our flow and runtime defined, let's run the experiment! "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "16937a65",
   "metadata": {},
   "outputs": [],
   "source": [
    "model = None\n",
    "best_model = None\n",
    "optimizer = None\n",
    "n_selected_collaborators = 2\n",
    "n_epochs = 5\n",
    "learning_rate = 0.01\n",
    "weight_decay = 0.001\n",
    "local_epoch = 5\n",
    "\n",
    "# Set `mu` to `1.0` for FedProx\n",
    "mu = 1.0\n",
    "\n",
    "flflow = FedProxFlow(n_selected_collaborators=n_selected_collaborators, n_rounds=n_epochs, checkpoint=False)\n",
    "flflow.runtime = local_runtime\n",
    "\n",
    "flflow.run()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "57ed0b50-5f1c-419c-808d-331cc7b69b69",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}

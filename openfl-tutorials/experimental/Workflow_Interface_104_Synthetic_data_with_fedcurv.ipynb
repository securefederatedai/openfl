{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "14821d97",
   "metadata": {},
   "source": [
    "# Workflow Interface 104: Synthetic Data with Fedcurv implementation\n",
    "[![Open In Colab](https://colab.research.google.com/assets/colab-badge.svg)](https://colab.research.google.com/github/intel/openfl/blob/develop/openfl-tutorials/experimental/Workflow_Interface_104_Synthetic_data_with_fedcurv.ipynb)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a7989e72",
   "metadata": {},
   "source": [
    "In this OpenFL workflow interface tutorial, we'll learn how to implement FedCurv aggregation algorithm using Synthetic dataset. For more information on comparison amongst various aggregation algorithms, visit the [FedProx tutorial]<insert link when ready>."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fc8e35da",
   "metadata": {},
   "source": [
    "# Getting Started"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4dbb89b6",
   "metadata": {},
   "source": [
    "First we start by installing the necessary dependencies for the workflow interface"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f7f98600",
   "metadata": {},
   "outputs": [],
   "source": [
    "# !pip install git+https://github.com/intel/openfl.git\n",
    "# !pip install -r https://raw.githubusercontent.com/intel/openfl/develop/openfl-tutorials/experimental/requirements_workflow_interface.txt\n",
    "\n",
    "# Uncomment this if running in Google Colab\n",
    "#import os\n",
    "#os.environ[\"USERNAME\"] = \"colab\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ac08226b-6127-4387-9c29-3becb93bdbc2",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.utils.data as data\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "\n",
    "import numpy as np\n",
    "\n",
    "import random\n",
    "import collections\n",
    "\n",
    "import warnings\n",
    "warnings.filterwarnings(\"ignore\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0001a45f-9aa6-4a75-8df0-eb57432e09dc",
   "metadata": {},
   "source": [
    "Now we'll generate synthetic dataset and define the Synthetic Dataset class for our experiment."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f4194228-08ec-4a1f-ba76-86161564fe9c",
   "metadata": {},
   "outputs": [],
   "source": [
    "RANDOM_SEED = 10\n",
    "batch_size = 10\n",
    "\n",
    "# Sets seed to reproduce the results\n",
    "def set_seed(seed):\n",
    "    torch.manual_seed(seed)\n",
    "    torch.cuda.manual_seed_all(seed)\n",
    "    torch.use_deterministic_algorithms(True)\n",
    "    torch.backends.cudnn.deterministic = True\n",
    "    torch.backends.cudnn.benchmark = False\n",
    "    torch.backends.cudnn.enabled = False\n",
    "    np.random.seed(seed)\n",
    "    random.seed(seed)\n",
    "\n",
    "# Uncomment the line below for setting seed.\n",
    "# set_seed(RANDOM_SEED)\n",
    "\n",
    "\n",
    "def one_hot(labels, classes):\n",
    "    return np.eye(classes)[labels]\n",
    "\n",
    "\n",
    "def softmax(x):\n",
    "    ex = np.exp(x)\n",
    "    sum_ex = np.sum(np.exp(x))\n",
    "    return ex / sum_ex\n",
    "\n",
    "\n",
    "def generate_synthetic(alpha, beta, iid, num_collaborators, num_classes):\n",
    "    dimension = 60\n",
    "    NUM_CLASS = num_classes\n",
    "    NUM_USER = num_collaborators\n",
    "\n",
    "    samples_per_user = np.random.lognormal(4, 2, (NUM_USER)).astype(int) + 50\n",
    "    num_samples = np.sum(samples_per_user)\n",
    "\n",
    "    X_split = [[] for _ in range(NUM_USER)]\n",
    "    y_split = [[] for _ in range(NUM_USER)]\n",
    "\n",
    "    #### define some eprior ####\n",
    "    mean_W = np.random.normal(0, alpha, NUM_USER)\n",
    "    mean_b = mean_W\n",
    "    B = np.random.normal(0, beta, NUM_USER)\n",
    "    mean_x = np.zeros((NUM_USER, dimension))\n",
    "\n",
    "    diagonal = np.zeros(dimension)\n",
    "    for j in range(dimension):\n",
    "        diagonal[j] = np.power((j + 1), -1.2)\n",
    "    cov_x = np.diag(diagonal)\n",
    "\n",
    "    for i in range(NUM_USER):\n",
    "        if iid == 1:\n",
    "            mean_x[i] = np.ones(dimension) * B[i]  # all zeros\n",
    "        else:\n",
    "            mean_x[i] = np.random.normal(B[i], 1, dimension)\n",
    "\n",
    "    if iid == 1:\n",
    "        W_global = np.random.normal(0, 1, (dimension, NUM_CLASS))\n",
    "        b_global = np.random.normal(0, 1, NUM_CLASS)\n",
    "\n",
    "    for i in range(NUM_USER):\n",
    "\n",
    "        W = np.random.normal(mean_W[i], 1, (dimension, NUM_CLASS))\n",
    "        b = np.random.normal(mean_b[i], 1, NUM_CLASS)\n",
    "\n",
    "        if iid == 1:\n",
    "            W = W_global\n",
    "            b = b_global\n",
    "\n",
    "        xx = np.random.multivariate_normal(\n",
    "            mean_x[i], cov_x, samples_per_user[i])\n",
    "        yy = np.zeros(samples_per_user[i])\n",
    "\n",
    "        for j in range(samples_per_user[i]):\n",
    "            tmp = np.dot(xx[j], W) + b\n",
    "            yy[j] = np.argmax(softmax(tmp))\n",
    "\n",
    "        X_split[i] = xx.tolist()\n",
    "        y_split[i] = yy.tolist()\n",
    "\n",
    "    return X_split, y_split\n",
    "\n",
    "\n",
    "class SyntheticFederatedDataset:\n",
    "    def __init__(self, num_collaborators, batch_size=1, num_classes=10, **kwargs):\n",
    "        self.batch_size = batch_size\n",
    "        X, y = generate_synthetic(0.0, 0.0, 0, num_collaborators, num_classes)\n",
    "        X = [np.array([np.array(sample).astype(np.float32)\n",
    "                      for sample in col]) for col in X]\n",
    "        y = [np.array([np.array(one_hot(int(sample), num_classes))\n",
    "                      for sample in col]) for col in y]\n",
    "        self.X_train_all = np.array([col[:int(0.9 * len(col))] for col in X], dtype=np.ndarray)\n",
    "        self.X_valid_all = np.array([col[int(0.9 * len(col)):] for col in X], dtype=np.ndarray)\n",
    "        self.y_train_all = np.array([col[:int(0.9 * len(col))] for col in y], dtype=np.ndarray)\n",
    "        self.y_valid_all = np.array([col[int(0.9 * len(col)):] for col in y], dtype=np.ndarray)\n",
    "\n",
    "    def split(self, collaborators):\n",
    "        for i, collaborator in enumerate(collaborators):\n",
    "            collaborator.private_attributes = {\n",
    "                \"train_loader\":\n",
    "                    data.DataLoader(\n",
    "                        data.TensorDataset(\n",
    "                            torch.from_numpy(self.X_train_all[i]),\n",
    "                            torch.from_numpy(self.y_train_all[i])\n",
    "                        ), \n",
    "                        batch_size=batch_size, shuffle=True\n",
    "                    ),\n",
    "                \"test_loader\":\n",
    "                    data.DataLoader(\n",
    "                        data.TensorDataset(\n",
    "                            torch.from_numpy(self.X_valid_all[i]),\n",
    "                            torch.from_numpy(self.y_valid_all[i])\n",
    "                        ), \n",
    "                        batch_size=batch_size, shuffle=True\n",
    "                    )\n",
    "            }"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "49081491-3b88-4339-adc0-4f8f5b39cada",
   "metadata": {},
   "source": [
    "Let's now define the model, optimizer and some helper functions."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7e85e030",
   "metadata": {},
   "outputs": [],
   "source": [
    "class Net(nn.Module):\n",
    "    def __init__(self):\n",
    "        # Set RANDOM_SEED to reproduce same model\n",
    "        torch.set_rng_state(torch.manual_seed(RANDOM_SEED).get_state())\n",
    "        super(Net, self).__init__()\n",
    "        self.linear1 = nn.Linear(60, 100)\n",
    "        self.linear2 = nn.Linear(100, 10)\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.linear1(x)\n",
    "        x = self.linear2(x)\n",
    "        return x\n",
    "    \n",
    "def cross_entropy(output, target, size_average=None):\n",
    "    \"\"\"\n",
    "    Binary cross-entropy metric\n",
    "\n",
    "    \"\"\"\n",
    "    return F.cross_entropy(output, torch.max(target, 1)[1], size_average=size_average)\n",
    "\n",
    "\n",
    "def compute_loss_and_acc(network, dataloader):\n",
    "    \"\"\"\n",
    "    Model test method\n",
    "\n",
    "    Args:\n",
    "        network: class Net object (model)\n",
    "        dataloader: torch.utils.data.DataLoader\n",
    "\n",
    "    Returns:\n",
    "        (accuracy,\n",
    "        loss,\n",
    "        correct,\n",
    "        dataloader_size)\n",
    "    \"\"\"\n",
    "    network.eval()\n",
    "    test_loss = 0\n",
    "    correct = 0\n",
    "    with torch.no_grad():\n",
    "        for data, target in dataloader:\n",
    "            output = network(data)\n",
    "            test_loss += cross_entropy(output, target).item()\n",
    "            tar = target.argmax(dim=1, keepdim=True)\n",
    "            pred = output.argmax(dim=1, keepdim=True)\n",
    "            correct += pred.eq(tar).sum().cpu().numpy()\n",
    "    dataloader_size = len(dataloader.dataset)\n",
    "    test_loss /= dataloader_size\n",
    "    accuracy = float(correct / dataloader_size)\n",
    "    return accuracy, test_loss, correct"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cd268911",
   "metadata": {},
   "source": [
    "Next we import the `FLSpec`, `LocalRuntime`, and placement decorators.\n",
    "\n",
    "- `FLSpec` – Defines the flow specification. User defined flows are subclasses of this.\n",
    "- `Runtime` – Defines where the flow runs, infrastructure for task transitions (how information gets sent). The `LocalRuntime` runs the flow on a single node.\n",
    "- `aggregator/collaborator` - placement decorators that define where the task will be assigned\n",
    "\n",
    "In addition to these, we also import `FedCurv` module along with `FedcurvWeightedAvg` aggregation algorithm."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "precise-studio",
   "metadata": {},
   "outputs": [],
   "source": [
    "from copy import deepcopy\n",
    "\n",
    "from openfl.experimental.interface import FLSpec, Aggregator, Collaborator\n",
    "from openfl.experimental.runtime import LocalRuntime\n",
    "from openfl.experimental.placement import aggregator, collaborator\n",
    "\n",
    "from openfl.experimental.interface.aggregation_functions.fedcurv_weighted_average import fedcurv_weighted_average\n",
    "from openfl.experimental.utilities.fedcurv import FedCurv"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3e09ee12-ce1a-43dd-8c17-0403da643a1e",
   "metadata": {},
   "source": [
    "Let us now define the Workflow for our experiment. We use the methodology as provided in quickstart, and define the workflow consisting of following steps:\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "difficult-madrid",
   "metadata": {},
   "outputs": [],
   "source": [
    "class FederatedFlow(FLSpec):\n",
    "\n",
    "    def __init__(self, model = None, optimizer = None, agg_method = None, n_selected_collaborators=10, total_rounds = 10, **kwargs):\n",
    "        super().__init__(**kwargs)\n",
    "        self.n_selected_collaborators = n_selected_collaborators\n",
    "        self.total_rounds = total_rounds\n",
    "        self.round_number = 0\n",
    "        self.total_rounds = total_rounds\n",
    "        if model is not None:\n",
    "            self.model = model\n",
    "            self.optimizer = optimizer\n",
    "            self.agg_method = agg_method\n",
    "        else:\n",
    "            self.model = Net()\n",
    "            self.optimizer = optim.SGD(self.model.parameters(), lr=learning_rate,\n",
    "                               momentum=momentum)\n",
    "            self.agg_method = FedCurv(self.model, importance=1e4)\n",
    "        self.device = 'cpu'\n",
    "        if torch.cuda.is_available():\n",
    "            self.device = 'cuda:0'\n",
    "\n",
    "    @aggregator\n",
    "    def start(self):\n",
    "        print(f'Performing initialization for model')\n",
    "        print(20*\"#\")\n",
    "        print(f\"Round {self.round_number}\")\n",
    "        print(20*\"#\")\n",
    "        self.collaborators = self.runtime.collaborators\n",
    "        self.next(self.compute_loss_and_accuracy,foreach='collaborators')\n",
    "\n",
    "    @collaborator\n",
    "    def compute_loss_and_accuracy(self):\n",
    "        \"\"\"\n",
    "        Compute training accuracy, training loss, aggregated validation accuracy,\n",
    "        aggregated validation loss, \n",
    "        \"\"\"\n",
    "        # Compute Train Loss and Train Acc\n",
    "        self.training_accuracy, self.training_loss, _, = compute_loss_and_acc(\n",
    "            self.model, self.train_loader)\n",
    "        \n",
    "        # Compute Test Loss and Test Acc\n",
    "        self.agg_validation_score, self.agg_validation_loss, test_correct = compute_loss_and_acc(\n",
    "            self.model, self.test_loader)\n",
    "\n",
    "        self.train_dataset_length = len(self.train_loader.dataset)\n",
    "        self.test_dataset_length = len(self.test_loader.dataset)\n",
    "\n",
    "        print(\n",
    "            \"<Collab: {:<5}> | Train Round: {:<5} : Train Loss {:<.6f}, Test Acc: {:<.6f} [{}/{}]\".format(\n",
    "                self.input,\n",
    "                self.round_number,\n",
    "                self.training_loss,\n",
    "                self.agg_validation_score,\n",
    "                test_correct, \n",
    "                self.test_dataset_length\n",
    "            )\n",
    "        )\n",
    "\n",
    "        self.next(self.gather_results_and_take_weighted_average)\n",
    "\n",
    "    @aggregator\n",
    "    def gather_results_and_take_weighted_average(self, inputs):\n",
    "        \"\"\"\n",
    "        Gather results of all collaborators computed in previous \n",
    "        step.\n",
    "        Compute train and test weightes, and compute weighted average of \n",
    "        aggregated training loss, and aggregated test accuracy\n",
    "        \"\"\"\n",
    "        # Calculate train_weights and test_weights\n",
    "        train_datasize, test_datasize = [], []\n",
    "        for input_ in inputs:\n",
    "            train_datasize.append(input_.train_dataset_length)\n",
    "            test_datasize.append(input_.test_dataset_length)\n",
    "\n",
    "        self.train_weights, self.test_weights = [], []\n",
    "        for input_ in inputs:\n",
    "            self.train_weights.append(input_.train_dataset_length / sum(train_datasize))\n",
    "            self.test_weights.append(input_.test_dataset_length / sum(test_datasize))\n",
    "\n",
    "        aggregated_model_accuracy_list, aggregated_model_loss_list = [], []\n",
    "        for input_ in inputs:\n",
    "            aggregated_model_loss_list.append(input_.training_loss)\n",
    "            aggregated_model_accuracy_list.append(input_.agg_validation_score)\n",
    "\n",
    "        # Weighted average of training loss\n",
    "        self.aggregated_model_training_loss = fedcurv_weighted_average(aggregated_model_loss_list, self.train_weights)\n",
    "\n",
    "        # Weighted average of aggregated model accuracy\n",
    "        self.aggregated_model_test_accuracy = fedcurv_weighted_average(aggregated_model_accuracy_list, self.test_weights)\n",
    "        \n",
    "        print(\n",
    "            \"<Agg> | Train Round: {:<5} : Agg Train Loss {:<.6f}, Agg Test Acc: {:<.6f}\".format(\n",
    "                self.round_number,\n",
    "                self.aggregated_model_training_loss,\n",
    "                self.aggregated_model_test_accuracy\n",
    "            )\n",
    "        )\n",
    "\n",
    "        self.next(self.select_collaborators)\n",
    "        \n",
    "    @aggregator\n",
    "    def select_collaborators(self):\n",
    "        \"\"\"\n",
    "        Randomly select n_selected_collaborators collaborator\n",
    "        \"\"\"\n",
    "        np.random.seed(self.round_number)\n",
    "        self.selected_collaborator_indices = np.random.choice(range(len(self.collaborators)), \\\n",
    "            self.n_selected_collaborators, replace=False)\n",
    "        self.selected_collaborators = [self.collaborators[idx] for idx in self.selected_collaborator_indices]\n",
    "\n",
    "        self.next(self.train_selected_collaborators, foreach=\"selected_collaborators\")\n",
    "\n",
    "        \n",
    "    @collaborator\n",
    "    def train_selected_collaborators(self):\n",
    "        \"\"\"\n",
    "        Train selected collaborators\n",
    "        \"\"\"\n",
    "\n",
    "        self.train_dataset_length = len(self.train_loader.dataset)\n",
    "\n",
    "        self.optimizer = optim.SGD(self.model.parameters(), lr=learning_rate,\n",
    "                               momentum=momentum)\n",
    "\n",
    "        self.agg_method.on_train_begin(self.model)\n",
    "        self.model = self.model.to(self.device)\n",
    "        self.model.train(mode=True)\n",
    "        \n",
    "        for epoch in range(local_epoch):\n",
    "            train_loss = []\n",
    "            correct = 0\n",
    "            for data, target in self.train_loader:\n",
    "                data = data.to(self.device)\n",
    "                target = target.to(self.device)\n",
    "                self.optimizer.zero_grad()\n",
    "                output = self.model(data)\n",
    "                loss = cross_entropy(output, target) + self.agg_method.get_penalty(self.model, self.device)\n",
    "                loss.backward()\n",
    "                self.optimizer.step()\n",
    "                pred = output.argmax(dim=1, keepdim=True)\n",
    "                tar = target.argmax(dim=1, keepdim=True)\n",
    "                correct += pred.eq(tar).sum().cpu().numpy()\n",
    "                train_loss.append(loss.item())\n",
    "            training_accuracy = float(correct / self.train_dataset_length)\n",
    "            training_loss = np.mean(train_loss)\n",
    "            print(\n",
    "                \"<Collab: {:<5}> | Train Round: {:<5} | Local Epoch: {:<3}: FedCurv Optimization Train Loss {:<.6f}, Train Acc: {:<.6f} [{}/{}]\".format(\n",
    "                    self.input,\n",
    "                    self.round_number,\n",
    "                    epoch,\n",
    "                    training_loss,\n",
    "                    training_accuracy,\n",
    "                    correct, \n",
    "                    len(self.train_loader.dataset)\n",
    "                )\n",
    "            )\n",
    "            self.agg_method.on_train_end(self.model, self.train_loader, self.device, 'cross_entropy')\n",
    "        self.next(self.join)\n",
    "\n",
    "\n",
    "    @aggregator\n",
    "    def join(self,inputs):\n",
    "        train_datasize = sum([input_.train_dataset_length for input_ in inputs])\n",
    "\n",
    "        train_weights, model_state_dict_list = [], [] \n",
    "        for input_ in inputs:\n",
    "            train_weights.append(input_.train_dataset_length / train_datasize)\n",
    "            model_state_dict_list.append(input_.model.state_dict())\n",
    "        fedcurv_model_dict = fedcurv_weighted_average(model_state_dict_list, train_weights)\n",
    "        self.model.load_state_dict(fedcurv_model_dict)\n",
    "        self.next(self.end)\n",
    " \n",
    "    @aggregator\n",
    "    def end(self):\n",
    "        if self.round_number == self.total_rounds - 1:\n",
    "            print(f'This is the end of the flow')\n",
    "        else:\n",
    "            self.round_number += 1"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "93f94ed7-5dd2-4faf-81a7-418ee973a4ef",
   "metadata": {},
   "source": [
    "****Federation Setup****\n",
    "\n",
    "We'll now setup the federation by defining number of collaborators, initializing dataset and Runtime."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "af23a974-534d-4728-96a9-e0f67f455720",
   "metadata": {},
   "outputs": [],
   "source": [
    "num_collaborators = 30\n",
    "\n",
    "# Setup aggregator\n",
    "aggregator = Aggregator()\n",
    "aggregator.private_attributes = {}\n",
    "\n",
    "# Setup collaborators with private attributes\n",
    "collaborator_names = [f\"col{i}\" for i in range(num_collaborators)]\n",
    "\n",
    "collaborators = [Collaborator(name=name) for name in collaborator_names]\n",
    "\n",
    "synthetic_federated_dataset = SyntheticFederatedDataset(\n",
    "    batch_size=batch_size, num_classes=10, num_collaborators=len(collaborators), seed=RANDOM_SEED)\n",
    "synthetic_federated_dataset.split(collaborators)\n",
    "\n",
    "local_runtime = LocalRuntime(\n",
    "    aggregator=aggregator, collaborators=collaborators, backend=\"single_process\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "89358389-c025-458b-bbec-1ceef2965317",
   "metadata": {},
   "outputs": [],
   "source": [
    "loss_and_acc = {\n",
    "    \"Fedcurv\": {\n",
    "        \"Train Loss\": [], \"Test Accuracy\": []\n",
    "    },\n",
    "    \"FedAvg\": {\n",
    "        \"Train Loss\": [], \"Test Accuracy\": []\n",
    "    }\n",
    "}"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "278ad46b",
   "metadata": {},
   "source": [
    "Now that we have our flow and runtime defined, let's run the experiment! "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "16937a65",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import torch.optim as optim\n",
    "\n",
    "n_selected_collaborators = 10\n",
    "n_epochs = 200\n",
    "local_epoch = 20\n",
    "total_rounds = 5\n",
    "\n",
    "learning_rate = 0.01\n",
    "momentum = 0.5\n",
    "log_interval = 10\n",
    "\n",
    "flflow = FederatedFlow(n_selected_collaborators=n_selected_collaborators,\n",
    "                       total_rounds=total_rounds)\n",
    "\n",
    "flflow.runtime = local_runtime\n",
    "for i in range(n_epochs):\n",
    "    flflow.run()\n",
    "    aggregated_model_training_loss = flflow.aggregated_model_training_loss\n",
    "    aggregated_model_test_accuracy = flflow.aggregated_model_test_accuracy\n",
    "\n",
    "    loss_and_acc[\"Fedcurv\"][\"Train Loss\"].append(aggregated_model_training_loss)\n",
    "    loss_and_acc[\"Fedcurv\"][\"Test Accuracy\"].append(aggregated_model_test_accuracy)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f1daf1fd-7bf6-49cc-9bfe-1eb6359228a3",
   "metadata": {},
   "source": [
    "**Comparison of aggregation algorithms**\n",
    "\n",
    "Now that we have demonstrated Fedcurv on synthetic dataset, let's run through the [FedProx tutorial] to see how Fedcurv compares to FedAvg and FedProx."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b3629e9a-1c55-43c8-84ee-eaaabe0a2112",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}

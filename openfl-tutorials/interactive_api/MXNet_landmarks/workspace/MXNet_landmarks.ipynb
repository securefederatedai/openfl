{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "26fdd9ed",
   "metadata": {},
   "source": [
    "# Federated MXNex Landmarks Tutorial\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dd7fe23e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Install dependencies if not already installed\n",
    "!pip install -r requirements.txt"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "246f9c98",
   "metadata": {},
   "source": [
    "## Connect to the Federation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d657e463",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create a federation\n",
    "from openfl.interface.interactive_api.federation import Federation\n",
    "\n",
    "# please use the same identificator that was used in signed certificate\n",
    "client_id = \"api\"\n",
    "cert_dir = \"cert\"\n",
    "director_node_fqdn = \"localhost\"\n",
    "# 1) Run with API layer - Director mTLS\n",
    "# If the user wants to enable mTLS their must provide CA root chain, and signed key pair to the federation interface\n",
    "# cert_chain = f'{cert_dir}/root_ca.crt'\n",
    "# api_certificate = f'{cert_dir}/{client_id}.crt'\n",
    "# api_private_key = f'{cert_dir}/{client_id}.key'\n",
    "\n",
    "# federation = Federation(client_id=client_id,\n",
    "#                         director_node_fqdn=director_node_fqdn,\n",
    "#                         director_port='50051',\n",
    "#                         cert_chain=cert_chain,\n",
    "#                         api_cert=api_certificate,\n",
    "#                         api_private_key=api_private_key)\n",
    "\n",
    "# --------------------------------------------------------------------------------------------------------------------\n",
    "\n",
    "# 2) Run with TLS disabled (trusted environment)\n",
    "# Federation can also determine local fqdn automatically\n",
    "federation = Federation(\n",
    "    client_id=client_id,\n",
    "    director_node_fqdn=director_node_fqdn,\n",
    "    director_port=\"50051\",\n",
    "    tls=False,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "47dcfab3",
   "metadata": {},
   "outputs": [],
   "source": [
    "shard_registry = federation.get_shard_registry()\n",
    "shard_registry"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "21d89d2c",
   "metadata": {},
   "outputs": [],
   "source": [
    "federation.target_shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a2a6c237",
   "metadata": {},
   "outputs": [],
   "source": [
    "# First, request a dummy_shard_desc that holds information about the federated dataset\n",
    "dummy_shard_desc = federation.get_dummy_shard_descriptor(size=10)\n",
    "dummy_shard_dataset = dummy_shard_desc.get_dataset(\"train\")\n",
    "sample, target = dummy_shard_dataset[0]\n",
    "f\"Sample shape: {sample.shape}, target shape: {target.shape}\""
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cc0dbdbd",
   "metadata": {},
   "source": [
    "## Describing FL experimen"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fc88700a",
   "metadata": {},
   "outputs": [],
   "source": [
    "from openfl.interface.interactive_api.experiment import (\n",
    "    DataInterface,\n",
    "    FLExperiment,\n",
    "    ModelInterface,\n",
    "    TaskInterface,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "feee0dff",
   "metadata": {},
   "outputs": [],
   "source": [
    "import mxnet as mx\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import tqdm\n",
    "from matplotlib import pyplot as plt\n",
    "from mxnet.gluon import data as gdata\n",
    "from mxnet.gluon import loss as gloss\n",
    "from mxnet.gluon import nn"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3b468ae1",
   "metadata": {},
   "source": [
    "### Describe a model and optimizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d3ce192b",
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "MXNet model definition\n",
    "\"\"\"\n",
    "model = nn.Sequential()\n",
    "model.add(\n",
    "    nn.Conv2D(channels=64, kernel_size=3, padding=1, activation=\"relu\"),\n",
    "    nn.BatchNorm(),\n",
    "    nn.MaxPool2D(),\n",
    "    nn.Conv2D(channels=128, kernel_size=3, padding=1, activation=\"relu\"),\n",
    "    nn.BatchNorm(),\n",
    "    nn.MaxPool2D(),\n",
    "    nn.Conv2D(channels=256, kernel_size=3, padding=1, activation=\"relu\"),\n",
    "    nn.BatchNorm(),\n",
    "    nn.MaxPool2D(),\n",
    "    nn.Flatten(),\n",
    "    nn.Dense(64),\n",
    "    nn.Activation(\"relu\"),\n",
    "    nn.Dropout(rate=0.005),\n",
    "    nn.Dense(30),\n",
    ")\n",
    "\n",
    "model.initialize(force_reinit=True, ctx=None, init=mx.init.Xavier())\n",
    "model(\n",
    "    mx.nd.ones((1, 1, 96, 96), ctx=None)\n",
    ")  # first forward pass for weight initialization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "20c39cce",
   "metadata": {},
   "outputs": [],
   "source": [
    "# optimizer\n",
    "optimizer = mx.optimizer.Adam(learning_rate=0.001)\n",
    "trainer = mx.gluon.Trainer(model.collect_params(), optimizer=optimizer)\n",
    "# loss function\n",
    "loss_fn = gloss.L2Loss()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "88b9dbf6",
   "metadata": {},
   "source": [
    "### Register model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d73f5518",
   "metadata": {},
   "outputs": [],
   "source": [
    "framework_adapter = \"mxnet_adapter.FrameworkAdapterPlugin\"\n",
    "\n",
    "MI = ModelInterface(model=model, optimizer=trainer, framework_plugin=framework_adapter)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b0979470",
   "metadata": {},
   "source": [
    "### Register dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d8c9eb50",
   "metadata": {},
   "outputs": [],
   "source": [
    "class LandmarkShardDataset(gdata.Dataset):\n",
    "    def __init__(self, dataset):\n",
    "        self._dataset = dataset\n",
    "\n",
    "    def __len__(self):\n",
    "        self.filelength = len(self._dataset)\n",
    "        return self.filelength\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        return self._dataset[idx]\n",
    "\n",
    "\n",
    "class LandmarkShardDescriptor(DataInterface):\n",
    "    def __init__(self, validation_fraction=1 / 5, **kwargs):\n",
    "        super().__init__(**kwargs)\n",
    "        self.validation_fraction = validation_fraction\n",
    "\n",
    "    @property\n",
    "    def shard_descriptor(self):\n",
    "        return self._shard_descriptor\n",
    "\n",
    "    @shard_descriptor.setter\n",
    "    def shard_descriptor(self, shard_descriptor):\n",
    "        \"\"\"\n",
    "        Describe per-collaborator procedures or sharding.\n",
    "\n",
    "        This method will be called during a collaborator initialization.\n",
    "        Local shard_descriptor will be set by Envoy.\n",
    "        \"\"\"\n",
    "        self._shard_descriptor = shard_descriptor\n",
    "        self._shard_dataset = LandmarkShardDataset(\n",
    "            shard_descriptor.get_dataset(\"train\")\n",
    "        )\n",
    "\n",
    "        self.validation_size = max(\n",
    "            1, int(len(self._shard_dataset) * self.validation_fraction)\n",
    "        )\n",
    "\n",
    "        self.train_indexes = len(self._shard_dataset) - self.validation_size\n",
    "        self.val_indexes = [self.validation_size, self.train_indexes]\n",
    "\n",
    "    def get_train_loader(self):\n",
    "        \"\"\"\n",
    "        Output of this method will be provided to tasks with optimizer in contract\n",
    "        \"\"\"\n",
    "        return gdata.DataLoader(\n",
    "            self._shard_dataset,\n",
    "            batch_size=self.kwargs[\"train_bs\"],\n",
    "            sampler=gdata.RandomSampler(self.train_indexes),\n",
    "            last_batch=\"keep\",\n",
    "        )\n",
    "\n",
    "    def get_valid_loader(self):\n",
    "        \"\"\"\n",
    "        Output of this method will be provided to tasks without optimizer in contract\n",
    "        \"\"\"\n",
    "        return gdata.DataLoader(\n",
    "            self._shard_dataset,\n",
    "            batch_size=self.kwargs[\"valid_bs\"],\n",
    "            sampler=gdata.SequentialSampler(*self.val_indexes),\n",
    "            last_batch=\"keep\",\n",
    "        )\n",
    "\n",
    "    def get_train_data_size(self):\n",
    "        \"\"\"\n",
    "        Information for aggregation\n",
    "        \"\"\"\n",
    "        return self.train_indexes\n",
    "\n",
    "    def get_valid_data_size(self):\n",
    "        \"\"\"\n",
    "        Information for aggregation\n",
    "        \"\"\"\n",
    "        return self.validation_size"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b0dfb459",
   "metadata": {},
   "source": [
    "### Create Mnist federated dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4af5c4c2",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_bs, valid_bs = 64, 64\n",
    "fed_dataset = LandmarkShardDescriptor(train_bs=train_bs, valid_bs=valid_bs)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "849c165b",
   "metadata": {},
   "source": [
    "## Define and register FL tasks"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b9649385",
   "metadata": {},
   "outputs": [],
   "source": [
    "TI = TaskInterface()\n",
    "\n",
    "\n",
    "@TI.register_fl_task(\n",
    "    model=\"model\",\n",
    "    data_loader=\"train_dataset\",\n",
    "    device=\"device\",\n",
    "    optimizer=\"optimizer\",\n",
    "    round_num=\"round_num\",\n",
    ")\n",
    "def train(model, train_dataset, optimizer, round_num, device, loss_fn=loss_fn):\n",
    "    device = (\n",
    "        mx.cpu()\n",
    "        if device.startswith(\"cpu\")\n",
    "        else mx.gpu(int(device.split(\":\")[1].strip()))\n",
    "    )\n",
    "\n",
    "    print(\"train on:\", device)\n",
    "\n",
    "    if round_num == 0:\n",
    "        optimizer._contexts = [device]\n",
    "\n",
    "    train_dataset = tqdm.tqdm(train_dataset, desc=\"train\")\n",
    "    train_sum_l = 0\n",
    "    for X, y in train_dataset:\n",
    "        X, y = X.expand_dims(axis=1).as_in_context(device), y.as_in_context(device)\n",
    "        with mx.autograd.record():\n",
    "            pred = model(X)\n",
    "            l = loss_fn(pred, y).mean()\n",
    "        l.backward()\n",
    "        optimizer.step(train_bs)\n",
    "        train_sum_l += l.mean().asscalar()\n",
    "    train_loss = train_sum_l / len(train_dataset)\n",
    "    return {\n",
    "        \"train_mse\": train_loss,\n",
    "    }\n",
    "\n",
    "\n",
    "@TI.register_fl_task(model=\"model\", data_loader=\"val_dataset\", device=\"device\")\n",
    "def validate(model, val_dataset, device):\n",
    "    device = (\n",
    "        mx.cpu()\n",
    "        if device.startswith(\"cpu\")\n",
    "        else mx.gpu(int(device.split(\":\")[1].strip()))\n",
    "    )\n",
    "\n",
    "    # Run a validation loop at the end of each epoch.\n",
    "    test_sum_l = 0\n",
    "    for X, y in val_dataset:\n",
    "        X, y = X.expand_dims(axis=1).as_in_context(device), y.as_in_context(device)\n",
    "        pred = model(X)\n",
    "        l = loss_fn(pred, y)\n",
    "        test_sum_l += l.mean().asscalar()\n",
    "    test_loss = test_sum_l / len(val_dataset)\n",
    "    return {\n",
    "        \"val_mse\": test_loss,\n",
    "    }"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8f0ebf2d",
   "metadata": {},
   "source": [
    "## Time to start a federated learning experiment"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d41b7896",
   "metadata": {},
   "outputs": [],
   "source": [
    "# create an experimnet in federation\n",
    "experiment_name = \"landmark_experiment\"\n",
    "fl_experiment = FLExperiment(federation=federation, experiment_name=experiment_name)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "41b44de9",
   "metadata": {},
   "outputs": [],
   "source": [
    "# The following command zips the workspace and python requirements to be transfered to collaborator nodes\n",
    "from openfl.utilities.enum_types import DevicePolicy\n",
    "from openfl.utilities.enum_types import OptTreatment\n",
    "\n",
    "fl_experiment.start(\n",
    "    model_provider=MI,\n",
    "    task_keeper=TI,\n",
    "    data_loader=fed_dataset,\n",
    "    rounds_to_train=10,\n",
    "    opt_treatment=OptTreatment.CONTINUE_GLOBAL,\n",
    "    device_assignment_policy=DevicePolicy.CUDA_PREFERRED,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "01fa7cea",
   "metadata": {},
   "outputs": [],
   "source": [
    "fl_experiment.stream_metrics()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e6055103",
   "metadata": {},
   "source": [
    "## Let's have a look at the results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ff804102",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "from zipfile import ZipFile\n",
    "\n",
    "from kaggle.api.kaggle_api_extended import KaggleApi"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "37dc7f56",
   "metadata": {},
   "outputs": [],
   "source": [
    "if not os.path.exists(\"./test\"):\n",
    "    api = KaggleApi()\n",
    "    api.authenticate()\n",
    "    api.competition_download_file(\"facial-keypoints-detection\", \"test.zip\")\n",
    "    with ZipFile(\"test.zip\", \"r\") as zipobj:\n",
    "        zipobj.extractall(\"./test\")\n",
    "    os.remove(\"test.zip\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "08fc3a7a",
   "metadata": {},
   "outputs": [],
   "source": [
    "last_model = fl_experiment.get_last_model()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "13f6cfd7",
   "metadata": {},
   "outputs": [],
   "source": [
    "Test_Dir = \"./test/test.csv\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "796c8e37",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_data(path_to_csv_file):\n",
    "    data_df = pd.read_csv(path_to_csv_file)\n",
    "    data_df.fillna(method=\"ffill\", inplace=True)\n",
    "    labels = data_df.drop(\"Image\", axis=1)\n",
    "    imag, keypoints = [], []\n",
    "    for i in range(data_df.shape[0]):\n",
    "        img = data_df[\"Image\"][i].split(\" \")\n",
    "        img = [\"0\" if x == \"\" else x for x in img]\n",
    "        imag.append(img)\n",
    "        y = labels.iloc[i, :]\n",
    "        keypoints.append(y)\n",
    "\n",
    "    X = np.array(imag, dtype=\"float\").reshape(-1, 96, 96)\n",
    "    y = np.array(keypoints, dtype=\"float\")\n",
    "\n",
    "    return X, y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6ed1ce74",
   "metadata": {},
   "outputs": [],
   "source": [
    "test_imgs, _ = get_data(Test_Dir)  # prepare test dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7cc6bed7",
   "metadata": {},
   "outputs": [],
   "source": [
    "fig = plt.figure(figsize=(10, 10))\n",
    "for i in range(9):\n",
    "    ax = fig.add_subplot(3, 3, i + 1)\n",
    "    in_for_net = (\n",
    "        mx.nd.array([test_imgs[i + 1]]).expand_dims(axis=1).as_in_context(mx.cpu())\n",
    "    )\n",
    "    pred = last_model(in_for_net)[0].asnumpy().reshape(-1, 2)\n",
    "    ax.imshow(test_imgs[i + 1], cmap=\"gray\")\n",
    "    x_cords = pred[:, 0]\n",
    "    y_cords = pred[:, 1]\n",
    "    plt.scatter(x_cords, y_cords, label='Predicted keypoints')\n",
    "plt.legend(bbox_to_anchor=(2.1, 3.4), prop={'size': 12})\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1ddc51e2",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}

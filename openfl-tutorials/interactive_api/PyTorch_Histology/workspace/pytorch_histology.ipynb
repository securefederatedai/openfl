{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "26fdd9ed",
   "metadata": {},
   "source": [
    "# Federated PyTorch Histology Tutorial"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "895288d0",
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip install torchvision==0.8.2"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "246f9c98",
   "metadata": {},
   "source": [
    "## Connect to the Federation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d657e463",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create a federation\n",
    "from openfl.interface.interactive_api.federation import Federation\n",
    "\n",
    "# please use the same identificator that was used in signed certificate\n",
    "client_id = 'api'\n",
    "cert_dir = 'cert'\n",
    "director_node_fqdn = 'localhost'\n",
    "# 1) Run with API layer - Director mTLS \n",
    "# If the user wants to enable mTLS their must provide CA root chain, and signed key pair to the federation interface\n",
    "# cert_chain = f'{cert_dir}/root_ca.crt'\n",
    "# api_certificate = f'{cert_dir}/{client_id}.crt'\n",
    "# api_private_key = f'{cert_dir}/{client_id}.key'\n",
    "\n",
    "# federation = Federation(client_id=client_id, director_node_fqdn=director_node_fqdn, director_port='50051',\n",
    "#                        cert_chain=cert_chain, api_cert=api_certificate, api_private_key=api_private_key)\n",
    "\n",
    "# --------------------------------------------------------------------------------------------------------------------\n",
    "\n",
    "# 2) Run with TLS disabled (trusted environment)\n",
    "# Federation can also determine local fqdn automatically\n",
    "federation = Federation(client_id=client_id, director_node_fqdn=director_node_fqdn, director_port='50051', tls=False)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1abebd90",
   "metadata": {},
   "outputs": [],
   "source": [
    "federation.target_shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "47dcfab3",
   "metadata": {},
   "outputs": [],
   "source": [
    "shard_registry = federation.get_shard_registry()\n",
    "shard_registry"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a2a6c237",
   "metadata": {},
   "outputs": [],
   "source": [
    "# First, request a dummy_shard_desc that holds information about the federated dataset \n",
    "dummy_shard_desc = federation.get_dummy_shard_descriptor(size=10)\n",
    "sample, target = dummy_shard_desc.get_dataset('train')[0]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cc0dbdbd",
   "metadata": {},
   "source": [
    "## Creating a FL experiment using Interactive API"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fc88700a",
   "metadata": {},
   "outputs": [],
   "source": [
    "from openfl.interface.interactive_api.experiment import TaskInterface, DataInterface, ModelInterface, FLExperiment"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b0979470",
   "metadata": {},
   "source": [
    "### Register dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7dda1680",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torchvision\n",
    "from torchvision import transforms as T\n",
    "\n",
    "normalize = T.Normalize(mean=[0.485, 0.456, 0.406],\n",
    "                                 std=[0.229, 0.224, 0.225])\n",
    "\n",
    "augmentation = T.RandomApply(\n",
    "    [T.RandomHorizontalFlip(),\n",
    "     T.RandomRotation(10),\n",
    "     T.RandomResizedCrop(64)], \n",
    "    p=.8\n",
    ")\n",
    "\n",
    "training_transform = T.ToTensor()\n",
    "\n",
    "valid_transform = T.ToTensor()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0314d5bf",
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch.utils.data import Dataset\n",
    "\n",
    "\n",
    "class TransformedDataset(Dataset):\n",
    "    \"\"\"Image Person ReID Dataset.\"\"\"\n",
    "\n",
    "    def __init__(self, dataset, transform=None, target_transform=None):\n",
    "        \"\"\"Initialize Dataset.\"\"\"\n",
    "        self.dataset = dataset\n",
    "        self.transform = transform\n",
    "        self.target_transform = target_transform\n",
    "\n",
    "    def __len__(self):\n",
    "        \"\"\"Length of dataset.\"\"\"\n",
    "        return len(self.dataset)\n",
    "\n",
    "    def __getitem__(self, index):\n",
    "        img, label = self.dataset[index]\n",
    "        label = self.target_transform(label) if self.target_transform else label\n",
    "        img = self.transform(img) if self.transform else img\n",
    "        return img, label\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "01369e3b",
   "metadata": {},
   "outputs": [],
   "source": [
    "class HistologyDataset(DataInterface):\n",
    "    def __init__(self, **kwargs):\n",
    "        self.kwargs = kwargs\n",
    "    \n",
    "    @property\n",
    "    def shard_descriptor(self):\n",
    "        return self._shard_descriptor\n",
    "        \n",
    "    @shard_descriptor.setter\n",
    "    def shard_descriptor(self, shard_descriptor):\n",
    "        \"\"\"\n",
    "        Describe per-collaborator procedures or sharding.\n",
    "\n",
    "        This method will be called during a collaborator initialization.\n",
    "        Local shard_descriptor  will be set by Envoy.\n",
    "        \"\"\"\n",
    "        self._shard_descriptor = shard_descriptor\n",
    "        \n",
    "        self.train_set = TransformedDataset(\n",
    "            self._shard_descriptor.get_dataset('train'),\n",
    "            transform=training_transform\n",
    "        )\n",
    "        self.valid_set = TransformedDataset(\n",
    "            self._shard_descriptor.get_dataset('val'),\n",
    "            transform=valid_transform\n",
    "        )\n",
    "        \n",
    "    def get_train_loader(self, **kwargs):\n",
    "        \"\"\"\n",
    "        Output of this method will be provided to tasks with optimizer in contract\n",
    "        \"\"\"\n",
    "        return DataLoader(\n",
    "            self.train_set, num_workers=8, batch_size=self.kwargs['train_bs'], shuffle=True\n",
    "            )\n",
    "\n",
    "    def get_valid_loader(self, **kwargs):\n",
    "        \"\"\"\n",
    "        Output of this method will be provided to tasks without optimizer in contract\n",
    "        \"\"\"\n",
    "        return DataLoader(self.valid_set, num_workers=8, batch_size=self.kwargs['valid_bs'])\n",
    "\n",
    "    def get_train_data_size(self):\n",
    "        \"\"\"\n",
    "        Information for aggregation\n",
    "        \"\"\"\n",
    "        return len(self.train_set)\n",
    "\n",
    "    def get_valid_data_size(self):\n",
    "        \"\"\"\n",
    "        Information for aggregation\n",
    "        \"\"\"\n",
    "        return len(self.valid_set)\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4a6cedef",
   "metadata": {},
   "outputs": [],
   "source": [
    "fed_dataset = HistologyDataset(train_bs=64, valid_bs=64)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "74cac654",
   "metadata": {},
   "source": [
    "### Describe the model and optimizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4949e16d",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import glob\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "from PIL import Image\n",
    "\n",
    "import numpy as np\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import torch.optim as optim"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "43e25fe3",
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "MobileNetV2 model\n",
    "\"\"\"\n",
    "\n",
    "class Net(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(Net, self).__init__()\n",
    "        conv_kwargs = {'kernel_size': 3, 'stride': 1, 'padding': 1}\n",
    "        self.conv1 = nn.Conv2d(3, 16, **conv_kwargs)\n",
    "        self.conv2 = nn.Conv2d(16, 32, **conv_kwargs)\n",
    "        self.conv3 = nn.Conv2d(32, 64, **conv_kwargs)\n",
    "        self.conv4 = nn.Conv2d(64, 128, **conv_kwargs)\n",
    "        self.conv5 = nn.Conv2d(128 + 32, 256, **conv_kwargs)\n",
    "        self.conv6 = nn.Conv2d(256, 512, **conv_kwargs)\n",
    "        self.conv7 = nn.Conv2d(512 + 128 + 32, 256, **conv_kwargs)\n",
    "        self.conv8 = nn.Conv2d(256, 512, **conv_kwargs)\n",
    "        self.fc1 = nn.Linear(1184 * 9 * 9, 128)\n",
    "        self.fc2 = nn.Linear(128, 8)\n",
    "\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = F.relu(self.conv1(x))\n",
    "        x = F.relu(self.conv2(x))\n",
    "        maxpool = F.max_pool2d(x, 2, 2)\n",
    "\n",
    "        x = F.relu(self.conv3(maxpool))\n",
    "        x = F.relu(self.conv4(x))\n",
    "        concat = torch.cat([maxpool, x], dim=1)\n",
    "        maxpool = F.max_pool2d(concat, 2, 2)\n",
    "\n",
    "        x = F.relu(self.conv5(maxpool))\n",
    "        x = F.relu(self.conv6(x))\n",
    "        concat = torch.cat([maxpool, x], dim=1)\n",
    "        maxpool = F.max_pool2d(concat, 2, 2)\n",
    "\n",
    "        x = F.relu(self.conv7(maxpool))\n",
    "        x = F.relu(self.conv8(x))\n",
    "        concat = torch.cat([maxpool, x], dim=1)\n",
    "        maxpool = F.max_pool2d(concat, 2, 2)\n",
    "\n",
    "        x = maxpool.flatten(start_dim=1)\n",
    "        x = F.dropout(self.fc1(x), p=0.5)\n",
    "        x = self.fc2(x)\n",
    "        return x\n",
    "\n",
    "model_net = Net()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d279b4fb",
   "metadata": {},
   "outputs": [],
   "source": [
    "optimizer_adam = optim.Adam(model_net.parameters(), lr=1e-4)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f097cdc5",
   "metadata": {},
   "source": [
    "### Register model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "06a8cca8",
   "metadata": {},
   "outputs": [],
   "source": [
    "from copy import deepcopy\n",
    "\n",
    "framework_adapter = 'openfl.plugins.frameworks_adapters.pytorch_adapter.FrameworkAdapterPlugin'\n",
    "model_interface = ModelInterface(model=model_net, optimizer=optimizer_adam, framework_plugin=framework_adapter)\n",
    "\n",
    "# Save the initial model state\n",
    "initial_model = deepcopy(model_net)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "849c165b",
   "metadata": {},
   "source": [
    "## Define and register FL tasks"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b9649385",
   "metadata": {},
   "outputs": [],
   "source": [
    "task_interface = TaskInterface()\n",
    "import torch\n",
    "\n",
    "import tqdm\n",
    "\n",
    "# The Interactive API supports registering functions definied in main module or imported.\n",
    "def function_defined_in_notebook(some_parameter):\n",
    "    print(f'Also I accept a parameter and it is {some_parameter}')\n",
    "\n",
    "# Task interface currently supports only standalone functions.\n",
    "@task_interface.add_kwargs(**{'some_parameter': 42})\n",
    "@task_interface.register_fl_task(model='net_model', data_loader='train_loader', \\\n",
    "                     device='device', optimizer='optimizer')     \n",
    "def train(net_model, train_loader, optimizer, device, loss_fn=F.cross_entropy, some_parameter=None):\n",
    "    device = torch.device('cuda')\n",
    "    if not torch.cuda.is_available():\n",
    "        device = 'cpu'\n",
    "    \n",
    "    function_defined_in_notebook(some_parameter)\n",
    "    \n",
    "    train_loader = tqdm.tqdm(train_loader, desc=\"train\")\n",
    "    net_model.train()\n",
    "    net_model.to(device)\n",
    "\n",
    "    losses = []\n",
    "\n",
    "    for data, target in train_loader:\n",
    "        data, target = torch.tensor(data).to(device), torch.tensor(\n",
    "            target).to(device) \n",
    "        optimizer.zero_grad()\n",
    "        output = net_model(data)\n",
    "        loss = loss_fn(output, target)\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        losses.append(loss.detach().cpu().numpy())\n",
    "        \n",
    "    return {'train_loss': np.mean(losses),}\n",
    "\n",
    "\n",
    "@task_interface.register_fl_task(model='net_model', data_loader='val_loader', device='device')     \n",
    "def validate(net_model, val_loader, device):\n",
    "    device = torch.device('cuda')\n",
    "    if not torch.cuda.is_available():\n",
    "        device = 'cpu'\n",
    "    net_model.eval()\n",
    "    net_model.to(device)\n",
    "    \n",
    "    val_loader = tqdm.tqdm(val_loader, desc=\"validate\")\n",
    "    val_score = 0\n",
    "    total_samples = 0\n",
    "\n",
    "    with torch.no_grad():\n",
    "        for data, target in val_loader:\n",
    "            samples = target.shape[0]\n",
    "            total_samples += samples\n",
    "            data, target = torch.tensor(data).to(device), \\\n",
    "                torch.tensor(target).to(device)\n",
    "            output = net_model(data)\n",
    "            pred = output.argmax(dim=1)\n",
    "            val_score += pred.eq(target).sum().cpu().numpy()\n",
    "            \n",
    "    return {'acc': val_score / total_samples,}"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8f0ebf2d",
   "metadata": {},
   "source": [
    "## Time to start a federated learning experiment"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d41b7896",
   "metadata": {},
   "outputs": [],
   "source": [
    "# create an experimnet in federation\n",
    "experiment_name = 'histology_test_experiment'\n",
    "fl_experiment = FLExperiment(federation=federation, experiment_name=experiment_name)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "41b44de9",
   "metadata": {},
   "outputs": [],
   "source": [
    "# The following command zips the workspace and python requirements to be transfered to collaborator nodes\n",
    "fl_experiment.start(\n",
    "    model_provider=model_interface, \n",
    "    task_keeper=task_interface,\n",
    "    data_loader=fed_dataset,\n",
    "    rounds_to_train=5,\n",
    "    opt_treatment='CONTINUE_GLOBAL'\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f3efc78f-57cd-42e0-9398-bdebd596fac5",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get status of the current experiment\n",
    "fl_experiment.get_experiment_status()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "acting-immunology",
   "metadata": {},
   "outputs": [],
   "source": [
    "# If user want to stop IPython session, then reconnect and check how experiment is going\n",
    "# fl_experiment.restore_experiment_state(model_interface)\n",
    "\n",
    "fl_experiment.stream_metrics(tensorboard_logs=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "432afab5-c44b-440a-9d54-1611fb48cf03",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}

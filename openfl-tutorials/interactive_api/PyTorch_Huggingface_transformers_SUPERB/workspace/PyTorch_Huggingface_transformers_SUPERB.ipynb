{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "liquid-jacket",
   "metadata": {},
   "source": [
    "# Federated Audio Classification tutorial with ðŸ¤— Transformers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "alike-sharing",
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip install \"datasets==1.14\" \"transformers==4.11.3\" \"librosa\" \"torch\" \"ipywidgets\" \"numpy==1.21.5\""
   ]
  },
  {
   "cell_type": "markdown",
   "id": "16986f22",
   "metadata": {},
   "source": [
    "# Connect to the Federation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4485ac79",
   "metadata": {},
   "outputs": [],
   "source": [
    "from openfl.interface.interactive_api.federation import Federation\n",
    "\n",
    "client_id = \"frontend\"\n",
    "director_node_fqdn = \"localhost\"\n",
    "director_port = 50050\n",
    "\n",
    "federation = Federation(\n",
    "    client_id=client_id,\n",
    "    director_node_fqdn=director_node_fqdn,\n",
    "    director_port=director_port,\n",
    "    tls=False,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e35802d5",
   "metadata": {},
   "outputs": [],
   "source": [
    "shard_registry = federation.get_shard_registry()\n",
    "shard_registry"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "67ae50de",
   "metadata": {},
   "outputs": [],
   "source": [
    "federation.target_shape"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "obvious-tyler",
   "metadata": {},
   "source": [
    "## Creating a FL experiment using Interactive API"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "rubber-address",
   "metadata": {},
   "outputs": [],
   "source": [
    "from openfl.interface.interactive_api.experiment import (\n",
    "    DataInterface,\n",
    "    FLExperiment,\n",
    "    ModelInterface,\n",
    "    TaskInterface,\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "sustainable-public",
   "metadata": {},
   "source": [
    "### Register dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8d9acb53",
   "metadata": {},
   "outputs": [],
   "source": [
    "import datasets\n",
    "import numpy as np\n",
    "import torch\n",
    "from torch.utils.data import Dataset\n",
    "from transformers import (\n",
    "    AutoFeatureExtractor,\n",
    "    AutoModelForAudioClassification,\n",
    "    Trainer,\n",
    "    TrainingArguments,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1eaecbb1",
   "metadata": {},
   "outputs": [],
   "source": [
    "model_checkpoint = \"facebook/wav2vec2-base\"\n",
    "\n",
    "labels = [\n",
    "    \"yes\",\n",
    "    \"no\",\n",
    "    \"up\",\n",
    "    \"down\",\n",
    "    \"left\",\n",
    "    \"right\",\n",
    "    \"on\",\n",
    "    \"off\",\n",
    "    \"stop\",\n",
    "    \"go\",\n",
    "    \"_silence_\",\n",
    "    \"_unknown_\",\n",
    "]\n",
    "\n",
    "label2id, id2label = dict(), dict()\n",
    "for i, label in enumerate(labels):\n",
    "    label2id[label] = str(i)\n",
    "    id2label[str(i)] = label"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "151fdff2",
   "metadata": {},
   "outputs": [],
   "source": [
    "feature_extractor = AutoFeatureExtractor.from_pretrained(model_checkpoint)\n",
    "max_duration = 1.0\n",
    "\n",
    "\n",
    "def preprocess_function(pre_processed_data):\n",
    "    audio_arrays = pre_processed_data\n",
    "    inputs = feature_extractor(\n",
    "        audio_arrays,\n",
    "        sampling_rate=feature_extractor.sampling_rate,\n",
    "        max_length=int(feature_extractor.sampling_rate * max_duration),\n",
    "        truncation=True,\n",
    "    )\n",
    "\n",
    "    return inputs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "64f37dcf",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "class SuperbShardDataset(Dataset):\n",
    "    def __init__(self, dataset):\n",
    "        self._dataset = dataset\n",
    "\n",
    "    def __getitem__(self, index):\n",
    "        x, y = self._dataset[index]\n",
    "        x = preprocess_function(x)\n",
    "        return {\"input_values\": x[\"input_values\"][0], \"labels\": y}\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self._dataset)\n",
    "\n",
    "\n",
    "class SuperbFedDataset(DataInterface):\n",
    "    def __init__(self, **kwargs):\n",
    "        super().__init__(**kwargs)\n",
    "\n",
    "    @property\n",
    "    def shard_descriptor(self):\n",
    "        return self._shard_descriptor\n",
    "\n",
    "    @shard_descriptor.setter\n",
    "    def shard_descriptor(self, shard_descriptor):\n",
    "        \"\"\"\n",
    "        Describe per-collaborator procedures for sharding.\n",
    "\n",
    "        This method will be called during a collaborator initialization.\n",
    "        Local shard_descriptor will be set by Envoy.\n",
    "        \"\"\"\n",
    "        self._shard_descriptor = shard_descriptor\n",
    "        self.train_set = SuperbShardDataset(\n",
    "            self._shard_descriptor.get_dataset(\"train\"),\n",
    "        )\n",
    "        self.valid_set = SuperbShardDataset(\n",
    "            self._shard_descriptor.get_dataset(\"val\"),\n",
    "        )\n",
    "        self.test_set = SuperbShardDataset(\n",
    "            self._shard_descriptor.get_dataset(\"test\"),\n",
    "        )\n",
    "\n",
    "    def __getitem__(self, index):\n",
    "        return self.shard_descriptor[index]\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.shard_descriptor)\n",
    "\n",
    "    def get_train_loader(self):\n",
    "        return self.train_set\n",
    "\n",
    "    def get_valid_loader(self):\n",
    "        return self.valid_set\n",
    "\n",
    "    def get_train_data_size(self):\n",
    "        return len(self.train_set)\n",
    "\n",
    "    def get_valid_data_size(self):\n",
    "        return len(self.valid_set)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d8df35f5",
   "metadata": {},
   "outputs": [],
   "source": [
    "fed_dataset = SuperbFedDataset()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "caring-distinction",
   "metadata": {},
   "source": [
    "### Describe a model and optimizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "foreign-gospel",
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "Download the pretrained model and fine-tune it. For classification we use the AutoModelForAudioClassification class.\n",
    "\"\"\"\n",
    "\n",
    "num_labels = len(id2label)\n",
    "\n",
    "model = AutoModelForAudioClassification.from_pretrained(\n",
    "    model_checkpoint,\n",
    "    num_labels=num_labels,\n",
    "    label2id=label2id,\n",
    "    id2label=id2label,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0d5afa68-4bd3-43d8-a86d-d59b5cad94bd",
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import AdamW\n",
    "\n",
    "params_to_update = []\n",
    "for param in model.parameters():\n",
    "    if param.requires_grad == True:\n",
    "        params_to_update.append(param)\n",
    "\n",
    "optimizer = AdamW(params_to_update, lr=3e-5)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "caroline-passion",
   "metadata": {},
   "source": [
    "#### Register model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "handled-teens",
   "metadata": {},
   "outputs": [],
   "source": [
    "framework_adapter = (\n",
    "    \"openfl.plugins.frameworks_adapters.pytorch_adapter.FrameworkAdapterPlugin\"\n",
    ")\n",
    "MI = ModelInterface(\n",
    "    model=model, optimizer=optimizer, framework_plugin=framework_adapter\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "portuguese-groove",
   "metadata": {},
   "source": [
    "### Define and register FL tasks"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5d4ff313-a17f-4119-a4c7-afa898b0f304",
   "metadata": {},
   "outputs": [],
   "source": [
    "batch_size = 16\n",
    "args = TrainingArguments(\n",
    "    \"finetuned_model\",\n",
    "    save_strategy=\"epoch\",\n",
    "    per_device_train_batch_size=batch_size,\n",
    "    per_device_eval_batch_size=batch_size,\n",
    "    num_train_epochs=1,\n",
    "    warmup_ratio=0.1,\n",
    "    logging_steps=10,\n",
    "    push_to_hub=False,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fd011594-f16a-4569-ae4e-26977e94b8c2",
   "metadata": {},
   "outputs": [],
   "source": [
    "from datasets import load_metric\n",
    "\n",
    "metric = load_metric(\"accuracy\")\n",
    "\n",
    "\n",
    "def compute_metrics(eval_pred):\n",
    "    \"\"\"Computes accuracy on a batch of predictions\"\"\"\n",
    "    predictions = np.argmax(eval_pred.predictions, axis=1)\n",
    "    return metric.compute(predictions=predictions, references=eval_pred.label_ids)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "increasing-builder",
   "metadata": {},
   "outputs": [],
   "source": [
    "TI = TaskInterface()\n",
    "\n",
    "import torch.nn as nn\n",
    "import tqdm\n",
    "\n",
    "\n",
    "@TI.register_fl_task(\n",
    "    model=\"model\", data_loader=\"train_loader\", device=\"device\", optimizer=\"optimizer\"\n",
    ")\n",
    "def train(model, train_loader, optimizer, device):\n",
    "\n",
    "    print(f\"\\n\\n TASK TRAIN GOT DEVICE {device}\\n\\n\")\n",
    "\n",
    "    trainer = Trainer(\n",
    "        model.to(device),\n",
    "        args,\n",
    "        train_dataset=train_loader,\n",
    "        tokenizer=feature_extractor,\n",
    "        optimizers=(optimizer, None),\n",
    "        compute_metrics=compute_metrics,\n",
    "    )\n",
    "    train_metrics = trainer.train()\n",
    "    return {\"train_loss\": train_metrics.metrics[\"train_loss\"]}\n",
    "\n",
    "\n",
    "@TI.register_fl_task(model=\"model\", data_loader=\"val_loader\", device=\"device\")\n",
    "def validate(model, val_loader, device):\n",
    "\n",
    "    print(f\"\\n\\n TASK VALIDATE GOT DEVICE {device}\\n\\n\")\n",
    "\n",
    "    trainer = Trainer(\n",
    "        model.to(device),\n",
    "        args,\n",
    "        eval_dataset=val_loader,\n",
    "        tokenizer=feature_extractor,\n",
    "        compute_metrics=compute_metrics,\n",
    "    )\n",
    "    eval_metrics = trainer.evaluate()\n",
    "    return {\"eval_accuracy\": eval_metrics[\"eval_accuracy\"]}"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "derived-bride",
   "metadata": {},
   "source": [
    "## Time to start a federated learning experiment"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "mature-renewal",
   "metadata": {},
   "outputs": [],
   "source": [
    "experiment_name = \"HF_audio_test_experiment\"\n",
    "fl_experiment = FLExperiment(federation=federation, experiment_name=experiment_name)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "lightweight-causing",
   "metadata": {},
   "outputs": [],
   "source": [
    "fl_experiment.start(\n",
    "    model_provider=MI,\n",
    "    task_keeper=TI,\n",
    "    data_loader=fed_dataset,\n",
    "    rounds_to_train=2,\n",
    "    opt_treatment=\"CONTINUE_GLOBAL\",\n",
    "    device_assignment_policy=\"CUDA_PREFERRED\",\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f1543a36",
   "metadata": {},
   "outputs": [],
   "source": [
    "fl_experiment.stream_metrics()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}

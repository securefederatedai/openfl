{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "liquid-jacket",
   "metadata": {},
   "source": [
    "# Federated Kvasir with Director example"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "alike-sharing",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: torchvision==0.8.1 in /home/idavidyu/.virtualenvs/corrupt-envoy/lib/python3.8/site-packages (0.8.1)\n",
      "Requirement already satisfied: pillow>=4.1.1 in /home/idavidyu/.virtualenvs/corrupt-envoy/lib/python3.8/site-packages (from torchvision==0.8.1) (9.0.1)\n",
      "Requirement already satisfied: torch==1.7.0 in /home/idavidyu/.virtualenvs/corrupt-envoy/lib/python3.8/site-packages (from torchvision==0.8.1) (1.7.0)\n",
      "Requirement already satisfied: numpy in /home/idavidyu/.virtualenvs/corrupt-envoy/lib/python3.8/site-packages (from torchvision==0.8.1) (1.22.2)\n",
      "Requirement already satisfied: future in /home/idavidyu/.virtualenvs/corrupt-envoy/lib/python3.8/site-packages (from torch==1.7.0->torchvision==0.8.1) (0.18.2)\n",
      "Requirement already satisfied: dataclasses in /home/idavidyu/.virtualenvs/corrupt-envoy/lib/python3.8/site-packages (from torch==1.7.0->torchvision==0.8.1) (0.6)\n",
      "Requirement already satisfied: typing-extensions in /home/idavidyu/.virtualenvs/corrupt-envoy/lib/python3.8/site-packages (from torch==1.7.0->torchvision==0.8.1) (3.10.0.2)\n",
      "\u001b[33mWARNING: You are using pip version 22.0.3; however, version 22.0.4 is available.\n",
      "You should consider upgrading via the '/home/idavidyu/.virtualenvs/corrupt-envoy/bin/python -m pip install --upgrade pip' command.\u001b[0m\u001b[33m\n",
      "\u001b[0m"
     ]
    }
   ],
   "source": [
    "# Install dependencies if not already installed\n",
    "!pip install torchvision==0.8.1"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "16986f22",
   "metadata": {},
   "source": [
    "# Connect to the Federation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "4485ac79",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create a federation\n",
    "from openfl.interface.interactive_api.federation import Federation\n",
    "\n",
    "# please use the same identificator that was used in signed certificate\n",
    "client_id = 'frontend'\n",
    "director_node_fqdn = 'localhost'\n",
    "director_port = 50050\n",
    "\n",
    "# Run with TLS disabled (trusted environment)\n",
    "# Federation can also determine local fqdn automatically\n",
    "federation = Federation(\n",
    "    client_id=client_id,\n",
    "    director_node_fqdn=director_node_fqdn,\n",
    "    director_port=director_port,\n",
    "    tls=False\n",
    ")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "e35802d5",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'env_3': {'shard_info': node_info {\n",
       "    name: \"env_3\"\n",
       "    cuda_devices {\n",
       "      index: 2\n",
       "      memory_total: 11554717696\n",
       "      memory_utilized: 6225920\n",
       "      device_utilization: \"0%\"\n",
       "      cuda_driver_version: \"470.57.02\"\n",
       "      cuda_version: \"11.4\"\n",
       "      name: \"NVIDIA GeForce RTX 2080 Ti\"\n",
       "    }\n",
       "  }\n",
       "  shard_description: \"Kvasir dataset, shard number 3 out of 3\"\n",
       "  sample_shape: \"300\"\n",
       "  sample_shape: \"400\"\n",
       "  sample_shape: \"3\"\n",
       "  target_shape: \"300\"\n",
       "  target_shape: \"400\",\n",
       "  'is_online': True,\n",
       "  'is_experiment_running': False,\n",
       "  'last_updated': '2022-03-14 17:14:06',\n",
       "  'current_time': '2022-03-14 17:14:08',\n",
       "  'valid_duration': seconds: 10,\n",
       "  'experiment_name': 'ExperimentName Mock'},\n",
       " 'env_1': {'shard_info': node_info {\n",
       "    name: \"env_1\"\n",
       "    cuda_devices {\n",
       "      memory_total: 11554717696\n",
       "      memory_utilized: 6225920\n",
       "      device_utilization: \"0%\"\n",
       "      cuda_driver_version: \"470.57.02\"\n",
       "      cuda_version: \"11.4\"\n",
       "      name: \"NVIDIA GeForce RTX 2080 Ti\"\n",
       "    }\n",
       "  }\n",
       "  shard_description: \"Kvasir dataset, shard number 1 out of 3\"\n",
       "  sample_shape: \"300\"\n",
       "  sample_shape: \"400\"\n",
       "  sample_shape: \"3\"\n",
       "  target_shape: \"300\"\n",
       "  target_shape: \"400\",\n",
       "  'is_online': True,\n",
       "  'is_experiment_running': False,\n",
       "  'last_updated': '2022-03-14 17:14:06',\n",
       "  'current_time': '2022-03-14 17:14:08',\n",
       "  'valid_duration': seconds: 10,\n",
       "  'experiment_name': 'ExperimentName Mock'},\n",
       " 'env_2': {'shard_info': node_info {\n",
       "    name: \"env_2\"\n",
       "    cuda_devices {\n",
       "      index: 1\n",
       "      memory_total: 11546394624\n",
       "      memory_utilized: 49283072\n",
       "      device_utilization: \"0%\"\n",
       "      cuda_driver_version: \"470.57.02\"\n",
       "      cuda_version: \"11.4\"\n",
       "      name: \"NVIDIA GeForce RTX 2080 Ti\"\n",
       "    }\n",
       "  }\n",
       "  shard_description: \"Kvasir dataset, shard number 2 out of 3\"\n",
       "  sample_shape: \"300\"\n",
       "  sample_shape: \"400\"\n",
       "  sample_shape: \"3\"\n",
       "  target_shape: \"300\"\n",
       "  target_shape: \"400\",\n",
       "  'is_online': True,\n",
       "  'is_experiment_running': False,\n",
       "  'last_updated': '2022-03-14 17:14:06',\n",
       "  'current_time': '2022-03-14 17:14:08',\n",
       "  'valid_duration': seconds: 10,\n",
       "  'experiment_name': 'ExperimentName Mock'}}"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "federation.get_shard_registry()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "67ae50de",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['300', '400']"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "federation.target_shape"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "obvious-tyler",
   "metadata": {},
   "source": [
    "## Creating a FL experiment using Interactive API"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "rubber-address",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/idavidyu/.virtualenvs/corrupt-envoy/lib/python3.8/site-packages/tqdm/auto.py:22: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    }
   ],
   "source": [
    "from openfl.interface.interactive_api.experiment import TaskInterface, DataInterface, ModelInterface, FLExperiment"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "sustainable-public",
   "metadata": {},
   "source": [
    "### Register dataset"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "unlike-texas",
   "metadata": {},
   "source": [
    "We extract User dataset class implementation.\n",
    "Is it convinient?\n",
    "What if the dataset is not a class?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "64f37dcf",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import PIL\n",
    "import numpy as np\n",
    "from torch.utils.data import Dataset, DataLoader, SubsetRandomSampler\n",
    "from torchvision import transforms as tsf\n",
    "\n",
    "\n",
    "class KvasirShardDataset(Dataset):\n",
    "    \n",
    "    def __init__(self, dataset):\n",
    "        self._dataset = dataset\n",
    "        \n",
    "        # Prepare transforms\n",
    "        self.img_trans = tsf.Compose([\n",
    "            tsf.ToPILImage(),\n",
    "            tsf.Resize((332, 332)),\n",
    "            tsf.ToTensor(),\n",
    "            tsf.Normalize(mean=[0.5, 0.5, 0.5], std=[0.5, 0.5, 0.5])])\n",
    "        self.mask_trans = tsf.Compose([\n",
    "            tsf.ToPILImage(),\n",
    "            tsf.Resize((332, 332), interpolation=PIL.Image.NEAREST),\n",
    "            tsf.ToTensor()])\n",
    "        \n",
    "    def __getitem__(self, index):\n",
    "        img, mask = self._dataset[index]\n",
    "        img = self.img_trans(img).numpy()\n",
    "        mask = self.mask_trans(mask).numpy()\n",
    "        return img, mask\n",
    "    \n",
    "    def __len__(self):\n",
    "        return len(self._dataset)\n",
    "\n",
    "    \n",
    "\n",
    "# Now you can implement you data loaders using dummy_shard_desc\n",
    "class KvasirSD(DataInterface):\n",
    "\n",
    "    def __init__(self, validation_fraction=1/8, **kwargs):\n",
    "        super().__init__(**kwargs)\n",
    "        \n",
    "        self.validation_fraction = validation_fraction\n",
    "        \n",
    "    @property\n",
    "    def shard_descriptor(self):\n",
    "        return self._shard_descriptor\n",
    "        \n",
    "    @shard_descriptor.setter\n",
    "    def shard_descriptor(self, shard_descriptor):\n",
    "        \"\"\"\n",
    "        Describe per-collaborator procedures or sharding.\n",
    "\n",
    "        This method will be called during a collaborator initialization.\n",
    "        Local shard_descriptor  will be set by Envoy.\n",
    "        \"\"\"\n",
    "        self._shard_descriptor = shard_descriptor\n",
    "        self._shard_dataset = KvasirShardDataset(shard_descriptor.get_dataset('train'))\n",
    "        \n",
    "        validation_size = max(1, int(len(self._shard_dataset) * self.validation_fraction))\n",
    "        \n",
    "        self.train_indeces = np.arange(len(self._shard_dataset) - validation_size)\n",
    "        self.val_indeces = np.arange(len(self._shard_dataset) - validation_size, len(self._shard_dataset))\n",
    "    \n",
    "    def get_train_loader(self, **kwargs):\n",
    "        \"\"\"\n",
    "        Output of this method will be provided to tasks with optimizer in contract\n",
    "        \"\"\"\n",
    "        train_sampler = SubsetRandomSampler(self.train_indeces)\n",
    "        return DataLoader(\n",
    "            self._shard_dataset,\n",
    "            num_workers=8,\n",
    "            batch_size=self.kwargs['train_bs'],\n",
    "            sampler=train_sampler\n",
    "        )\n",
    "\n",
    "    def get_valid_loader(self, **kwargs):\n",
    "        \"\"\"\n",
    "        Output of this method will be provided to tasks without optimizer in contract\n",
    "        \"\"\"\n",
    "        val_sampler = SubsetRandomSampler(self.val_indeces)\n",
    "        return DataLoader(\n",
    "            self._shard_dataset,\n",
    "            num_workers=8,\n",
    "            batch_size=self.kwargs['valid_bs'],\n",
    "            sampler=val_sampler\n",
    "        )\n",
    "\n",
    "    def get_train_data_size(self):\n",
    "        \"\"\"\n",
    "        Information for aggregation\n",
    "        \"\"\"\n",
    "        return len(self.train_indeces)\n",
    "\n",
    "    def get_valid_data_size(self):\n",
    "        \"\"\"\n",
    "        Information for aggregation\n",
    "        \"\"\"\n",
    "        return len(self.val_indeces)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "caring-distinction",
   "metadata": {},
   "source": [
    "### Describe a model and optimizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "visible-victor",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/idavidyu/.virtualenvs/corrupt-envoy/lib/python3.8/site-packages/tqdm/auto.py:22: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "foreign-gospel",
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "UNet model definition\n",
    "\"\"\"\n",
    "from layers import soft_dice_coef, soft_dice_loss, DoubleConv, Down, Up\n",
    "\n",
    "\n",
    "class UNet(nn.Module):\n",
    "    def __init__(self, n_channels=3, n_classes=1):\n",
    "        super().__init__()\n",
    "        self.inc = DoubleConv(n_channels, 64)\n",
    "        self.down1 = Down(64, 128)\n",
    "        self.down2 = Down(128, 256)\n",
    "        self.down3 = Down(256, 512)\n",
    "        self.up1 = Up(512, 256)\n",
    "        self.up2 = Up(256, 128)\n",
    "        self.up3 = Up(128, 64)\n",
    "        self.outc = nn.Conv2d(64, n_classes, 1)\n",
    "\n",
    "    def forward(self, x):\n",
    "        x1 = self.inc(x)\n",
    "        x2 = self.down1(x1)\n",
    "        x3 = self.down2(x2)\n",
    "        x4 = self.down3(x3)\n",
    "        x = self.up1(x4, x3)\n",
    "        x = self.up2(x, x2)\n",
    "        x = self.up3(x, x1)\n",
    "        x = self.outc(x)\n",
    "        x = torch.sigmoid(x)\n",
    "        return x\n",
    "    \n",
    "model_unet = UNet()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "greater-activation",
   "metadata": {},
   "outputs": [],
   "source": [
    "optimizer_adam = optim.Adam(model_unet.parameters(), lr=1e-4,)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "caroline-passion",
   "metadata": {},
   "source": [
    "#### Register model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "handled-teens",
   "metadata": {},
   "outputs": [],
   "source": [
    "framework_adapter = 'openfl.plugins.frameworks_adapters.pytorch_adapter.FrameworkAdapterPlugin'\n",
    "MI = ModelInterface(model=model_unet, optimizer=optimizer_adam, framework_plugin=framework_adapter)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c3fd3f69",
   "metadata": {},
   "source": [
    "### Choose an aggregation function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9fd4c499",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from openfl.component.aggregation_functions import Median, WeightedAverage, AggregationFunction\n",
    "\n",
    "#The Interactive API supports overriding of the aggregation function\n",
    "class One_Good_Envoy(AggregationFunction):\n",
    "    def __init__(self, col_name='3', weight_scale: float = 0.5):\n",
    "        self.good_col = col_name\n",
    "        self.weight_scale = weight_scale\n",
    "\n",
    "    def call(self, local_tensors, *_) -> np.ndarray:\n",
    "        weights = [x.weight if self.good_col in x.col_name else x.weight * self.weight_scale\n",
    "                   for x in local_tensors]\n",
    "        tensors = np.array([x.tensor for x in local_tensors])\n",
    "        return np.average(tensors, weights=weights, axis=0)\n",
    "    \n",
    "    \n",
    "# aggregation_function = One_Good_Envoy()\n",
    "aggregation_function = WeightedAverage()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "portuguese-groove",
   "metadata": {},
   "source": [
    "### Define and register FL tasks"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "increasing-builder",
   "metadata": {},
   "outputs": [],
   "source": [
    "TI = TaskInterface()\n",
    "import torch\n",
    "import tqdm\n",
    "\n",
    "\n",
    "@TI.register_fl_task(model='unet_model', data_loader='train_loader', \\\n",
    "                     device='device', optimizer='optimizer')     \n",
    "@TI.set_aggregation_function(aggregation_function)\n",
    "def train(unet_model, train_loader, optimizer, device, loss_fn=soft_dice_loss):\n",
    "    \n",
    "    \"\"\"    \n",
    "    The following constructions, that may lead to resource race\n",
    "    is no longer needed:\n",
    "    \n",
    "    if not torch.cuda.is_available():\n",
    "        device = 'cpu'\n",
    "    else:\n",
    "        device = 'cuda'\n",
    "        \n",
    "    \"\"\"\n",
    "\n",
    "    print(f'\\n\\n TASK TRAIN GOT DEVICE {device}\\n\\n')\n",
    "        \n",
    "    train_loader = tqdm.tqdm(train_loader, desc=\"train\")\n",
    "    \n",
    "    unet_model.train()\n",
    "    unet_model.to(device)\n",
    "\n",
    "    losses = []\n",
    "\n",
    "    for data, target in train_loader:\n",
    "        data, target = torch.tensor(data).to(device), torch.tensor(\n",
    "            target).to(device, dtype=torch.float32)\n",
    "        optimizer.zero_grad()\n",
    "        output = unet_model(data)\n",
    "        loss = loss_fn(output=output, target=target)\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        losses.append(loss.detach().cpu().numpy())\n",
    "        \n",
    "    return {'train_loss': np.mean(losses),}\n",
    "\n",
    "\n",
    "@TI.register_fl_task(model='unet_model', data_loader='val_loader', device='device')     \n",
    "def validate(unet_model, val_loader, device):\n",
    "    print(f'\\n\\n TASK VALIDATE GOT DEVICE {device}\\n\\n')\n",
    "    \n",
    "    unet_model.eval()\n",
    "    unet_model.to(device)\n",
    "    \n",
    "    val_loader = tqdm.tqdm(val_loader, desc=\"validate\")\n",
    "\n",
    "    val_score = 0\n",
    "    total_samples = 0\n",
    "\n",
    "    with torch.no_grad():\n",
    "        for data, target in val_loader:\n",
    "            samples = target.shape[0]\n",
    "            total_samples += samples\n",
    "            data, target = torch.tensor(data).to(device), \\\n",
    "                torch.tensor(target).to(device, dtype=torch.int64)\n",
    "            output = unet_model(data)\n",
    "            val = soft_dice_coef(output, target)\n",
    "            val_score += val.sum().cpu().numpy()\n",
    "            \n",
    "    return {'dice_coef': val_score / total_samples,}"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}

{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "liquid-jacket",
   "metadata": {},
   "source": [
    "# Federated GAN tutorial with PyTorch Lightning"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "alike-sharing",
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip install \"pytorch-lightning>=1.3\" \"torch==1.9.1\" \"torchvision==0.10.1\" \"torchmetrics>=0.3\" \"scikit-image\" \"matplotlib\""
   ]
  },
  {
   "cell_type": "markdown",
   "id": "16986f22",
   "metadata": {},
   "source": [
    "# Connect to the Federation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4485ac79",
   "metadata": {},
   "outputs": [],
   "source": [
    "from openfl.interface.interactive_api.federation import Federation\n",
    "\n",
    "client_id = \"frontend\"\n",
    "director_node_fqdn = \"localhost\"\n",
    "director_port = 50050\n",
    "\n",
    "#Run with TLS disabled (trusted environment)\n",
    "federation = Federation(\n",
    "    client_id=client_id,\n",
    "    director_node_fqdn=director_node_fqdn,\n",
    "    director_port=director_port,\n",
    "    tls=False,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e35802d5",
   "metadata": {},
   "outputs": [],
   "source": [
    "shard_registry = federation.get_shard_registry()\n",
    "shard_registry"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "67ae50de",
   "metadata": {},
   "outputs": [],
   "source": [
    "federation.target_shape"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "obvious-tyler",
   "metadata": {},
   "source": [
    "## Creating a FL experiment using Interactive API"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "rubber-address",
   "metadata": {},
   "outputs": [],
   "source": [
    "from openfl.interface.interactive_api.experiment import (\n",
    "    DataInterface,\n",
    "    FLExperiment,\n",
    "    ModelInterface,\n",
    "    TaskInterface,\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "sustainable-public",
   "metadata": {},
   "source": [
    "### Register dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8d9acb53",
   "metadata": {},
   "outputs": [],
   "source": [
    "import copy\n",
    "import os\n",
    "import shutil\n",
    "import PIL\n",
    "from collections import OrderedDict\n",
    "\n",
    "import numpy as np\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import torch.optim as optim\n",
    "import torchvision\n",
    "import torchvision.transforms as transforms\n",
    "from pytorch_lightning import LightningDataModule, LightningModule, Trainer\n",
    "from torch.utils.data import DataLoader, Dataset, random_split\n",
    "from torchvision.datasets import MNIST"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "64f37dcf",
   "metadata": {},
   "outputs": [],
   "source": [
    "mnist_transform = transforms.Compose(\n",
    "    [\n",
    "        transforms.ToPILImage(),\n",
    "        transforms.Resize((28, 28)),\n",
    "        transforms.ToTensor(),\n",
    "        transforms.Normalize((0.1307,), (0.3081,)),\n",
    "    ]\n",
    ")\n",
    "\n",
    "\n",
    "class MnistShardDataset(Dataset):\n",
    "    def __init__(self, x, y, transform=None):\n",
    "        self.x, self.y = x, y\n",
    "        self.transform = transform\n",
    "\n",
    "    def __getitem__(self, index):\n",
    "        x, y = self.x[index], self.y[index]\n",
    "        x = self.transform(x).numpy()\n",
    "        y = y.numpy()\n",
    "        return x, y\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.x)\n",
    "\n",
    "\n",
    "class MnistFedDataset(DataInterface):\n",
    "    def __init__(self, **kwargs):\n",
    "        super().__init__(**kwargs)\n",
    "\n",
    "    @property\n",
    "    def shard_descriptor(self):\n",
    "        return self._shard_descriptor\n",
    "\n",
    "    @shard_descriptor.setter\n",
    "    def shard_descriptor(self, shard_descriptor):\n",
    "        \"\"\"\n",
    "        Describe per-collaborator procedures or sharding.\n",
    "\n",
    "        This method will be called during a collaborator initialization.\n",
    "        Local shard_descriptor will be set by Envoy.\n",
    "        \"\"\"\n",
    "        self._shard_descriptor = shard_descriptor\n",
    "        self.train_set = MnistShardDataset(\n",
    "            self._shard_descriptor.get_dataset(\"train\")[:][0],\n",
    "            self._shard_descriptor.get_dataset(\"train\")[:][1],\n",
    "            transform=mnist_transform,\n",
    "        )\n",
    "        self.valid_set = MnistShardDataset(\n",
    "            self._shard_descriptor.get_dataset(\"val\")[:][0],\n",
    "            self._shard_descriptor.get_dataset(\"val\")[:][1],\n",
    "            transform=mnist_transform,\n",
    "        )\n",
    "\n",
    "    def __getitem__(self, index):\n",
    "        return self.shard_descriptor[index]\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.shard_descriptor)\n",
    "\n",
    "    def get_train_loader(self):\n",
    "        \"\"\"\n",
    "        Output of this method will be provided to tasks with optimizer in contract\n",
    "        \"\"\"\n",
    "        if self.kwargs[\"train_bs\"]:\n",
    "            batch_size = self.kwargs[\"train_bs\"]\n",
    "        else:\n",
    "            batch_size = 256\n",
    "        return DataLoader(self.train_set, batch_size=batch_size, num_workers=4)\n",
    "\n",
    "    def get_valid_loader(self):\n",
    "        \"\"\"\n",
    "        Output of this method will be provided to tasks without optimizer in contract\n",
    "        \"\"\"\n",
    "        if self.kwargs[\"valid_bs\"]:\n",
    "            batch_size = self.kwargs[\"valid_bs\"]\n",
    "        else:\n",
    "            batch_size = 64\n",
    "        return DataLoader(self.valid_set, batch_size=batch_size)\n",
    "\n",
    "    def get_train_data_size(self):\n",
    "        \"\"\"\n",
    "        train data size\n",
    "        \"\"\"\n",
    "\n",
    "        return len(self.train_set)\n",
    "\n",
    "    def get_valid_data_size(self):\n",
    "        \"\"\"\n",
    "        val data size\n",
    "        \"\"\"\n",
    "        return len(self.valid_set)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d8df35f5",
   "metadata": {},
   "outputs": [],
   "source": [
    "fed_dataset = MnistFedDataset(train_bs=256, valid_bs=64)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "caring-distinction",
   "metadata": {},
   "source": [
    "### Describe a model and optimizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "foreign-gospel",
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "Generator and discriminator model definition\n",
    "\"\"\"\n",
    "\n",
    "\n",
    "class Generator(nn.Module):\n",
    "    def __init__(self, latent_dim, img_shape):\n",
    "        super().__init__()\n",
    "        self.img_shape = img_shape\n",
    "\n",
    "        def block(in_feat, out_feat, normalize=True):\n",
    "            layers = [nn.Linear(in_feat, out_feat)]\n",
    "            if normalize:\n",
    "                layers.append(nn.BatchNorm1d(out_feat, 0.8))\n",
    "            layers.append(nn.LeakyReLU(0.2, inplace=True))\n",
    "            return layers\n",
    "\n",
    "        self.model = nn.Sequential(\n",
    "            *block(latent_dim, 128, normalize=False),\n",
    "            *block(128, 256),\n",
    "            *block(256, 512),\n",
    "            *block(512, 1024),\n",
    "            nn.Linear(1024, int(np.prod(img_shape))),\n",
    "            nn.Tanh(),\n",
    "        )\n",
    "\n",
    "    def forward(self, z):\n",
    "        z = z.float()\n",
    "        img = self.model(z)\n",
    "        img = img.view(img.size(0), *self.img_shape)\n",
    "        return img"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "981b810c",
   "metadata": {},
   "outputs": [],
   "source": [
    "class Discriminator(nn.Module):\n",
    "    def __init__(self, img_shape):\n",
    "        super().__init__()\n",
    "\n",
    "        self.model = nn.Sequential(\n",
    "            nn.Linear(int(np.prod(img_shape)), 512),\n",
    "            nn.LeakyReLU(0.2, inplace=True),\n",
    "            nn.Linear(512, 256),\n",
    "            nn.LeakyReLU(0.2, inplace=True),\n",
    "            nn.Linear(256, 1),\n",
    "            nn.Sigmoid(),\n",
    "        )\n",
    "\n",
    "    def forward(self, img):\n",
    "        img_flat = img.view(img.size(0), -1)\n",
    "        img_flat = img_flat.float()\n",
    "        validity = self.model(img_flat)\n",
    "\n",
    "        return validity"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6cc92e98",
   "metadata": {},
   "outputs": [],
   "source": [
    "class GAN(LightningModule):\n",
    "    def __init__(\n",
    "        self,\n",
    "        channels,\n",
    "        width,\n",
    "        height,\n",
    "        train_disc_only,\n",
    "        train_gen_only,\n",
    "        latent_dim: int = 100,\n",
    "        lr: float = 0.0002,\n",
    "        b1: float = 0.5,\n",
    "        b2: float = 0.999,\n",
    "        batch_size: int = 256,\n",
    "        **kwargs\n",
    "    ):\n",
    "        super().__init__()\n",
    "        self.save_hyperparameters()\n",
    "\n",
    "        data_shape = (channels, width, height)\n",
    "        self.generator = Generator(\n",
    "            latent_dim=self.hparams.latent_dim, img_shape=data_shape\n",
    "        )\n",
    "        self.discriminator = Discriminator(img_shape=data_shape)\n",
    "\n",
    "        self.validation_z = torch.randn(8, self.hparams.latent_dim)\n",
    "        self.example_input_array = torch.zeros(2, self.hparams.latent_dim)\n",
    "        self.train_disc_only = train_disc_only\n",
    "        self.train_gen_only = train_gen_only\n",
    "\n",
    "    def forward(self, z):\n",
    "        return self.generator(z)\n",
    "\n",
    "    def adversarial_loss(self, y_hat, y):\n",
    "        return F.binary_cross_entropy(y_hat, y)\n",
    "\n",
    "    def training_step(self, batch, batch_idx, optimizer_idx):\n",
    "        imgs, _ = batch\n",
    "\n",
    "        # sample noise\n",
    "        z = torch.randn(imgs.shape[0], self.hparams.latent_dim)\n",
    "        z = z.type_as(imgs)\n",
    "\n",
    "        if optimizer_idx == 0 and self.train_disc_only == 0:\n",
    "            return self.train_generator(imgs, z, display_images=0)\n",
    "\n",
    "        elif optimizer_idx == 1 and self.train_gen_only == 0:\n",
    "            return self.train_discriminator(imgs, z)\n",
    "\n",
    "    def train_generator(self, imgs, z, display_images=0):\n",
    "        self.generated_imgs = self(z)\n",
    "        sample_imgs = self.generated_imgs[:10]\n",
    "        sample_imgs = np.reshape(sample_imgs.detach().cpu().numpy(), (10, 28, 28, 1))\n",
    "            \n",
    "        if display_images:\n",
    "            from skimage import data, io\n",
    "            from matplotlib import pyplot as plt\n",
    "            for img in sample_imgs:\n",
    "                io.imshow(img.reshape((28, 28)), cmap='gray_r')\n",
    "                plt.axis('off')\n",
    "                plt.show()\n",
    "\n",
    "        valid = torch.ones(imgs.size(0), 1)\n",
    "        valid = valid.type_as(imgs).float()\n",
    "\n",
    "        g_loss = self.adversarial_loss(self.discriminator(self(z)), valid)\n",
    "        tqdm_dict = {\"g_loss\": g_loss}\n",
    "        output = OrderedDict(\n",
    "            {\"loss\": g_loss, \"progress_bar\": tqdm_dict, \"log\": tqdm_dict}\n",
    "        )\n",
    "        self.log(name=\"Generator training loss\", value=g_loss, on_epoch=True)\n",
    "        return output\n",
    "\n",
    "    def train_discriminator(self, imgs, z):\n",
    "        valid = torch.ones(imgs.size(0), 1)\n",
    "        valid = valid.type_as(imgs).float()\n",
    "\n",
    "        real_loss = self.adversarial_loss(self.discriminator(imgs), valid)\n",
    "\n",
    "        fake = torch.zeros(imgs.size(0), 1)\n",
    "        fake = fake.type_as(imgs).float()\n",
    "\n",
    "        fake_loss = self.adversarial_loss(self.discriminator(self(z).detach()), fake)\n",
    "\n",
    "        d_loss = (real_loss + fake_loss) / 2\n",
    "        tqdm_dict = {\"d_loss\": d_loss}\n",
    "        output = OrderedDict(\n",
    "            {\"loss\": d_loss, \"progress_bar\": tqdm_dict, \"log\": tqdm_dict}\n",
    "        )\n",
    "        self.log(name=\"Discriminator training loss\", value=d_loss, on_epoch=True)\n",
    "        return output\n",
    "\n",
    "    def configure_optimizers(self):\n",
    "        lr = self.hparams.lr\n",
    "        b1 = self.hparams.b1\n",
    "        b2 = self.hparams.b2\n",
    "\n",
    "        opt_g = torch.optim.Adam(self.generator.parameters(), lr=lr, betas=(b1, b2))\n",
    "        opt_d = torch.optim.Adam(self.discriminator.parameters(), lr=lr, betas=(b1, b2))\n",
    "\n",
    "        return [opt_g, opt_d]\n",
    "\n",
    "    def validation_step(self, batch, batch_idx, optimizer_idx=1):\n",
    "        imgs, _ = batch\n",
    "\n",
    "        valid = torch.ones(imgs.size(0), 1)\n",
    "        valid = valid.type_as(imgs).float()\n",
    "\n",
    "        val_real_loss = self.adversarial_loss(self.discriminator(imgs), valid)\n",
    "        self.log(name=\"Discriminator val loss\", value=val_real_loss, on_epoch=True)\n",
    "        return {\"val_loss\": val_real_loss}\n",
    "\n",
    "    def on_epoch_end(self):\n",
    "        z = self.validation_z.type_as(self.generator.model[0].weight)\n",
    "\n",
    "        sample_imgs = self(z)\n",
    "        grid = torchvision.utils.make_grid(sample_imgs)\n",
    "        self.logger.experiment.add_image(\"generated_images\", grid, self.current_epoch)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5719fa50",
   "metadata": {},
   "outputs": [],
   "source": [
    "from pytorch_lightning.callbacks import Callback\n",
    "\n",
    "\n",
    "class MetricsCallback(Callback):\n",
    "    \"\"\"PyTorch Lightning metric callback.\"\"\"\n",
    "\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "        self.metrics = []\n",
    "\n",
    "    def on_epoch_end(self, trainer, pl_module):\n",
    "        met = copy.deepcopy(trainer.callback_metrics)\n",
    "        self.metrics.append(met)\n",
    "\n",
    "    def __call__(self):\n",
    "        return self.get_callbacks()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "46692b08",
   "metadata": {},
   "outputs": [],
   "source": [
    "model = GAN(channels=1, width=28, height=28, train_disc_only=0, train_gen_only=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "greater-activation",
   "metadata": {},
   "outputs": [],
   "source": [
    "optimizer = model.configure_optimizers()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "caroline-passion",
   "metadata": {},
   "source": [
    "#### Register model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "handled-teens",
   "metadata": {},
   "outputs": [],
   "source": [
    "from copy import deepcopy\n",
    "\n",
    "# Need this plugin only if multiple optimizers are used. Not required for PyTorch Lightning with a single optimizer.\n",
    "framework_adapter = (\n",
    "    \"plugin_for_multiple_optimizers.FrameworkAdapterPluginforMultipleOpt\"\n",
    ")\n",
    "MI = ModelInterface(\n",
    "    model=model, optimizer=optimizer, framework_plugin=framework_adapter\n",
    ")\n",
    "\n",
    "initial_model = deepcopy(model)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "portuguese-groove",
   "metadata": {},
   "source": [
    "### Define and register FL tasks"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "increasing-builder",
   "metadata": {},
   "outputs": [],
   "source": [
    "TI = TaskInterface()\n",
    "\n",
    "import tqdm\n",
    "\n",
    "@TI.register_fl_task(\n",
    "    model=\"model\", data_loader=\"train_loader\", device=\"device\", optimizer=\"optimizer\"\n",
    ")\n",
    "def train(model, train_loader, optimizer, device, some_parameter=None):\n",
    "\n",
    "    print(f\"\\n\\n TASK TRAIN GOT DEVICE {device}\\n\\n\")\n",
    "\n",
    "    AVAIL_GPUS = 1 if \"cuda\" in device else 0\n",
    "\n",
    "    trainer = Trainer(gpus=AVAIL_GPUS, max_epochs=1, callbacks=[MetricsCallback()])\n",
    "    trainer.fit(model=model, train_dataloaders=train_loader)\n",
    "    print(\"training logged metrics\", trainer.logged_metrics)\n",
    "\n",
    "    if \"Discriminator training loss_epoch\" in trainer.logged_metrics:\n",
    "        train_loss = trainer.logged_metrics[\"Discriminator training loss_epoch\"]\n",
    "    else:\n",
    "        train_loss = trainer.logged_metrics[\"Generator training loss_epoch\"]\n",
    "    return {\"train_loss\": train_loss}\n",
    "\n",
    "\n",
    "@TI.register_fl_task(model=\"model\", data_loader=\"val_loader\", device=\"device\")\n",
    "def validate(model, val_loader, device):\n",
    "\n",
    "    print(f\"\\n\\n TASK VALIDATE GOT DEVICE {device}\\n\\n\")\n",
    "\n",
    "    model.eval()\n",
    "    model.to(device)\n",
    "\n",
    "    AVAIL_GPUS = 1 if \"cuda\" in device else 0\n",
    "\n",
    "    trainer = Trainer(gpus=AVAIL_GPUS, max_epochs=1, callbacks=[MetricsCallback()])\n",
    "\n",
    "    trainer.validate(model=model, dataloaders=val_loader)\n",
    "    print(\"validation logged metrics\", trainer.logged_metrics)\n",
    "\n",
    "    val_loss = trainer.logged_metrics[\"Discriminator val loss\"]\n",
    "\n",
    "    return {\"val_loss\": val_loss}"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "derived-bride",
   "metadata": {},
   "source": [
    "## Time to start a federated learning experiment"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "mature-renewal",
   "metadata": {},
   "outputs": [],
   "source": [
    "# create an experimnet in federation\n",
    "experiment_name = \"PL_MNIST_test_experiment\"\n",
    "fl_experiment = FLExperiment(federation=federation, experiment_name=experiment_name)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "lightweight-causing",
   "metadata": {},
   "outputs": [],
   "source": [
    "fl_experiment.start(\n",
    "    model_provider=MI,\n",
    "    task_keeper=TI,\n",
    "    data_loader=fed_dataset,\n",
    "    rounds_to_train=10,\n",
    "    opt_treatment=\"CONTINUE_GLOBAL\",\n",
    "    device_assignment_policy=\"CUDA_PREFERRED\",\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f1543a36",
   "metadata": {},
   "outputs": [],
   "source": [
    "fl_experiment.stream_metrics()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f987d2c4",
   "metadata": {},
   "source": [
    "## Check the images generated by the model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fec7a708",
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip install -r ../envoy/sd_requirements.txt\n",
    "import sys\n",
    "\n",
    "sys.path.insert(1, \"../envoy\")\n",
    "from mnist_shard_descriptor import MnistShardDescriptor"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2bc77cd8",
   "metadata": {},
   "outputs": [],
   "source": [
    "fed_dataset = MnistFedDataset(train_bs=256, valid_bs=64)\n",
    "fed_dataset.shard_descriptor = MnistShardDescriptor(rank_worldsize=\"1,1\")\n",
    "\n",
    "last_model = fl_experiment.get_last_model()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5f821972",
   "metadata": {},
   "outputs": [],
   "source": [
    "val_imgs, _ = next(iter(fed_dataset.get_valid_loader()))\n",
    "\n",
    "z = torch.randn(val_imgs.shape[0], 100)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9549d1ab",
   "metadata": {},
   "outputs": [],
   "source": [
    "last_model.train_generator(val_imgs, z, display_images=1)\n",
    "pass"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}

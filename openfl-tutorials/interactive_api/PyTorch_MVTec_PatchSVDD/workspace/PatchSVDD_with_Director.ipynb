{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "liquid-jacket",
   "metadata": {},
   "source": [
    "# Federated PatchSVDD algorithm with Director example"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "83e6aae2",
   "metadata": {},
   "source": [
    "# PatchSVDD algorithm\n",
    "Anomaly detection involves making a binary decision as to whether an input image contains an anomaly, and anomaly segmentation aims to locate the anomaly on the pixel level. The deep learning variant of Support vector data description (SVDD: a long-standing algorithm used for anomaly detection) is used to the patch-based method using self-supervised learning. This extension enables anomaly segmentation and improves detection performances which are measured in AUROC on MVTec AD dataset.\n",
    "\n",
    "![alt text](https://media.arxiv-vanity.com/render-output/5520416/x4.png \"Patch Level SVDD for Anomaly Detection\")\n",
    "\n",
    "* Original paper: https://arxiv.org/abs/2006.16067\n",
    "* Original Github code: https://github.com/nuclearboy95/Anomaly-Detection-PatchSVDD-PyTorch/tree/934d6238e5e0ad511e2a0e7fc4f4899010e7d892\n",
    "* MVTec ad dataset download link: https://www.mydrive.ch/shares/38536/3830184030e49fe74747669442f0f282/download/420938113-1629952094/mvtec_anomaly_detection.tar.xz"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "alike-sharing",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# Install dependencies if not already installed\n",
    "!pip install torchvision==0.8.1 matplotlib numpy scikit-image scikit-learn torch tqdm Pillow imageio opencv-python ngt"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "16986f22",
   "metadata": {},
   "source": [
    "# Connect to the Federation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4485ac79",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create a federation\n",
    "from openfl.interface.interactive_api.federation import Federation\n",
    "\n",
    "# 1) Run with API layer - Director mTLS \n",
    "# If the user wants to enable mTLS their must provide CA root chain, and signed key pair to the federation interface\n",
    "# cert_chain = 'cert/root_ca.crt'\n",
    "# API_certificate = 'cert/frontend.crt'\n",
    "# API_private_key = 'cert/frontend.key'\n",
    "\n",
    "# federation = Federation(client_id='frontend', director_node_fqdn='localhost', director_port='50051',\n",
    "#                        cert_chain=cert_chain, api_cert=API_certificate, api_private_key=API_private_key)\n",
    "\n",
    "# --------------------------------------------------------------------------------------------------------------------\n",
    "\n",
    "# 2) Run with TLS disabled (trusted environment)\n",
    "# Federation can also determine local fqdn automatically\n",
    "federation = Federation(client_id='frontend', director_node_fqdn='localhost', director_port='50050', tls=False)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e35802d5",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "shard_registry = federation.get_shard_registry()\n",
    "shard_registry"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "67ae50de",
   "metadata": {},
   "outputs": [],
   "source": [
    "federation.target_shape"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "obvious-tyler",
   "metadata": {},
   "source": [
    "## Creating a FL experiment using Interactive API"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "rubber-address",
   "metadata": {},
   "outputs": [],
   "source": [
    "from openfl.interface.interactive_api.experiment import TaskInterface, DataInterface, ModelInterface, FLExperiment"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "sustainable-public",
   "metadata": {},
   "source": [
    "### Register dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dd2407cf",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "#Arguments\n",
    "args = {\n",
    "'obj' : 'bottle',\n",
    "'lambda_value': '1e-3',\n",
    "'D' : 64,\n",
    "'lr' : '1e-4',\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8669bc63",
   "metadata": {},
   "outputs": [],
   "source": [
    "import argparse\n",
    "import torch\n",
    "from functools import reduce\n",
    "from torch.utils.data import DataLoader,Dataset\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import math\n",
    "import numpy as np\n",
    "from PIL import Image\n",
    "from imageio import imread\n",
    "from glob import glob\n",
    "from sklearn.metrics import roc_auc_score\n",
    "import os, shutil\n",
    "import _pickle as p\n",
    "from contextlib import contextmanager\n",
    "import PIL\n",
    "from torch.utils.data import Dataset, DataLoader, SubsetRandomSampler\n",
    "from torchvision import transforms as tsf\n",
    "from utils import to_device, task, DictionaryConcatDataset, crop_chw, cnn_output_size, crop_image_chw\n",
    "from functools import reduce"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "64f37dcf",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "def generate_coords(H, W, K):\n",
    "    h = np.random.randint(0, H - K + 1)\n",
    "    w = np.random.randint(0, W - K + 1)\n",
    "    return h, w\n",
    "\n",
    "\n",
    "def generate_coords_position(H, W, K):\n",
    "    with task('P1'):\n",
    "        p1 = generate_coords(H, W, K)\n",
    "        h1, w1 = p1\n",
    "\n",
    "    pos = np.random.randint(8)\n",
    "\n",
    "    with task('P2'):\n",
    "        J = K // 4\n",
    "\n",
    "        K3_4 = 3 * K // 4\n",
    "        h_dir, w_dir = pos_to_diff[pos]\n",
    "        h_del, w_del = np.random.randint(J, size=2)\n",
    "\n",
    "        h_diff = h_dir * (h_del + K3_4)\n",
    "        w_diff = w_dir * (w_del + K3_4)\n",
    "\n",
    "        h2 = h1 + h_diff\n",
    "        w2 = w1 + w_diff\n",
    "\n",
    "        h2 = np.clip(h2, 0, H - K)\n",
    "        w2 = np.clip(w2, 0, W - K)\n",
    "\n",
    "        p2 = (h2, w2)\n",
    "\n",
    "    return p1, p2, pos\n",
    "\n",
    "\n",
    "def generate_coords_svdd(H, W, K):\n",
    "    with task('P1'):\n",
    "        p1 = generate_coords(H, W, K)\n",
    "        h1, w1 = p1\n",
    "\n",
    "    with task('P2'):\n",
    "        J = K // 32\n",
    "\n",
    "        h_jit, w_jit = 0, 0\n",
    "\n",
    "        while h_jit == 0 and w_jit == 0:\n",
    "            h_jit = np.random.randint(-J, J + 1)\n",
    "            w_jit = np.random.randint(-J, J + 1)\n",
    "\n",
    "        h2 = h1 + h_jit\n",
    "        w2 = w1 + w_jit\n",
    "\n",
    "        h2 = np.clip(h2, 0, H - K)\n",
    "        w2 = np.clip(w2, 0, W - K)\n",
    "\n",
    "        p2 = (h2, w2)\n",
    "\n",
    "    return p1, p2\n",
    "\n",
    "\n",
    "pos_to_diff = {\n",
    "    0: (-1, -1),\n",
    "    1: (-1, 0),\n",
    "    2: (-1, 1),\n",
    "    3: (0, -1),\n",
    "    4: (0, 1),\n",
    "    5: (1, -1),\n",
    "    6: (1, 0),\n",
    "    7: (1, 1)\n",
    "}\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "242e3109",
   "metadata": {},
   "outputs": [],
   "source": [
    "class SVDD_Dataset(Dataset):\n",
    "    def __init__(self, memmap, K=64, repeat=1):\n",
    "        super().__init__()\n",
    "        self.arr = np.asarray(memmap)\n",
    "        self.K = K\n",
    "        self.repeat = repeat\n",
    "        \n",
    "\n",
    "    def __len__(self):\n",
    "        N = self.arr.shape[0]\n",
    "        return N * self.repeat\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        N = self.arr.shape[0]\n",
    "        K = self.K\n",
    "        n = idx % N\n",
    "\n",
    "        p1, p2 = generate_coords_svdd(256, 256, K)\n",
    "\n",
    "        image = self.arr[n]\n",
    "\n",
    "        patch1 = crop_image_chw(image, p1, K)\n",
    "        patch2 = crop_image_chw(image, p2, K)\n",
    "\n",
    "        return patch1, patch2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cea3ee13",
   "metadata": {},
   "outputs": [],
   "source": [
    "class PositionDataset(Dataset):\n",
    "    def __init__(self, x, K=64, repeat=1):\n",
    "        super(PositionDataset, self).__init__()\n",
    "        self.x = np.asarray(x)\n",
    "        self.K = K\n",
    "        self.repeat = repeat\n",
    "\n",
    "    def __len__(self):\n",
    "        N = self.x.shape[0]\n",
    "        return N * self.repeat\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        N = self.x.shape[0]\n",
    "        K = self.K\n",
    "        n = idx % N\n",
    "\n",
    "        image = self.x[n]\n",
    "        p1, p2, pos = generate_coords_position(256, 256, K)\n",
    "\n",
    "        patch1 = crop_image_chw(image, p1, K).copy()\n",
    "        patch2 = crop_image_chw(image, p2, K).copy()\n",
    "\n",
    "        # perturb RGB\n",
    "        rgbshift1 = np.random.normal(scale=0.02, size=(3, 1, 1))\n",
    "        rgbshift2 = np.random.normal(scale=0.02, size=(3, 1, 1))\n",
    "\n",
    "        patch1 += rgbshift1\n",
    "        patch2 += rgbshift2\n",
    "\n",
    "        # additive noise\n",
    "        noise1 = np.random.normal(scale=0.02, size=(3, K, K))\n",
    "        noise2 = np.random.normal(scale=0.02, size=(3, K, K))\n",
    "\n",
    "        patch1 += noise1\n",
    "        patch2 += noise2\n",
    "\n",
    "        return patch1, patch2, pos\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6efc53ee",
   "metadata": {},
   "outputs": [],
   "source": [
    "class PatchDataset_NCHW(Dataset):\n",
    "    def __init__(self, memmap, tfs=None, K=32, S=1):\n",
    "        super().__init__()\n",
    "        self.arr = memmap\n",
    "        self.tfs = tfs\n",
    "        self.S = S\n",
    "        self.K = K\n",
    "        self.N = self.arr.shape[0]\n",
    "    \n",
    "    def __len__(self):\n",
    "        return self.N * self.row_num * self.col_num\n",
    "\n",
    "    @property\n",
    "    def row_num(self):\n",
    "        N, C, H, W = self.arr.shape\n",
    "        K = self.K\n",
    "        S = self.S\n",
    "        I = cnn_output_size(H, k=K, s=S)\n",
    "        return I\n",
    "\n",
    "    @property\n",
    "    def col_num(self):\n",
    "        N, C, H, W = self.arr.shape\n",
    "        K = self.K\n",
    "        S = self.S\n",
    "        J = cnn_output_size(W, k=K, s=S)\n",
    "        return J\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        N = self.N\n",
    "        n, i, j = np.unravel_index(idx, (N, self.row_num, self.col_num))\n",
    "        K = self.K\n",
    "        S = self.S\n",
    "        image = self.arr[n]\n",
    "        patch = crop_chw(image, i, j, K, S)\n",
    "\n",
    "        if self.tfs:\n",
    "            patch = self.tfs(patch)\n",
    "\n",
    "        return patch, n, i, j\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fd781c21",
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "ShardDataset class\n",
    "\"\"\"\n",
    "class MVTecShardDataset(Dataset):\n",
    "    \n",
    "    def __init__(self, dataset):\n",
    "        self._dataset = dataset\n",
    "        \n",
    "    def __getitem__(self, index):\n",
    "        img, mask, label = self._dataset[index]\n",
    "        return img, mask, label\n",
    "    \n",
    "    def __len__(self):\n",
    "        return len(self._dataset)\n",
    "    \n",
    "class MVTecSD(DataInterface):\n",
    "\n",
    "    def __init__(self, **kwargs):\n",
    "        super().__init__(**kwargs)\n",
    "    \n",
    "    @property\n",
    "    def shard_descriptor(self):\n",
    "        return self._shard_descriptor\n",
    "        \n",
    "    @shard_descriptor.setter\n",
    "    def shard_descriptor(self, shard_descriptor):\n",
    "        \"\"\"\n",
    "        Describe per-collaborator procedures or sharding.\n",
    "\n",
    "        This method will be called during a collaborator initialization.\n",
    "        Local shard_descriptor  will be set by Envoy.\n",
    "        \"\"\"\n",
    "        self._shard_descriptor = shard_descriptor\n",
    "        \n",
    "        self.train_set = MVTecShardDataset(shard_descriptor.get_dataset('train'))\n",
    "\n",
    "        self.test_set = MVTecShardDataset(shard_descriptor.get_dataset('test'))    \n",
    "    \n",
    "    def get_train_loader(self, **kwargs):\n",
    "        \"\"\"\n",
    "        Output of this method will be provided to tasks with optimizer in contract\n",
    "        \"\"\"\n",
    "        train_x = np.stack([image for image, mask, label in self.train_set]).astype(np.float32)\n",
    "        mean = train_x.astype(np.float32).mean(axis=0)\n",
    "        train_x = (train_x.astype(np.float32) - mean) / 255\n",
    "        train_x = np.transpose(train_x, [0, 3, 1, 2])\n",
    "    \n",
    "        if self.kwargs['train_bs']:\n",
    "            batch_size = self.kwargs['train_bs']\n",
    "        else:\n",
    "            batch_size = 64\n",
    "            \n",
    "        loader = DataLoader(self.get_train_dataset_dict(train_x), batch_size=batch_size, shuffle=True, num_workers=2, pin_memory=True)\n",
    "        return loader\n",
    "\n",
    "    def get_valid_loader(self, **kwargs):\n",
    "        \"\"\"\n",
    "        Output of this method will be provided to tasks without optimizer in contract\n",
    "        \"\"\"\n",
    "        # We need both train and test data for obtaining embeddings\n",
    "        train_x = np.stack([image for image, mask, label in self.train_set]).astype(np.float32)\n",
    "        mean = train_x.astype(np.float32).mean(axis=0)\n",
    "        train_x = (train_x.astype(np.float32) - mean) / 255\n",
    "        train_x = np.transpose(train_x, [0, 3, 1, 2])\n",
    "        \n",
    "        #getting val loader\n",
    "        test_x = np.stack([image for image, mask, label in self.test_set]).astype(np.float32)\n",
    "        mean = test_x.astype(np.float32).mean(axis=0)\n",
    "        test_x = (test_x.astype(np.float32) - mean) / 255\n",
    "        test_x = np.transpose(test_x, [0, 3, 1, 2])\n",
    "        \n",
    "        masks = np.stack([mask for image, mask, label in self.test_set]).astype(np.int32)\n",
    "        masks[masks <= 128] = 0\n",
    "        masks[masks > 128] = 255\n",
    "        labels = np.stack([label for image, mask, label in self.test_set]).astype(np.int32)\n",
    "\n",
    "        return (train_x, test_x, masks, labels, mean)\n",
    "\n",
    "\n",
    "    def get_train_data_size(self):\n",
    "        \"\"\"\n",
    "        Information for aggregation\n",
    "        \"\"\"\n",
    "        return len(self.train_set)\n",
    "        \n",
    "\n",
    "    def get_valid_data_size(self):\n",
    "        \"\"\"\n",
    "        Information for aggregation\n",
    "        \"\"\"\n",
    "        return len(self.test_set)\n",
    "    \n",
    "    def get_train_dataset_dict(self,inp_x):\n",
    "        rep = 100\n",
    "        datasets = dict()\n",
    "        datasets[f'pos_64'] = PositionDataset(inp_x, K=64, repeat=rep)\n",
    "        datasets[f'pos_32'] = PositionDataset(inp_x, K=32, repeat=rep)\n",
    "\n",
    "        datasets[f'svdd_64'] = SVDD_Dataset(inp_x, K=64, repeat=rep)\n",
    "        datasets[f'svdd_32'] = SVDD_Dataset(inp_x, K=32, repeat=rep)\n",
    "        dataset = DictionaryConcatDataset(datasets)\n",
    "        return dataset\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d8df35f5",
   "metadata": {},
   "outputs": [],
   "source": [
    "fed_dataset = MVTecSD(train_bs=64, val_bs=64)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8f786b8b",
   "metadata": {},
   "source": [
    "### Describe a model and optimizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "visible-victor",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch.optim as optim\n",
    "import torch.nn as nn\n",
    "import torch\n",
    "import torch.nn.functional as F\n",
    "import math\n",
    "from utils import makedirpath"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "138e1493",
   "metadata": {},
   "outputs": [],
   "source": [
    "class Encoder(nn.Module):\n",
    "    def __init__(self, K, D=64, bias=True):\n",
    "        super().__init__()\n",
    "\n",
    "        self.conv1 = nn.Conv2d(3, 64, 5, 2, 0, bias=bias)\n",
    "        self.conv2 = nn.Conv2d(64, 64, 5, 2, 0, bias=bias)\n",
    "        self.conv3 = nn.Conv2d(64, 128, 5, 2, 0, bias=bias)\n",
    "        self.conv4 = nn.Conv2d(128, D, 5, 1, 0, bias=bias)\n",
    "\n",
    "        self.K = K\n",
    "        self.D = D\n",
    "        self.bias = bias\n",
    "\n",
    "    def forward(self, x):\n",
    "        h = self.conv1(x)\n",
    "        h = F.leaky_relu(h, 0.1)\n",
    "\n",
    "        h = self.conv2(h)\n",
    "        h = F.leaky_relu(h, 0.1)\n",
    "\n",
    "        h = self.conv3(h)\n",
    "\n",
    "        if self.K == 64:\n",
    "            h = F.leaky_relu(h, 0.1)\n",
    "            h = self.conv4(h)\n",
    "\n",
    "        h = torch.tanh(h)\n",
    "\n",
    "        return h\n",
    "\n",
    "def forward_hier(x, emb_small, K):\n",
    "    K_2 = K // 2\n",
    "    n = x.size(0)\n",
    "    x1 = x[..., :K_2, :K_2]\n",
    "    x2 = x[..., :K_2, K_2:]\n",
    "    x3 = x[..., K_2:, :K_2]\n",
    "    x4 = x[..., K_2:, K_2:]\n",
    "    xx = torch.cat([x1, x2, x3, x4], dim=0)\n",
    "    hh = emb_small(xx)\n",
    "\n",
    "    h1 = hh[:n]\n",
    "    h2 = hh[n: 2 * n]\n",
    "    h3 = hh[2 * n: 3 * n]\n",
    "    h4 = hh[3 * n:]\n",
    "\n",
    "    h12 = torch.cat([h1, h2], dim=3)\n",
    "    h34 = torch.cat([h3, h4], dim=3)\n",
    "    h = torch.cat([h12, h34], dim=2)\n",
    "    return h\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "113f5a73",
   "metadata": {},
   "outputs": [],
   "source": [
    "class EncoderDeep(nn.Module):\n",
    "    def __init__(self, K, D=64, bias=True):\n",
    "        super().__init__()\n",
    "\n",
    "        self.conv1 = nn.Conv2d(3, 32, 3, 2, 0, bias=bias)\n",
    "        self.conv2 = nn.Conv2d(32, 64, 3, 1, 0, bias=bias)\n",
    "        self.conv3 = nn.Conv2d(64, 128, 3, 1, 0, bias=bias)\n",
    "        self.conv4 = nn.Conv2d(128, 128, 3, 1, 0, bias=bias)\n",
    "        self.conv5 = nn.Conv2d(128, 64, 3, 1, 0, bias=bias)\n",
    "        self.conv6 = nn.Conv2d(64, 32, 3, 1, 0, bias=bias)\n",
    "        self.conv7 = nn.Conv2d(32, 32, 3, 1, 0, bias=bias)\n",
    "        self.conv8 = nn.Conv2d(32, D, 3, 1, 0, bias=bias)\n",
    "\n",
    "        self.K = K\n",
    "        self.D = D\n",
    "\n",
    "    def forward(self, x):\n",
    "        h = self.conv1(x)\n",
    "        h = F.leaky_relu(h, 0.1)\n",
    "\n",
    "        h = self.conv2(h)\n",
    "        h = F.leaky_relu(h, 0.1)\n",
    "\n",
    "        h = self.conv3(h)\n",
    "        h = F.leaky_relu(h, 0.1)\n",
    "\n",
    "        h = self.conv4(h)\n",
    "        h = F.leaky_relu(h, 0.1)\n",
    "\n",
    "        h = self.conv5(h)\n",
    "        h = F.leaky_relu(h, 0.1)\n",
    "\n",
    "        h = self.conv6(h)\n",
    "        h = F.leaky_relu(h, 0.1)\n",
    "\n",
    "        h = self.conv7(h)\n",
    "        h = F.leaky_relu(h, 0.1)\n",
    "\n",
    "        h = self.conv8(h)\n",
    "        h = torch.tanh(h)\n",
    "\n",
    "        return h\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "70ccf01d",
   "metadata": {},
   "outputs": [],
   "source": [
    "class EncoderHier(nn.Module):\n",
    "    def __init__(self, K, D=64, bias=True):\n",
    "        super().__init__()\n",
    "\n",
    "        if K > 64:\n",
    "            self.enc = EncoderHier(K // 2, D, bias=bias)\n",
    "\n",
    "        elif K == 64:\n",
    "            self.enc = EncoderDeep(K // 2, D, bias=bias)\n",
    "\n",
    "        else:\n",
    "            raise ValueError()\n",
    "\n",
    "        self.conv1 = nn.Conv2d(D, 128, 2, 1, 0, bias=bias)\n",
    "        self.conv2 = nn.Conv2d(128, D, 1, 1, 0, bias=bias)\n",
    "\n",
    "        self.K = K\n",
    "        self.D = D\n",
    "\n",
    "    def forward(self, x):\n",
    "        h = forward_hier(x, self.enc, K=self.K)\n",
    "\n",
    "        h = self.conv1(h)\n",
    "        h = F.leaky_relu(h, 0.1)\n",
    "\n",
    "        h = self.conv2(h)\n",
    "        h = torch.tanh(h)\n",
    "\n",
    "        return h\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6e7688de",
   "metadata": {},
   "outputs": [],
   "source": [
    "xent = nn.CrossEntropyLoss()\n",
    "\n",
    "class NormalizedLinear(nn.Module):\n",
    "    __constants__ = ['bias', 'in_features', 'out_features']\n",
    "\n",
    "    def __init__(self, in_features, out_features, bias=True):\n",
    "        super(NormalizedLinear, self).__init__()\n",
    "        self.in_features = in_features\n",
    "        self.out_features = out_features\n",
    "        self.weight = nn.Parameter(torch.Tensor(out_features, in_features))\n",
    "        if bias:\n",
    "            self.bias = nn.Parameter(torch.Tensor(out_features))\n",
    "        else:\n",
    "            self.register_parameter('bias', None)\n",
    "        self.reset_parameters()\n",
    "\n",
    "    def reset_parameters(self):\n",
    "        nn.init.kaiming_uniform_(self.weight, a=math.sqrt(5))\n",
    "        if self.bias is not None:\n",
    "            fan_in, _ = nn.init._calculate_fan_in_and_fan_out(self.weight)\n",
    "            bound = 1 / math.sqrt(fan_in)\n",
    "            nn.init.uniform_(self.bias, -bound, bound)\n",
    "\n",
    "    def forward(self, x):\n",
    "        with torch.no_grad():\n",
    "            w = self.weight / self.weight.data.norm(keepdim=True, dim=0)\n",
    "        return F.linear(x, w, self.bias)\n",
    "\n",
    "    def extra_repr(self):\n",
    "        return 'in_features={}, out_features={}, bias={}'.format(\n",
    "            self.in_features, self.out_features, self.bias is not None\n",
    "        )\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "foreign-gospel",
   "metadata": {},
   "outputs": [],
   "source": [
    "class PositionClassifier(nn.Module):\n",
    "    def __init__(self, K, D, class_num=8):\n",
    "        super().__init__()\n",
    "        self.D = D\n",
    "\n",
    "        self.fc1 = nn.Linear(D, 128)\n",
    "        self.act1 = nn.LeakyReLU(0.1)\n",
    "\n",
    "        self.fc2 = nn.Linear(128, 128)\n",
    "        self.act2 = nn.LeakyReLU(0.1)\n",
    "\n",
    "        self.fc3 = NormalizedLinear(128, class_num)\n",
    "        self.fc3.requires_grad_(False)\n",
    "\n",
    "        self.K = K\n",
    "\n",
    "    def forward(self, h1, h2):\n",
    "        h1 = h1.view(-1, self.D)\n",
    "        h2 = h2.view(-1, self.D)\n",
    "\n",
    "        h = h1 - h2\n",
    "\n",
    "        h = self.fc1(h)\n",
    "        h = self.act1(h)\n",
    "\n",
    "        h = self.fc2(h)\n",
    "        h = self.act2(h)\n",
    "\n",
    "        h = self.fc3(h)\n",
    "        return h\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "db853fe0",
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "Model definition (ensembled)\n",
    "\"\"\"\n",
    "class MyEnsembledModel(nn.Module):\n",
    "    def __init__(self, enc, cls_64, cls_32):\n",
    "        super().__init__()\n",
    "        self._enc = enc\n",
    "        self._cls_64 = cls_64\n",
    "        self._cls_32 = cls_32\n",
    "        \n",
    "    def forward(self):\n",
    "        pass\n",
    "\n",
    "\n",
    "enc = EncoderHier(64, args['D'])\n",
    "cls_64 = PositionClassifier(64, args['D'])\n",
    "cls_32 = PositionClassifier(32, args['D'])\n",
    "\n",
    "model = MyEnsembledModel(enc, cls_64, cls_32)\n",
    "\n",
    "params_to_update = []\n",
    "for p in model.parameters():\n",
    "    if p.requires_grad:\n",
    "        params_to_update.append(p)\n",
    "optimizer_adam = torch.optim.Adam(params=params_to_update , lr=float(args['lr']))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "caroline-passion",
   "metadata": {},
   "source": [
    "#### Register model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "handled-teens",
   "metadata": {},
   "outputs": [],
   "source": [
    "from copy import deepcopy\n",
    "\n",
    "framework_adapter = 'openfl.plugins.frameworks_adapters.pytorch_adapter.FrameworkAdapterPlugin'\n",
    "MI = ModelInterface(model=model, optimizer=optimizer_adam, framework_plugin=framework_adapter)\n",
    "\n",
    "# Save the initial model state\n",
    "initial_model = deepcopy(model)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "portuguese-groove",
   "metadata": {},
   "source": [
    "### Define and register FL tasks"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "increasing-builder",
   "metadata": {},
   "outputs": [],
   "source": [
    "TI = TaskInterface()\n",
    "import torch\n",
    "import tqdm\n",
    "from utils import cnn_output_size\n",
    "from inspection import eval_embeddings_nn_multik\n",
    "\n",
    "# The Interactive API supports registering functions definied in main module or imported.\n",
    "def function_defined_in_notebook(some_parameter):\n",
    "    print(f'Also I accept a parameter and it is {some_parameter}')\n",
    "\n",
    "# Task interface currently supports only standalone functions.\n",
    "@TI.add_kwargs(**{'some_parameter': 42})\n",
    "@TI.register_fl_task(model='model', data_loader='train_loader', \\\n",
    "                     device='device', optimizer='optimizer')     \n",
    "\n",
    "def train(model, train_loader, optimizer, device, some_parameter=None):\n",
    "    print(f'\\n\\n TASK TRAIN GOT DEVICE {device}\\n\\n')\n",
    "    \n",
    "    function_defined_in_notebook(some_parameter)\n",
    "    \n",
    "    train_loader = tqdm.tqdm(train_loader, desc=\"train\")\n",
    "\n",
    "    model.train()\n",
    "    model.to(device)\n",
    "    losses = []\n",
    "\n",
    "    for d in train_loader:\n",
    "        d = to_device(d, device, non_blocking=True)\n",
    "        optimizer.zero_grad()\n",
    "        loss_pos_64 = PositionClassifier_infer(model._cls_64, model._enc, d['pos_64'])\n",
    "        loss_pos_32 = PositionClassifier_infer(model._cls_32, model._enc.enc, d['pos_32'])\n",
    "        loss_svdd_64 = SVDD_Dataset_infer(model._enc, d['svdd_64'])\n",
    "        loss_svdd_32 = SVDD_Dataset_infer(model._enc.enc, d['svdd_32'])\n",
    "\n",
    "        loss = loss_pos_64 + loss_pos_32 + float(args['lambda_value']) * (loss_svdd_64 + loss_svdd_32)\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        losses.append(loss.detach().cpu().numpy())\n",
    "    return {'train_loss': np.mean(losses),}\n",
    "    \n",
    "\n",
    "@TI.register_fl_task(model='model', data_loader='val_loader', device='device')     \n",
    "def validate(model, val_loader, device):\n",
    "    print(f'\\n\\n TASK VALIDATE GOT DEVICE {device}\\n\\n')\n",
    "    \n",
    "    model._enc.eval()\n",
    "    model._enc.to(device)\n",
    "    \n",
    "    x_tr, x_te, masks, labels, mean = val_loader\n",
    "\n",
    "    embs64_tr = infer_(x_tr, model._enc, K=64, S=16, device=device)\n",
    "    embs64_te = infer_(x_te, model._enc, K=64, S=16, device=device)\n",
    "    embs32_tr = infer_(x_tr, model._enc.enc, K=32, S=4, device=device)\n",
    "    embs32_te = infer_(x_te, model._enc.enc, K=32, S=4, device=device)\n",
    "\n",
    "    embs64 = embs64_tr, embs64_te\n",
    "    embs32 = embs32_tr, embs32_te\n",
    "\n",
    "    results = eval_embeddings_nn_multik(args['obj'], embs64, embs32, masks, labels)\n",
    "    \n",
    "    maps = results['maps_mult']\n",
    "    obj = args['obj']\n",
    "\n",
    "    print(\"| K64 | Det: {:.3f} Seg:{:.3f} BA: {:.3f}\".format(results['det_64'],results['seg_64'],results['bal_acc_64']))\n",
    "    print(\"| K32 | Det: {:.3f} Seg:{:.3f} BA: {:.3f}\".format(results['det_32'],results['seg_32'],results['bal_acc_32']))\n",
    "    print(\"| sum | Det: {:.3f} Seg:{:.3f} BA: {:.3f}\".format(results['det_sum'],results['seg_sum'],results['bal_acc_sum']))\n",
    "    print(\"| mult | Det: {:.3f} Seg:{:.3f} BA: {:.3f}\".format(results['det_mult'],results['seg_mult'],results['bal_acc_mult']))\n",
    "\n",
    "    return {'detection_score_sum': results['det_sum'], 'segmentation_score_sum': results['seg_sum'], 'balanced_accuracy_score_sum': results['bal_acc_sum'], 'detection_score_mult': results['det_mult'], 'segmentation_score_mult': results['seg_mult'], 'balanced_accuracy_score_mult': results['bal_acc_mult']}\n",
    "\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "17d53d96",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Infer functions\n",
    "def PositionClassifier_infer(c, enc, batch):\n",
    "    x1s, x2s, ys = batch\n",
    "    h1 = enc(x1s)\n",
    "    h2 = enc(x2s)\n",
    "    logits = c(h1, h2)\n",
    "    loss = xent(logits, ys)\n",
    "    return loss\n",
    "\n",
    "def SVDD_Dataset_infer(enc, batch):\n",
    "    x1s, x2s, = batch\n",
    "    h1s = enc(x1s)\n",
    "    h2s = enc(x2s)\n",
    "    diff = h1s - h2s\n",
    "    l2 = diff.norm(dim=1)\n",
    "    loss = l2.mean()\n",
    "    return loss\n",
    "\n",
    "def infer_(x, enc, K, S, device):\n",
    "    dataset = PatchDataset_NCHW(x, K=K, S=S)\n",
    "    loader = DataLoader(dataset, batch_size=64, shuffle=False, pin_memory=True)\n",
    "    embs = np.empty((dataset.N, dataset.row_num, dataset.col_num, args['D']), dtype=np.float32)  # [-1, I, J, D]\n",
    "    enc = enc.eval()\n",
    "    with torch.no_grad():\n",
    "        for xs, ns, iis, js in loader:\n",
    "            xs = xs.to(device)\n",
    "            embedding = enc(xs)\n",
    "            embedding = embedding.detach().cpu().numpy()\n",
    "\n",
    "            for embed, n, i, j in zip(embedding, ns, iis, js):\n",
    "                embs[n, i, j] = np.squeeze(embed)\n",
    "    return embs"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "derived-bride",
   "metadata": {},
   "source": [
    "## Time to start a federated learning experiment"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "mature-renewal",
   "metadata": {},
   "outputs": [],
   "source": [
    "# create an experimnet in federation\n",
    "experiment_name = 'MVTec_test_experiment'\n",
    "fl_experiment = FLExperiment(federation=federation, experiment_name=experiment_name)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "lightweight-causing",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# If I use autoreload I got a pickling error\n",
    "\n",
    "# The following command zips the workspace and python requirements to be transfered to collaborator nodes\n",
    "fl_experiment.start(model_provider=MI, \n",
    "                    task_keeper=TI,\n",
    "                    data_loader=fed_dataset,\n",
    "                    rounds_to_train=10,\n",
    "                    opt_treatment='CONTINUE_GLOBAL',\n",
    "                    device_assignment_policy='CUDA_PREFERRED')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f1543a36",
   "metadata": {},
   "outputs": [],
   "source": [
    "# If user wants to stop IPython session, then reconnect and check how experiment is going \n",
    "# fl_experiment.restore_experiment_state(MI)\n",
    "\n",
    "fl_experiment.stream_metrics()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8c30b301",
   "metadata": {},
   "source": [
    "## Now we validate the best model and print anomaly maps!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "12186086",
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip install -r ../envoy/sd_requirements.txt\n",
    "import sys\n",
    "sys.path.insert(1, '../envoy')\n",
    "from mvtec_shard_descriptor import MVTecShardDescriptor\n",
    "from inspection import measure_emb_nn, eval_embeddings_nn_maps\n",
    "import matplotlib.pyplot as plt\n",
    "from PIL import Image\n",
    "from skimage.segmentation import mark_boundaries\n",
    "from utils import makedirpath, distribute_scores\n",
    "import pickle"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "439049e1",
   "metadata": {},
   "outputs": [],
   "source": [
    "def obtain_maps(model, val_loader, device):\n",
    "    print(f'\\n\\n OBTAIN MAPS GOT DEVICE {device}\\n\\n')\n",
    "    \n",
    "    model._enc.eval()\n",
    "    model._enc.to(device)\n",
    "    \n",
    "    x_tr, x_te, masks, labels, mean = val_loader\n",
    "\n",
    "    embs64_tr = infer_(x_tr, model._enc, K=64, S=16, device=device)\n",
    "    embs64_te = infer_(x_te, model._enc, K=64, S=16, device=device)\n",
    "    embs32_tr = infer_(x_tr, model._enc.enc, K=32, S=4, device=device)\n",
    "    embs32_te = infer_(x_te, model._enc.enc, K=32, S=4, device=device)\n",
    "\n",
    "    embs64 = embs64_tr, embs64_te\n",
    "    embs32 = embs32_tr, embs32_te\n",
    "\n",
    "    maps = eval_embeddings_nn_maps(args['obj'], embs64, embs32, masks, labels)    \n",
    "    print_anomaly_maps(args['obj'], maps, x_te, masks, mean)\n",
    "    \n",
    "\n",
    "def infer_(x, enc, K, S, device):\n",
    "    dataset = PatchDataset_NCHW(x, K=K, S=S)\n",
    "    loader = DataLoader(dataset, batch_size=64, shuffle=False, pin_memory=True)\n",
    "    embs = np.empty((dataset.N, dataset.row_num, dataset.col_num, args['D']), dtype=np.float32)  # [-1, I, J, D]\n",
    "    enc = enc.eval()\n",
    "    with torch.no_grad():\n",
    "        for xs, ns, iis, js in loader:\n",
    "            xs = xs.to(device)\n",
    "            embedding = enc(xs)\n",
    "            embedding = embedding.detach().cpu().numpy()\n",
    "\n",
    "            for embed, n, i, j in zip(embedding, ns, iis, js):\n",
    "                embs[n, i, j] = np.squeeze(embed)\n",
    "    return embs\n",
    "\n",
    "def print_anomaly_maps(obj, maps, images, masks, mean):\n",
    "    \"\"\"Print generated anomaly maps.\"\"\"\n",
    "    mshape = maps.shape[0]\n",
    "    images = np.transpose(images, [0, 3, 2, 1])\n",
    "    images = (images.astype(np.float32) * 255 + mean)\n",
    "\n",
    "    for n in range(10):\n",
    "        fig, axes = plt.subplots(ncols=2)\n",
    "        fig.set_size_inches(6, 3)\n",
    "\n",
    "        shape = (128, 128)\n",
    "        image = np.array(Image.fromarray((images[n] * 255).astype(np.uint8)).resize(shape[::-1]))\n",
    "        mask = np.array(Image.fromarray(masks[n]).resize(shape[::-1]))\n",
    "        image = mark_boundaries(image, mask, color=(1, 0, 0), mode='thick')\n",
    "\n",
    "        axes[0].imshow(image)\n",
    "        axes[0].set_axis_off()\n",
    "\n",
    "        axes[1].imshow(maps[n], vmax=maps[n].max(), cmap='Reds')\n",
    "        axes[1].set_axis_off()\n",
    "\n",
    "        plt.tight_layout()\n",
    "        plt.show()\n",
    "        plt.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "52ba543f",
   "metadata": {},
   "outputs": [],
   "source": [
    "fed_dataset = MVTecSD(train_bs=64, val_bs=64)\n",
    "fed_dataset.shard_descriptor = MVTecShardDescriptor(obj=args['obj'], data_folder='MVTec_data',rank_worldsize='1,1')\n",
    "\n",
    "last_model = fl_experiment.get_last_model()\n",
    "obtain_maps(last_model, fed_dataset.get_valid_loader(), 'cuda')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}

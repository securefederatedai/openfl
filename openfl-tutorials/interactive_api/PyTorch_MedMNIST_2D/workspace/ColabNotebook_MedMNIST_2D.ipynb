{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "4dd5da0c-1ae1-43e6-8ad9-360c8974476c",
   "metadata": {
    "id": "4dd5da0c-1ae1-43e6-8ad9-360c8974476c"
   },
   "source": [
    "[![Open In Colab](https://colab.research.google.com/assets/colab-badge.svg)](https://colab.research.google.com/github/intel/openfl/blob/develop/openfl-tutorials/interactive_api/PyTorch_MedMNIST_2D/workspace/ColabNotebook_MedMNIST_2D.ipynb)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ee73e205-d273-4b6c-878a-5ea958bfe267",
   "metadata": {
    "id": "ee73e205-d273-4b6c-878a-5ea958bfe267"
   },
   "source": [
    "### Preparations in colab:\n",
    "We need to clone the repository to run a federation because it contains director and envoy configs to start from.\n",
    "\n",
    "1. Clone the OpenFL repository\n",
    "2. Install OpenFL \n",
    "3. Go to the linreg workspace"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c6fafe9e",
   "metadata": {
    "id": "c6fafe9e"
   },
   "outputs": [],
   "source": [
    "# For right now, install from source, later we would migrate to PyPI install\n",
    "# !pip install openfl==1.2.1\n",
    "import shutil\n",
    "# shutil.rmtree('./openfl')\n",
    "!git clone https://github.com/intel/openfl.git\n",
    "!cd openfl && git checkout develop && pip install . --upgrade pip\n",
    "!pip install medmnist"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2698f1da-fa69-4543-bb15-c7c0dcb776b9",
   "metadata": {
    "id": "2698f1da-fa69-4543-bb15-c7c0dcb776b9"
   },
   "outputs": [],
   "source": [
    "import os\n",
    "from time import sleep\n",
    "os.chdir('openfl/openfl-tutorials/interactive_api/PyTorch_MedMNIST_2D/workspace')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ca564a9a-8473-4207-aa41-c80111021e48",
   "metadata": {
    "id": "ca564a9a-8473-4207-aa41-c80111021e48"
   },
   "outputs": [],
   "source": [
    "# Install dependencies if not already installed\n",
    "from tqdm import tqdm\n",
    "import numpy as np\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "from torchvision import transforms as T\n",
    "import torch.nn.functional as F\n",
    "\n",
    "import medmnist\n",
    "\n",
    "from medmnist import INFO, Evaluator"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1637381d-84d0-4132-92c3-bf1a1e9c7f7a",
   "metadata": {
    "id": "1637381d-84d0-4132-92c3-bf1a1e9c7f7a"
   },
   "source": [
    "# Describing the model and optimizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "oJur9-6midCi",
   "metadata": {
    "id": "oJur9-6midCi"
   },
   "outputs": [],
   "source": [
    "num_epochs = 3\n",
    "TRAIN_BS, VALID_BS = 64, 128\n",
    "\n",
    "lr = 0.001\n",
    "gamma=0.1\n",
    "milestones = [0.5 * num_epochs, 0.75 * num_epochs]\n",
    "data_flag = 'bloodmnist'\n",
    "\n",
    "info = INFO[data_flag]\n",
    "task = info['task']\n",
    "n_channels = info['n_channels']\n",
    "n_classes = len(info['label'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fde856d8-da4e-4d2f-bee2-85e673050623",
   "metadata": {
    "id": "fde856d8-da4e-4d2f-bee2-85e673050623"
   },
   "outputs": [],
   "source": [
    "# define a simple CNN model\n",
    "\n",
    "class Net(nn.Module):\n",
    "    def __init__(self, in_channels, num_classes):\n",
    "        super(Net, self).__init__()\n",
    "\n",
    "        self.layer1 = nn.Sequential(\n",
    "            nn.Conv2d(in_channels, 16, kernel_size=3),\n",
    "            nn.BatchNorm2d(16),\n",
    "            nn.ReLU())\n",
    "\n",
    "        self.layer2 = nn.Sequential(\n",
    "            nn.Conv2d(16, 16, kernel_size=3),\n",
    "            nn.BatchNorm2d(16),\n",
    "            nn.ReLU(),\n",
    "            nn.MaxPool2d(kernel_size=2, stride=2))\n",
    "\n",
    "        self.layer3 = nn.Sequential(\n",
    "            nn.Conv2d(16, 64, kernel_size=3),\n",
    "            nn.BatchNorm2d(64),\n",
    "            nn.ReLU())\n",
    "        \n",
    "        self.layer4 = nn.Sequential(\n",
    "            nn.Conv2d(64, 64, kernel_size=3),\n",
    "            nn.BatchNorm2d(64),\n",
    "            nn.ReLU())\n",
    "\n",
    "        self.layer5 = nn.Sequential(\n",
    "            nn.Conv2d(64, 64, kernel_size=3, padding=1),\n",
    "            nn.BatchNorm2d(64),\n",
    "            nn.ReLU(),\n",
    "            nn.MaxPool2d(kernel_size=2, stride=2))\n",
    "\n",
    "        self.fc = nn.Sequential(\n",
    "            nn.Linear(64 * 4 * 4, 128),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(128, 128),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(128, num_classes))\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.layer1(x)\n",
    "        x = self.layer2(x)\n",
    "        x = self.layer3(x)\n",
    "        x = self.layer4(x)\n",
    "        x = self.layer5(x)\n",
    "        x = x.view(x.size(0), -1)\n",
    "        x = self.fc(x)\n",
    "        return x\n",
    "\n",
    "model = Net(in_channels=n_channels, num_classes=n_classes)\n",
    "    \n",
    "# define loss function and optimizer\n",
    "if task == \"multi-label, binary-class\":\n",
    "    criterion = nn.BCEWithLogitsLoss()\n",
    "else:\n",
    "    criterion = nn.CrossEntropyLoss()\n",
    "    \n",
    "optimizer = optim.SGD(model.parameters(), lr=lr, momentum=0.9)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c9a860ab-91a5-410e-9d1e-4b9bd5a33d70",
   "metadata": {
    "id": "c9a860ab-91a5-410e-9d1e-4b9bd5a33d70"
   },
   "outputs": [],
   "source": [
    "# from torchvision import models\n",
    "print(model)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cd2e26a5-9f4e-4011-a999-e428246aa8c1",
   "metadata": {
    "id": "cd2e26a5-9f4e-4011-a999-e428246aa8c1"
   },
   "source": [
    "# Now we run the same training on federated data"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "83378ece-9cd5-4d40-a134-24cf68bdb79a",
   "metadata": {
    "id": "83378ece-9cd5-4d40-a134-24cf68bdb79a",
    "tags": []
   },
   "source": [
    "## 1. Start the Director service and several envoys with generated data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b0d105a0-04c4-4c26-81c7-a350e14393c2",
   "metadata": {
    "id": "b0d105a0-04c4-4c26-81c7-a350e14393c2"
   },
   "outputs": [],
   "source": [
    "# Here are the main parameters for our Federation\n",
    "n_cols = 5\n",
    "datapath = 'data/.' \n",
    "dataname = 'bloodmnist'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c0c3e78b-6e9d-4efc-9b30-3ddc413c0423",
   "metadata": {
    "id": "c0c3e78b-6e9d-4efc-9b30-3ddc413c0423"
   },
   "outputs": [],
   "source": [
    "import os\n",
    "from pathlib import Path\n",
    "import yaml\n",
    "from typing import Dict, List, Union"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "463a2821-b657-4e12-90ac-33b7810c5ff4",
   "metadata": {
    "id": "463a2821-b657-4e12-90ac-33b7810c5ff4"
   },
   "source": [
    "### Start the Director service"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e736d33f-5df2-4a2f-8210-f1feba9fd367",
   "metadata": {
    "id": "e736d33f-5df2-4a2f-8210-f1feba9fd367"
   },
   "outputs": [],
   "source": [
    "cwd = Path.cwd()\n",
    "director_workspace_path = Path('../director/').absolute()\n",
    "director_config_file = director_workspace_path / 'director_config.yaml'\n",
    "director_logfile = director_workspace_path / 'director.log'\n",
    "if director_logfile.is_file(): director_logfile.unlink()\n",
    "\n",
    "os.environ['main_folder'] = str(cwd)\n",
    "os.environ['director_workspace_path'] = str(director_workspace_path)\n",
    "os.environ['director_logfile'] = str(director_logfile)\n",
    "os.environ['director_config_file'] = str(director_config_file)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bb950328-c1e6-4062-8b36-b42486d60241",
   "metadata": {
    "id": "bb950328-c1e6-4062-8b36-b42486d60241"
   },
   "outputs": [],
   "source": [
    "%%script /bin/bash --bg\n",
    "cd $director_workspace_path\n",
    "fx director start --disable-tls -c $director_config_file > $director_logfile &\n",
    "cd $main_folder"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "223f0037-c87e-440d-b8df-8fe9211c34dc",
   "metadata": {
    "id": "223f0037-c87e-440d-b8df-8fe9211c34dc",
    "tags": []
   },
   "source": [
    "## Start Envoys"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f6deeee4-5dc8-433d-a4ea-c464c74b1b2b",
   "metadata": {
    "id": "f6deeee4-5dc8-433d-a4ea-c464c74b1b2b"
   },
   "source": [
    "#### First, we create several envoy config files "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c0e65a39-15f7-4cca-90bb-a2970b7be9f0",
   "metadata": {
    "id": "c0e65a39-15f7-4cca-90bb-a2970b7be9f0"
   },
   "outputs": [],
   "source": [
    "# Read the original envoy config file content\n",
    "with open(Path('../envoy/envoy_config_one.yaml'), \"r\") as stream:\n",
    "    orig_config = yaml.safe_load(stream)\n",
    "\n",
    "def generate_envoy_configs(config: Dict,\n",
    "                           save_path: Union[str, Path] = '../envoy/',\n",
    "                           n_cols: int = 2,\n",
    "                           datapath: str = '',\n",
    "                           dataname: str = 'bloodmnist') -> List[Path]:\n",
    "\n",
    "    config['shard_descriptor']['params']['datapath'] = datapath\n",
    "    config['shard_descriptor']['params']['dataname'] = dataname\n",
    "        \n",
    "    config_paths = [(Path(save_path) / f'{i}_envoy_config.yaml').absolute()\n",
    "                for i in range(1, n_cols + 1)]\n",
    "\n",
    "    for i, path in enumerate(config_paths):\n",
    "        config['shard_descriptor']['params']['rank_worldsize'] = str(i+1) + \", \" + str(n_cols)\n",
    "        with open(path, \"w\") as stream:\n",
    "            yaml.safe_dump(config, stream)\n",
    "            \n",
    "    return config_paths\n",
    "            \n",
    "def remove_configs(config_paths):\n",
    "    for path in config_paths:\n",
    "        path.unlink()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "90109c5b-c785-4af7-ace9-dcd913018dca",
   "metadata": {
    "id": "90109c5b-c785-4af7-ace9-dcd913018dca"
   },
   "outputs": [],
   "source": [
    "config_paths = generate_envoy_configs(orig_config,\n",
    "                                      n_cols=n_cols,\n",
    "                                      datapath=datapath,\n",
    "                                      dataname=dataname)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "70c3078a-7beb-47c5-bcee-2de264ef3266",
   "metadata": {
    "id": "70c3078a-7beb-47c5-bcee-2de264ef3266"
   },
   "source": [
    "#### Now start Envoy processes in a loop"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "843f698e-5582-4918-828c-cf095988da92",
   "metadata": {
    "id": "843f698e-5582-4918-828c-cf095988da92",
    "tags": []
   },
   "outputs": [],
   "source": [
    "# envoy_workspace_path = Path('../envoy/').absolute()\n",
    "def start_envoys(config_paths: List[Path]) -> None:\n",
    "    envoy_workspace_path = config_paths[0].parent\n",
    "    cwd = Path.cwd()\n",
    "    os.chdir(envoy_workspace_path)\n",
    "    for i, path in enumerate(config_paths):\n",
    "        os.system(f'fx envoy start -n env_{i + 1} --disable-tls '\n",
    "                  f'--envoy-config-path {path} -dh localhost -dp 50051 '\n",
    "                  f'>env_{i + 1}.log &')\n",
    "    os.chdir(cwd)\n",
    "\n",
    "sleep(5)\n",
    "\n",
    "start_envoys(config_paths)\n",
    "\n",
    "sleep(25)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6216f14c-78d8-444c-9144-ee8316d1487b",
   "metadata": {
    "id": "6216f14c-78d8-444c-9144-ee8316d1487b"
   },
   "source": [
    "## 2. Connect to the Director service of out Federation as Data scientist"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b9d3b764-cb86-4eec-ba8e-df119da7a27f",
   "metadata": {
    "id": "b9d3b764-cb86-4eec-ba8e-df119da7a27f"
   },
   "outputs": [],
   "source": [
    "# Create a federation\n",
    "from openfl.interface.interactive_api.federation import Federation\n",
    "\n",
    "# please use the same identificator that was used in signed certificate\n",
    "client_id = 'frontend'\n",
    "director_node_fqdn = 'localhost'\n",
    "director_port=50051\n",
    "\n",
    "# 2) Run with TLS disabled (trusted environment)\n",
    "# Federation can also determine local fqdn automatically\n",
    "federation = Federation(\n",
    "    client_id=client_id,\n",
    "    director_node_fqdn=director_node_fqdn,\n",
    "    director_port=director_port, \n",
    "    tls=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1bed370b-d0c0-46bc-8114-ea8255b2478b",
   "metadata": {
    "id": "1bed370b-d0c0-46bc-8114-ea8255b2478b",
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Data scientist may request a list of connected envoys\n",
    "shard_registry = federation.get_shard_registry()\n",
    "\n",
    "# WARNING!\n",
    "\n",
    "# Make sure shard registry contains all the envoys you started!\n",
    "# In other case try rereconnecting to the Director (the cell above).\n",
    "shard_registry"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c6401026-795f-491e-90cb-dd59b451df5f",
   "metadata": {
    "id": "c6401026-795f-491e-90cb-dd59b451df5f"
   },
   "source": [
    "### Now we will prepare an FL experiment using OpenFL Python API"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8166c689-9dde-4500-b05c-5b1ddf968978",
   "metadata": {
    "id": "8166c689-9dde-4500-b05c-5b1ddf968978"
   },
   "source": [
    "### Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7d73d8bf-5e44-4f05-b5c1-643caeea1b23",
   "metadata": {
    "id": "7d73d8bf-5e44-4f05-b5c1-643caeea1b23"
   },
   "outputs": [],
   "source": [
    "from openfl.interface.interactive_api.experiment import TaskInterface, DataInterface, ModelInterface, FLExperiment\n",
    "from PIL import Image\n",
    "\n",
    "data_transform = T.Compose([T.ToTensor(), \n",
    "                            T.Normalize(mean=[.5], std=[.5])]\n",
    "                 )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "55fb1a98-b44f-47ff-950d-5a40a1cca0d8",
   "metadata": {
    "id": "55fb1a98-b44f-47ff-950d-5a40a1cca0d8"
   },
   "outputs": [],
   "source": [
    "class TransformedDataset(Dataset):\n",
    "    \"\"\"Image Person ReID Dataset.\"\"\"\n",
    "\n",
    "\n",
    "    def __init__(self, dataset, transform=None, target_transform=None):\n",
    "        \"\"\"Initialize Dataset.\"\"\"\n",
    "        self.dataset = dataset\n",
    "        self.transform = transform\n",
    "        self.target_transform = target_transform\n",
    "\n",
    "    def __len__(self):\n",
    "        \"\"\"Length of dataset.\"\"\"\n",
    "        return len(self.dataset)\n",
    "\n",
    "    def __getitem__(self, index):\n",
    "                \n",
    "        img, label = self.dataset[index]\n",
    "        \n",
    "        if self.target_transform:\n",
    "            label = self.target_transform(label)  \n",
    "        else:\n",
    "            label = label.astype(int)\n",
    "        \n",
    "        if self.transform:\n",
    "            img = Image.fromarray(img)\n",
    "            img = self.transform(img)\n",
    "        else:\n",
    "            base_transform = T.PILToTensor()\n",
    "            img = Image.fromarray(img)\n",
    "            img = base_transform(img)  \n",
    "\n",
    "        return img, label\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "13f9d6e8-d020-4ceb-89d0-14113e0f8b9e",
   "metadata": {
    "id": "13f9d6e8-d020-4ceb-89d0-14113e0f8b9e"
   },
   "outputs": [],
   "source": [
    "class MedMnistFedDataset(DataInterface):\n",
    "    def __init__(self, **kwargs):\n",
    "        self.kwargs = kwargs\n",
    "        \n",
    "    @property\n",
    "    def shard_descriptor(self):\n",
    "        return self._shard_descriptor\n",
    "        \n",
    "    @shard_descriptor.setter\n",
    "    def shard_descriptor(self, shard_descriptor):\n",
    "        \"\"\"\n",
    "        Describe per-collaborator procedures or sharding.\n",
    "\n",
    "        This method will be called during a collaborator initialization.\n",
    "        Local shard_descriptor  will be set by Envoy.\n",
    "        \"\"\"\n",
    "        self._shard_descriptor = shard_descriptor\n",
    "\n",
    "        self.train_set = TransformedDataset(\n",
    "            self._shard_descriptor.get_dataset('train'),\n",
    "            transform=data_transform\n",
    "        )       \n",
    "        \n",
    "        self.valid_set = TransformedDataset(\n",
    "            self._shard_descriptor.get_dataset('val'),\n",
    "            transform=data_transform\n",
    "        )\n",
    "        \n",
    "    def get_train_loader(self, **kwargs):\n",
    "        \"\"\"\n",
    "        Output of this method will be provided to tasks with optimizer in contract\n",
    "        \"\"\"\n",
    "        return DataLoader(\n",
    "            self.train_set, num_workers=8, batch_size=self.kwargs['train_bs'], shuffle=True)\n",
    "\n",
    "    def get_valid_loader(self, **kwargs):\n",
    "        \"\"\"\n",
    "        Output of this method will be provided to tasks without optimizer in contract\n",
    "        \"\"\"\n",
    "        return DataLoader(self.valid_set, num_workers=8, batch_size=self.kwargs['valid_bs'])\n",
    "\n",
    "    def get_train_data_size(self):\n",
    "        \"\"\"\n",
    "        Information for aggregation\n",
    "        \"\"\"\n",
    "        return len(self.train_set)\n",
    "\n",
    "    def get_valid_data_size(self):\n",
    "        \"\"\"\n",
    "        Information for aggregation\n",
    "        \"\"\"\n",
    "        return len(self.valid_set)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ccccaa67-6dfb-437f-b201-46b2a2561730",
   "metadata": {
    "id": "ccccaa67-6dfb-437f-b201-46b2a2561730"
   },
   "outputs": [],
   "source": [
    "fed_dataset = MedMnistFedDataset(train_bs=TRAIN_BS, valid_bs=VALID_BS)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6233e1ed-a2f2-456f-9417-f35a2c27b236",
   "metadata": {
    "id": "6233e1ed-a2f2-456f-9417-f35a2c27b236"
   },
   "source": [
    "### Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "885a8530-6248-4060-a30a-45cdc79bc41a",
   "metadata": {
    "id": "885a8530-6248-4060-a30a-45cdc79bc41a"
   },
   "outputs": [],
   "source": [
    "from copy import deepcopy\n",
    "\n",
    "framework_adapter = 'openfl.plugins.frameworks_adapters.pytorch_adapter.FrameworkAdapterPlugin'\n",
    "MI = ModelInterface(model=model, optimizer=optimizer, framework_plugin=framework_adapter)\n",
    "\n",
    "# Save the initial model state\n",
    "initial_model = deepcopy(model)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dc9da235-02a8-4e7a-9455-5fe2462aa317",
   "metadata": {
    "id": "dc9da235-02a8-4e7a-9455-5fe2462aa317",
    "tags": []
   },
   "source": [
    "### Tasks"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7e101689-8a63-4562-98ff-09443b1ab9f2",
   "metadata": {
    "id": "7e101689-8a63-4562-98ff-09443b1ab9f2"
   },
   "outputs": [],
   "source": [
    "TI = TaskInterface()\n",
    "\n",
    "import torch\n",
    "import tqdm\n",
    "\n",
    "train_custom_params={'criterion':criterion,'task':task}\n",
    "\n",
    "# Task interface currently supports only standalone functions.\n",
    "@TI.add_kwargs(**train_custom_params)\n",
    "@TI.register_fl_task(model='model', data_loader='train_loader',\n",
    "                     device='device', optimizer='optimizer')\n",
    "def train(model, train_loader, device, optimizer, criterion, task):\n",
    "    total_loss = []\n",
    "    \n",
    "    train_loader = tqdm.tqdm(train_loader, desc=\"train\")\n",
    "    model.train()\n",
    "    model.to(device)\n",
    "    \n",
    "    for inputs, targets in train_loader:\n",
    "    \n",
    "        optimizer.zero_grad()\n",
    "        outputs = model(inputs.to(device))\n",
    "        \n",
    "        if task == 'multi-label, binary-class':\n",
    "            targets = targets.to(torch.float32).to(device)\n",
    "            loss = criterion(outputs, targets)\n",
    "        else:\n",
    "            targets = torch.squeeze(targets, 1).long().to(device)\n",
    "            loss = criterion(outputs, targets)\n",
    "        \n",
    "        total_loss.append(loss.item())\n",
    "        \n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "\n",
    "    return {'train_loss': np.mean(total_loss),}\n",
    "\n",
    "\n",
    "\n",
    "val_custom_params={'criterion':criterion, \n",
    "                   'run':'model1',\n",
    "                   'task':task}\n",
    "\n",
    "@TI.add_kwargs(**val_custom_params)\n",
    "@TI.register_fl_task(model='model', data_loader='val_loader', device='device')\n",
    "def validate(model, val_loader, device, criterion, run, task):\n",
    "\n",
    "    val_loader = tqdm.tqdm(val_loader, desc=\"validate\")\n",
    "    model.eval()\n",
    "    model.to(device)\n",
    "\n",
    "    val_score = 0\n",
    "    total_samples = 0\n",
    "    total_loss = []\n",
    "    y_score = torch.tensor([]).to(device)\n",
    "\n",
    "    with torch.no_grad():\n",
    "        for inputs, targets in val_loader:\n",
    "            outputs = model(inputs.to(device))\n",
    "            \n",
    "            if task == 'multi-label, binary-class':\n",
    "                targets = targets.to(torch.float32).to(device)\n",
    "                loss = criterion(outputs, targets)\n",
    "                m = nn.Sigmoid()\n",
    "                outputs = m(outputs).to(device)\n",
    "            else:\n",
    "                targets = torch.squeeze(targets, 1).long().to(device)\n",
    "                loss = criterion(outputs, targets)\n",
    "                m = nn.Softmax(dim=1)\n",
    "                outputs = m(outputs).to(device)\n",
    "                targets = targets.float().resize_(len(targets), 1)\n",
    "\n",
    "            total_loss.append(loss.item())\n",
    "            \n",
    "            total_samples += targets.shape[0]\n",
    "            pred = outputs.argmax(dim=1)\n",
    "            val_score += pred.eq(targets).sum().cpu().numpy()\n",
    "        \n",
    "        acc = val_score / total_samples        \n",
    "        test_loss = sum(total_loss) / len(total_loss)\n",
    "\n",
    "        return {'acc': acc,\n",
    "                'test_loss': test_loss,\n",
    "                }\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "40a4623e-6559-4d4c-b199-f9afe16c0bbd",
   "metadata": {
    "id": "40a4623e-6559-4d4c-b199-f9afe16c0bbd",
    "tags": []
   },
   "source": [
    "### Run"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fb357a88-7098-45b2-85f4-71fe2f2e0f82",
   "metadata": {
    "id": "fb357a88-7098-45b2-85f4-71fe2f2e0f82"
   },
   "outputs": [],
   "source": [
    "experiment_name = 'medmnist_exp'\n",
    "fl_experiment = FLExperiment(federation=federation, experiment_name=experiment_name)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "db20124a-949d-4218-abfd-aaf4d0758284",
   "metadata": {
    "id": "db20124a-949d-4218-abfd-aaf4d0758284"
   },
   "outputs": [],
   "source": [
    "fl_experiment.start(model_provider=MI, \n",
    "                    task_keeper=TI,\n",
    "                    data_loader=fed_dataset,\n",
    "                    rounds_to_train=3,\n",
    "                    device_assignment_policy='CUDA_PREFERRED')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4909be2b-d23b-4356-b2af-10a212382d52",
   "metadata": {
    "id": "4909be2b-d23b-4356-b2af-10a212382d52",
    "tags": []
   },
   "outputs": [],
   "source": [
    "# This method not only prints messages recieved from the director, \n",
    "# but also saves logs in the tensorboard format (by default)\n",
    "fl_experiment.stream_metrics()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9e365766-4ea6-40bc-96ae-a183274e8b8c",
   "metadata": {
    "id": "9e365766-4ea6-40bc-96ae-a183274e8b8c"
   },
   "source": [
    "## Cleaning"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e5d793be-6c20-4a22-bad7-c082c1ee76ca",
   "metadata": {
    "id": "e5d793be-6c20-4a22-bad7-c082c1ee76ca"
   },
   "outputs": [],
   "source": [
    "# To stop all services run\n",
    "!pkill fx"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "809b8eb3-4775-43d9-8f96-de84a089a54e",
   "metadata": {
    "id": "809b8eb3-4775-43d9-8f96-de84a089a54e",
    "tags": []
   },
   "outputs": [],
   "source": [
    "remove_configs(config_paths)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "09daa8e5-ef58-42d8-aef3-4028aff91dc0",
   "metadata": {
    "id": "09daa8e5-ef58-42d8-aef3-4028aff91dc0"
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "colab": {
   "collapsed_sections": [],
   "name": "ColabNotebook_Medmnist.ipynb",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}

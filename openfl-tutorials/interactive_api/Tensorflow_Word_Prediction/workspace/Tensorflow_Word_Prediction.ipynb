{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "2390f64a",
   "metadata": {},
   "source": [
    "# Federated Next Word Prediction with Director example"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e3f166dd",
   "metadata": {},
   "outputs": [],
   "source": [
    "# install requirements\n",
    "!pip install -r requirements.txt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "52a6f355",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "\n",
    "import os\n",
    "# disable GPUs due to Tensoflow not supporting CUDA 11\n",
    "os.environ['CUDA_VISIBLE_DEVICES'] = '-1'"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1a8e22c1",
   "metadata": {},
   "source": [
    "# Connect to the Federation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5c479dad",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create a federation\n",
    "from openfl.interface.interactive_api.federation import Federation\n",
    "\n",
    "# please use the same identificator that was used in signed certificate\n",
    "cliend_id = 'frontend'\n",
    "\n",
    "# 1) Run with API layer - Director mTLS\n",
    "# If the user wants to enable mTLS their must provide CA root chain, and signed key pair to the federation interface\n",
    "# cert_chain = 'cert/root_ca.crt'\n",
    "# API_certificate = 'cert/frontend.crt'\n",
    "# API_private_key = 'cert/frontend.key'\n",
    "\n",
    "# federation = Federation(client_id='frontend', director_node_fqdn='localhost', director_port='50051', disable_tls=False,\n",
    "#                        cert_chain=cert_chain, api_cert=API_certificate, api_private_key=API_private_key)\n",
    "\n",
    "# --------------------------------------------------------------------------------------------------------------------\n",
    "\n",
    "# 2) Run with TLS disabled (trusted environment)\n",
    "# Federation can also determine local fqdn automatically\n",
    "federation = Federation(client_id='frontend', director_node_fqdn='localhost', director_port='50051', tls=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "466edd2c",
   "metadata": {},
   "outputs": [],
   "source": [
    "shard_registry = federation.get_shard_registry()\n",
    "shard_registry"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "381a2e96",
   "metadata": {},
   "outputs": [],
   "source": [
    "federation.target_shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d72d9aef",
   "metadata": {},
   "outputs": [],
   "source": [
    "# First, request a dummy_shard_desc that holds information about the federated dataset \n",
    "dummy_shard_desc = federation.get_dummy_shard_descriptor(size=10)\n",
    "sample, target = dummy_shard_desc.get_dataset(dataset_type='')[0]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "obvious-tyler",
   "metadata": {},
   "source": [
    "## Creating a FL experiment using Interactive API"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9001b56e",
   "metadata": {},
   "outputs": [],
   "source": [
    "from openfl.interface.interactive_api.experiment import TaskInterface, DataInterface, ModelInterface, FLExperiment"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "sustainable-public",
   "metadata": {},
   "source": [
    "### Register dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "41ba9a55",
   "metadata": {},
   "outputs": [],
   "source": [
    "from tensorflow.keras.utils import Sequence\n",
    "\n",
    "class DataGenerator(Sequence):\n",
    "\n",
    "    def __init__(self, dataset, batch_size):\n",
    "        self.dataset = dataset\n",
    "        self.batch_size = batch_size\n",
    "        self.on_epoch_end()\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.dataset) // self.batch_size\n",
    "\n",
    "    def __getitem__(self, index):\n",
    "        return self.dataset[index * self.batch_size:(index + 1) * self.batch_size]\n",
    "\n",
    "# Now you can implement you data loaders using dummy_shard_desc\n",
    "class NextWordSD(DataInterface):\n",
    "\n",
    "    def __init__(self, train_val_split=0.8, **kwargs):\n",
    "        super().__init__(**kwargs)\n",
    "        self.train_val_split = train_val_split\n",
    "\n",
    "    @property\n",
    "    def shard_descriptor(self):\n",
    "        return self._shard_descriptor\n",
    "\n",
    "    @shard_descriptor.setter\n",
    "    def shard_descriptor(self, shard_descriptor):\n",
    "        \"\"\"\n",
    "        Describe per-collaborator procedures or sharding.\n",
    "\n",
    "        This method will be called during a collaborator initialization.\n",
    "        Local shard_descriptor will be set by Envoy.\n",
    "        \"\"\"\n",
    "        self._shard_descriptor = shard_descriptor\n",
    "\n",
    "    def __getitem__(self, index):\n",
    "        return self.shard_descriptor[index]\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.shard_descriptor)\n",
    "\n",
    "    def get_train_loader(self):\n",
    "        \"\"\"\n",
    "        Output of this method will be provided to tasks with optimizer in contract\n",
    "        \"\"\"\n",
    "        if self.kwargs['train_bs']:\n",
    "            batch_size = self.kwargs['train_bs']\n",
    "        else:\n",
    "            batch_size = 64\n",
    "\n",
    "        self.train_dataset = self.shard_descriptor.get_dataset('train', self.train_val_split)\n",
    "        return DataGenerator(self.train_dataset, batch_size=batch_size)\n",
    "\n",
    "    def get_valid_loader(self):\n",
    "        \"\"\"\n",
    "        Output of this method will be provided to tasks without optimizer in contract\n",
    "        \"\"\"\n",
    "        if self.kwargs['valid_bs']:\n",
    "            batch_size = self.kwargs['valid_bs']\n",
    "        else:\n",
    "            batch_size = 512\n",
    "\n",
    "        self.val_dataset = self.shard_descriptor.get_dataset('val', self.train_val_split)\n",
    "        return DataGenerator(self.val_dataset, batch_size=batch_size)\n",
    "\n",
    "    def get_train_data_size(self):\n",
    "        \"\"\"\n",
    "        Information for aggregation\n",
    "        \"\"\"\n",
    "        return len(self.train_dataset)\n",
    "\n",
    "    def get_valid_data_size(self):\n",
    "        \"\"\"\n",
    "        Information for aggregation\n",
    "        \"\"\"\n",
    "        return len(self.val_dataset)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "caring-distinction",
   "metadata": {},
   "source": [
    "### Describe a model and optimizer\n",
    "#### Sequential API"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3524931d",
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "from tensorflow.keras.layers import LSTM, Dense\n",
    "from tensorflow.keras.legacy.optimizers import Adam\n",
    "from tensorflow.keras.metrics import TopKCategoricalAccuracy\n",
    "from tensorflow.keras.losses import CategoricalCrossentropy\n",
    "from tensorflow.keras.models import Sequential\n",
    "\n",
    "model = Sequential()\n",
    "model.add(LSTM(1000, return_sequences=True))\n",
    "model.add(LSTM(1000))\n",
    "model.add(Dense(1000, activation='tanh'))\n",
    "model.add(Dense(10719, activation='softmax'))\n",
    "\n",
    "optimizer = Adam(learning_rate=0.001)\n",
    "loss_fn = CategoricalCrossentropy()\n",
    "train_acc_metric = TopKCategoricalAccuracy(k=10)\n",
    "val_acc_metric = TopKCategoricalAccuracy(k=10)\n",
    "\n",
    "batch_size = 64\n",
    "model.build(input_shape=[batch_size, 3, 96])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9b26d13e",
   "metadata": {},
   "outputs": [],
   "source": [
    "fed_dataset = NextWordSD(train_bs=64, valid_bs=512)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "portuguese-groove",
   "metadata": {},
   "source": [
    "### Define and register FL tasks"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7e326d8f",
   "metadata": {},
   "outputs": [],
   "source": [
    "TI = TaskInterface()\n",
    "\n",
    "# https://www.tensorflow.org/guide/keras/customizing_what_happens_in_fit\n",
    "@TI.register_fl_task(model='model', data_loader='train_loader', device='device', optimizer='optimizer')\n",
    "def train(model, train_loader, device, optimizer):\n",
    "\n",
    "    # Iterate over the batches of the dataset.\n",
    "    for step, (x_batch_train, y_batch_train) in enumerate(train_loader):\n",
    "\n",
    "        y = tf.convert_to_tensor(y_batch_train)\n",
    "        with tf.GradientTape() as tape:\n",
    "            y_pred = model(x_batch_train, training=True)  # Forward pass\n",
    "            # Compute the loss value\n",
    "            # (the loss function is configured in `compile()`)\n",
    "            loss = loss_fn(y, y_pred)\n",
    "\n",
    "        # Compute gradients\n",
    "        trainable_vars = model.trainable_variables\n",
    "        gradients = tape.gradient(loss, trainable_vars)\n",
    "\n",
    "        # Update weights\n",
    "        optimizer.apply_gradients(zip(gradients, trainable_vars))\n",
    "\n",
    "        # Update metrics\n",
    "        train_acc_metric.update_state(y, y_pred)\n",
    "    \n",
    "    # Reset training metrics at the end of each epoch\n",
    "    train_acc = train_acc_metric.result()\n",
    "    train_acc_metric.reset_states()\n",
    "    return {'train_acc': train_acc, 'loss': loss}\n",
    "\n",
    "\n",
    "@TI.register_fl_task(model='model', data_loader='val_loader', device='device')\n",
    "def validate(model, val_loader, device=''):\n",
    "    for x_batch_val, y_batch_val in val_loader:\n",
    "        y = tf.convert_to_tensor(y_batch_val)\n",
    "        # Compute predictions\n",
    "        y_pred = model(x_batch_val, training=False)\n",
    "        # Update the metrics.\n",
    "        val_acc_metric.update_state(y, y_pred)\n",
    "    val_acc = val_acc_metric.result()\n",
    "    val_acc_metric.reset_states()\n",
    "    return {'validation_accuracy': val_acc}\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "caroline-passion",
   "metadata": {},
   "source": [
    "#### Register model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "145676d1",
   "metadata": {},
   "outputs": [],
   "source": [
    "from copy import deepcopy\n",
    "\n",
    "framework_adapter = 'openfl.plugins.frameworks_adapters.keras_adapter.FrameworkAdapterPlugin'\n",
    "MI = ModelInterface(model=model, optimizer=optimizer, framework_plugin=framework_adapter)\n",
    "# Save the initial model state\n",
    "initial_model = deepcopy(model)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "derived-bride",
   "metadata": {},
   "source": [
    "## Time to start a federated learning experiment"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "df9d0e68",
   "metadata": {},
   "outputs": [],
   "source": [
    "# create an experimnet in federation\n",
    "experiment_name = 'word_prediction_test_experiment'\n",
    "fl_experiment = FLExperiment(federation=federation, experiment_name=experiment_name,serializer_plugin='openfl.plugins.interface_serializer.keras_serializer.KerasSerializer)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1fdb59e1",
   "metadata": {},
   "outputs": [],
   "source": [
    "# If I use autoreload I got a pickling error\n",
    "\n",
    "# The following command zips the workspace and python requirements to be transfered to collaborator nodes\n",
    "fl_experiment.start(model_provider=MI, \n",
    "                    task_keeper=TI,\n",
    "                    data_loader=fed_dataset,\n",
    "                    rounds_to_train=20,\n",
    "                    opt_treatment='RESET')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "92510227",
   "metadata": {},
   "outputs": [],
   "source": [
    "# If user want to stop IPython session, then reconnect and check how experiment is going \n",
    "# fl_experiment.restore_experiment_state(MI)\n",
    "\n",
    "fl_experiment.stream_metrics()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e9e0c0a8",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "## Testing the best model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0b55ddee",
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip install -r ../envoy/sd_requirements.txt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b23a6ec6",
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "sys.path.insert(1, '../envoy')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6bbdfedc",
   "metadata": {},
   "outputs": [],
   "source": [
    "from shard_descriptor import NextWordShardDescriptor\n",
    "\n",
    "# https://www.gutenberg.org/files/2892/2892-h/2892-h.htm\n",
    "fed_dataset = NextWordSD(train_bs=64, valid_bs=512, train_val_split=0)\n",
    "fed_dataset.shard_descriptor = NextWordShardDescriptor(title='Irish Fairy Tales', author='James Stephens')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "821b69a7",
   "metadata": {},
   "outputs": [],
   "source": [
    "best_model = fl_experiment.get_best_model()\n",
    "\n",
    "# We remove data from director\n",
    "fl_experiment.remove_experiment_data()\n",
    "\n",
    "# Validating initial model\n",
    "validate(initial_model, fed_dataset.get_valid_loader())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5c75b1f5",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Validating trained model\n",
    "validate(best_model, fed_dataset.get_valid_loader())"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}

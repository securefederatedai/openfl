{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "liquid-jacket",
   "metadata": {},
   "source": [
    "# Federated Keras MNIST Tutorial\n",
    "## Using low-level Python API"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "alike-sharing",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: tensorflow==2.5.0 in /home/mansi/anaconda3/lib/python3.8/site-packages (2.5.0)\n",
      "Requirement already satisfied: tensorboard~=2.5 in /home/mansi/anaconda3/lib/python3.8/site-packages (from tensorflow==2.5.0) (2.5.0)\n",
      "Requirement already satisfied: termcolor~=1.1.0 in /home/mansi/anaconda3/lib/python3.8/site-packages (from tensorflow==2.5.0) (1.1.0)\n",
      "Requirement already satisfied: wrapt~=1.12.1 in /home/mansi/anaconda3/lib/python3.8/site-packages (from tensorflow==2.5.0) (1.12.1)\n",
      "Requirement already satisfied: numpy~=1.19.2 in /home/mansi/anaconda3/lib/python3.8/site-packages (from tensorflow==2.5.0) (1.19.2)\n",
      "Requirement already satisfied: opt-einsum~=3.3.0 in /home/mansi/anaconda3/lib/python3.8/site-packages (from tensorflow==2.5.0) (3.3.0)\n",
      "Requirement already satisfied: typing-extensions~=3.7.4 in /home/mansi/anaconda3/lib/python3.8/site-packages (from tensorflow==2.5.0) (3.7.4.3)\n",
      "Requirement already satisfied: gast==0.4.0 in /home/mansi/anaconda3/lib/python3.8/site-packages (from tensorflow==2.5.0) (0.4.0)\n",
      "Requirement already satisfied: protobuf>=3.9.2 in /home/mansi/anaconda3/lib/python3.8/site-packages (from tensorflow==2.5.0) (3.14.0)\n",
      "Requirement already satisfied: google-pasta~=0.2 in /home/mansi/anaconda3/lib/python3.8/site-packages (from tensorflow==2.5.0) (0.2.0)\n",
      "Requirement already satisfied: flatbuffers~=1.12.0 in /home/mansi/anaconda3/lib/python3.8/site-packages (from tensorflow==2.5.0) (1.12)\n",
      "Requirement already satisfied: h5py~=3.1.0 in /home/mansi/anaconda3/lib/python3.8/site-packages (from tensorflow==2.5.0) (3.1.0)\n",
      "Requirement already satisfied: wheel~=0.35 in /home/mansi/anaconda3/lib/python3.8/site-packages (from tensorflow==2.5.0) (0.35.1)\n",
      "Requirement already satisfied: absl-py~=0.10 in /home/mansi/anaconda3/lib/python3.8/site-packages (from tensorflow==2.5.0) (0.11.0)\n",
      "Requirement already satisfied: tensorflow-estimator<2.6.0,>=2.5.0rc0 in /home/mansi/anaconda3/lib/python3.8/site-packages (from tensorflow==2.5.0) (2.5.0)\n",
      "Requirement already satisfied: six~=1.15.0 in /home/mansi/anaconda3/lib/python3.8/site-packages (from tensorflow==2.5.0) (1.15.0)\n",
      "Requirement already satisfied: keras-preprocessing~=1.1.2 in /home/mansi/anaconda3/lib/python3.8/site-packages (from tensorflow==2.5.0) (1.1.2)\n",
      "Requirement already satisfied: astunparse~=1.6.3 in /home/mansi/anaconda3/lib/python3.8/site-packages (from tensorflow==2.5.0) (1.6.3)\n",
      "Requirement already satisfied: keras-nightly~=2.5.0.dev in /home/mansi/anaconda3/lib/python3.8/site-packages (from tensorflow==2.5.0) (2.5.0.dev2021032900)\n",
      "Requirement already satisfied: grpcio~=1.34.0 in /home/mansi/anaconda3/lib/python3.8/site-packages (from tensorflow==2.5.0) (1.34.1)\n",
      "Requirement already satisfied: markdown>=2.6.8 in /home/mansi/anaconda3/lib/python3.8/site-packages (from tensorboard~=2.5->tensorflow==2.5.0) (3.3.3)\n",
      "Requirement already satisfied: tensorboard-data-server<0.7.0,>=0.6.0 in /home/mansi/anaconda3/lib/python3.8/site-packages (from tensorboard~=2.5->tensorflow==2.5.0) (0.6.1)\n",
      "Requirement already satisfied: requests<3,>=2.21.0 in /home/mansi/anaconda3/lib/python3.8/site-packages (from tensorboard~=2.5->tensorflow==2.5.0) (2.24.0)\n",
      "Requirement already satisfied: tensorboard-plugin-wit>=1.6.0 in /home/mansi/anaconda3/lib/python3.8/site-packages (from tensorboard~=2.5->tensorflow==2.5.0) (1.7.0)\n",
      "Requirement already satisfied: google-auth-oauthlib<0.5,>=0.4.1 in /home/mansi/anaconda3/lib/python3.8/site-packages (from tensorboard~=2.5->tensorflow==2.5.0) (0.4.2)\n",
      "Requirement already satisfied: werkzeug>=0.11.15 in /home/mansi/anaconda3/lib/python3.8/site-packages (from tensorboard~=2.5->tensorflow==2.5.0) (1.0.1)\n",
      "Requirement already satisfied: google-auth<2,>=1.6.3 in /home/mansi/anaconda3/lib/python3.8/site-packages (from tensorboard~=2.5->tensorflow==2.5.0) (1.24.0)\n",
      "Requirement already satisfied: setuptools>=41.0.0 in /home/mansi/anaconda3/lib/python3.8/site-packages (from tensorboard~=2.5->tensorflow==2.5.0) (50.3.1.post20201107)\n",
      "Requirement already satisfied: chardet<4,>=3.0.2 in /home/mansi/anaconda3/lib/python3.8/site-packages (from requests<3,>=2.21.0->tensorboard~=2.5->tensorflow==2.5.0) (3.0.4)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in /home/mansi/anaconda3/lib/python3.8/site-packages (from requests<3,>=2.21.0->tensorboard~=2.5->tensorflow==2.5.0) (2020.6.20)\n",
      "Requirement already satisfied: idna<3,>=2.5 in /home/mansi/anaconda3/lib/python3.8/site-packages (from requests<3,>=2.21.0->tensorboard~=2.5->tensorflow==2.5.0) (2.10)\n",
      "Requirement already satisfied: urllib3!=1.25.0,!=1.25.1,<1.26,>=1.21.1 in /home/mansi/anaconda3/lib/python3.8/site-packages (from requests<3,>=2.21.0->tensorboard~=2.5->tensorflow==2.5.0) (1.25.11)\n",
      "Requirement already satisfied: requests-oauthlib>=0.7.0 in /home/mansi/anaconda3/lib/python3.8/site-packages (from google-auth-oauthlib<0.5,>=0.4.1->tensorboard~=2.5->tensorflow==2.5.0) (1.3.0)\n",
      "Requirement already satisfied: cachetools<5.0,>=2.0.0 in /home/mansi/anaconda3/lib/python3.8/site-packages (from google-auth<2,>=1.6.3->tensorboard~=2.5->tensorflow==2.5.0) (4.2.0)\n",
      "Requirement already satisfied: pyasn1-modules>=0.2.1 in /home/mansi/anaconda3/lib/python3.8/site-packages (from google-auth<2,>=1.6.3->tensorboard~=2.5->tensorflow==2.5.0) (0.2.8)\n",
      "Requirement already satisfied: rsa<5,>=3.1.4; python_version >= \"3.6\" in /home/mansi/anaconda3/lib/python3.8/site-packages (from google-auth<2,>=1.6.3->tensorboard~=2.5->tensorflow==2.5.0) (4.7)\n",
      "Requirement already satisfied: oauthlib>=3.0.0 in /home/mansi/anaconda3/lib/python3.8/site-packages (from requests-oauthlib>=0.7.0->google-auth-oauthlib<0.5,>=0.4.1->tensorboard~=2.5->tensorflow==2.5.0) (3.1.0)\n",
      "Requirement already satisfied: pyasn1<0.5.0,>=0.4.6 in /home/mansi/anaconda3/lib/python3.8/site-packages (from pyasn1-modules>=0.2.1->google-auth<2,>=1.6.3->tensorboard~=2.5->tensorflow==2.5.0) (0.4.8)\n"
     ]
    }
   ],
   "source": [
    "# Install dependencies if not already installed\n",
    "!pip install tensorflow==2.5.0"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "caring-distinction",
   "metadata": {},
   "source": [
    "### Describe the model and optimizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "visible-victor",
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "from tensorflow import keras\n",
    "from tensorflow.keras import layers\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "further-plate",
   "metadata": {},
   "outputs": [],
   "source": [
    "inputs = keras.Input(shape=(784,), name=\"digits\")\n",
    "x1 = layers.Dense(64, activation=\"relu\")(inputs)\n",
    "x2 = layers.Dense(64, activation=\"relu\")(x1)\n",
    "outputs = layers.Dense(10, name=\"predictions\")(x2)\n",
    "model = keras.Model(inputs=inputs, outputs=outputs)\n",
    "\n",
    "# Instantiate an optimizer.\n",
    "optimizer = keras.optimizers.SGD(learning_rate=1e-3)\n",
    "# Instantiate a loss function.\n",
    "loss_fn = keras.losses.SparseCategoricalCrossentropy(from_logits=True)\n",
    "\n",
    "# Prepare the metrics.\n",
    "train_acc_metric = keras.metrics.SparseCategoricalAccuracy()\n",
    "val_acc_metric = keras.metrics.SparseCategoricalAccuracy()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fantastic-dispute",
   "metadata": {},
   "source": [
    "### Prepare data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "specific-chosen",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Prepare the training dataset.\n",
    "batch_size = 64\n",
    "(x_train, y_train), (x_test, y_test) = keras.datasets.mnist.load_data()\n",
    "x_train = np.reshape(x_train, (-1, 784))\n",
    "x_test = np.reshape(x_test, (-1, 784))\n",
    "\n",
    "X_valid = x_train[-10000:]\n",
    "y_valid = y_train[-10000:]\n",
    "X_train = x_train[:-10000]\n",
    "y_train = y_train[:-10000]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "obvious-tyler",
   "metadata": {},
   "source": [
    "## Describing FL experiment"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "rubber-address",
   "metadata": {},
   "outputs": [],
   "source": [
    "from openfl.interface.interactive_api.experiment import TaskInterface, DataInterface, ModelInterface, FLExperiment"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "caroline-passion",
   "metadata": {},
   "source": [
    "### Register model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "handled-teens",
   "metadata": {},
   "outputs": [],
   "source": [
    "framework_adapter = 'openfl.plugins.frameworks_adapters.keras_adapter.FrameworkAdapterPlugin'\n",
    "MI = ModelInterface(model=model, optimizer=optimizer, framework_plugin=framework_adapter)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "sustainable-public",
   "metadata": {},
   "source": [
    "### Register dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "verbal-newton",
   "metadata": {},
   "outputs": [],
   "source": [
    "class FedDataset(DataInterface):\n",
    "    def __init__(self, x_train, y_train, x_valid, y_valid, **kwargs):\n",
    "        self.X_train = X_train\n",
    "        self.y_train = y_train\n",
    "        self.X_valid = X_valid\n",
    "        self.y_valid = y_valid\n",
    "        self.batch_size = kwargs['batch_size']\n",
    "        self.kwargs = kwargs\n",
    "        self._setup_datasets()\n",
    "        \n",
    "    def _setup_datasets(self):\n",
    "        self.train_dataset = tf.data.Dataset.from_tensor_slices((self.X_train, self.y_train))\n",
    "        self.train_dataset = self.train_dataset.shuffle(buffer_size=1024).batch(self.batch_size)\n",
    "        self.valid_dataset = tf.data.Dataset.from_tensor_slices((self.X_valid, self.y_valid))\n",
    "        self.valid_dataset = self.valid_dataset.shuffle(buffer_size=1024).batch(self.batch_size)\n",
    "    \n",
    "    def _delayed_init(self, data_path='1,1'):\n",
    "        # With the next command the local dataset will be loaded on the collaborator node\n",
    "        # For this example we have the same dataset on the same path, and we will shard it\n",
    "        # So we use `data_path` information for this purpose.\n",
    "        self.rank, self.world_size = [int(part) for part in data_path.split(',')]\n",
    "        \n",
    "        # Do the actual sharding\n",
    "        self._do_sharding(self.rank , self.world_size)\n",
    "        \n",
    "    def _do_sharding(self, rank, world_size):\n",
    "        self.X_train = self.X_train[ rank-1 :: world_size ]\n",
    "        self.y_train = self.y_train[ rank-1 :: world_size ]\n",
    "        self.X_valid = self.X_valid[ rank-1 :: world_size ]\n",
    "        self.y_valid = self.y_valid[ rank-1 :: world_size ]\n",
    "        self._setup_datasets()\n",
    "\n",
    "    def get_train_loader(self, **kwargs):\n",
    "        \"\"\"\n",
    "        Output of this method will be provided to tasks with optimizer in contract\n",
    "        \"\"\"\n",
    "        return self.train_dataset\n",
    "\n",
    "    def get_valid_loader(self, **kwargs):\n",
    "        \"\"\"\n",
    "        Output of this method will be provided to tasks without optimizer in contract\n",
    "        \"\"\"\n",
    "        return self.valid_dataset\n",
    "\n",
    "    def get_train_data_size(self):\n",
    "        \"\"\"\n",
    "        Information for aggregation\n",
    "        \"\"\"\n",
    "        return len(self.X_train)\n",
    "\n",
    "    def get_valid_data_size(self):\n",
    "        \"\"\"\n",
    "        Information for aggregation\n",
    "        \"\"\"\n",
    "        return len(self.X_valid)\n",
    "    \n",
    "fed_dataset = FedDataset(X_train, y_train, X_valid, y_valid, batch_size=batch_size)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "marine-kansas",
   "metadata": {},
   "source": [
    "### Register tasks"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "increasing-builder",
   "metadata": {},
   "outputs": [],
   "source": [
    "TI = TaskInterface()\n",
    "\n",
    "import time\n",
    "\n",
    "@TI.register_fl_task(model='model', data_loader='train_dataset', \\\n",
    "                     device='device', optimizer='optimizer')     \n",
    "def train(model, train_dataset, optimizer, device, loss_fn=loss_fn, warmup=False):\n",
    "    start_time = time.time()\n",
    "\n",
    "    # Iterate over the batches of the dataset.\n",
    "    for step, (x_batch_train, y_batch_train) in enumerate(train_dataset):\n",
    "        with tf.GradientTape() as tape:\n",
    "            logits = model(x_batch_train, training=True)\n",
    "            loss_value = loss_fn(y_batch_train, logits)\n",
    "        grads = tape.gradient(loss_value, model.trainable_weights)\n",
    "        optimizer.apply_gradients(zip(grads, model.trainable_weights))\n",
    "\n",
    "        # Update training metric.\n",
    "        train_acc_metric.update_state(y_batch_train, logits)\n",
    "\n",
    "        # Log every 200 batches.\n",
    "        if step % 200 == 0:\n",
    "            print(\n",
    "                \"Training loss (for one batch) at step %d: %.4f\"\n",
    "                % (step, float(loss_value))\n",
    "            )\n",
    "            print(\"Seen so far: %d samples\" % ((step + 1) * 64))\n",
    "        if warmup:\n",
    "            break\n",
    "\n",
    "    # Display metrics at the end of each epoch.\n",
    "    train_acc = train_acc_metric.result()\n",
    "    print(\"Training acc over epoch: %.4f\" % (float(train_acc),))\n",
    "\n",
    "    # Reset training metrics at the end of each epoch\n",
    "    train_acc_metric.reset_states()\n",
    "\n",
    "        \n",
    "    return {'train_acc': train_acc,}\n",
    "\n",
    "\n",
    "@TI.register_fl_task(model='model', data_loader='val_dataset', device='device')     \n",
    "def validate(model, val_dataset, device):\n",
    "    # Run a validation loop at the end of each epoch.\n",
    "    for x_batch_val, y_batch_val in val_dataset:\n",
    "        val_logits = model(x_batch_val, training=False)\n",
    "        # Update val metrics\n",
    "        val_acc_metric.update_state(y_batch_val, val_logits)\n",
    "    val_acc = val_acc_metric.result()\n",
    "    val_acc_metric.reset_states()\n",
    "    print(\"Validation acc: %.4f\" % (float(val_acc),))\n",
    "            \n",
    "    return {'validation_accuracy': val_acc,}\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "massive-proxy",
   "metadata": {},
   "source": [
    "### Perform model warm up\n",
    "The model warmup is necessary to initialize weights when using Tensorflow Gradient Tape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "about-wholesale",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training loss (for one batch) at step 0: 96.0741\n",
      "Seen so far: 64 samples\n",
      "Training acc over epoch: 0.1875\n"
     ]
    }
   ],
   "source": [
    "train(model,fed_dataset.get_train_loader(), optimizer, 'cpu', warmup=True)\n",
    "\n",
    "#Make a copy of the model for later comparison\n",
    "initial_model = tf.keras.models.clone_model(model)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "curious-exercise",
   "metadata": {},
   "source": [
    "### Prepare Federated Dataset for Serialization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "opened-pontiac",
   "metadata": {},
   "outputs": [],
   "source": [
    "# tf.data.DataSet does not serialize well with pickle. It will be recreated on the collaborators with the delayed init function\n",
    "fed_dataset.train_dataset = None\n",
    "fed_dataset.valid_dataset = None"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "derived-bride",
   "metadata": {},
   "source": [
    "## Start a federated learning experiment"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "silver-baltimore",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create a federation\n",
    "from openfl.interface.interactive_api.federation import Federation\n",
    "# will determine fqdn by itself\n",
    "federation = Federation(central_node_fqdn='localhost', disable_tls=True)\n",
    "# Datapath corresonds to 'RANK,WORLD_SIZE'\n",
    "col_data_paths = {'one': '1,2',\n",
    "                'two': '2,2'}\n",
    "federation.register_collaborators(col_data_paths=col_data_paths)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "mature-renewal",
   "metadata": {},
   "outputs": [],
   "source": [
    "# create an experimnet in federation\n",
    "fl_experiment = FLExperiment(federation=federation,)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "lightweight-causing",
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "tried to remove tensor: __opt_state_needed not present in the tensor dict\n",
      "tried to remove tensor: __opt_state_needed not present in the tensor dict\n",
      "WARNING:openfl.transport.grpc.server:gRPC is running on insecure channel with TLS disabled.\n",
      "INFO:openfl.transport.grpc.server:Starting Aggregator gRPC Server\n"
     ]
    }
   ],
   "source": [
    "# If I use autoreload I got a pickling error\n",
    "fl_experiment.prepare_workspace_distribution(model_provider=MI, task_keeper=TI, data_loader=fed_dataset, rounds_to_train=7, \\\n",
    "                              opt_treatment='CONTINUE_GLOBAL')\n",
    "fl_experiment.start_experiment(model_provider=MI)\n",
    "\n",
    "# When the aggregator server blocks the notebook one can start collaborators\n",
    "# For the test run just type console command from the workspace directory:\n",
    "# `fx collaborator start -d data.yaml -n {col_name}` for all collaborators\n",
    "# For the distributed experiment transfer zipped workspace to the collaborator nodes and run\n",
    "# `fx workspace import --archive {workspace_name}.zip` cd to the workspace and start collaborators"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8c30b301",
   "metadata": {},
   "source": [
    "## Now we validate the best model!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "55acff59",
   "metadata": {},
   "outputs": [],
   "source": [
    "best_model = fl_experiment.get_best_model()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9fb5718d",
   "metadata": {},
   "outputs": [],
   "source": [
    "fed_dataset._delayed_init()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a2acb7e6",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Validating initial model\n",
    "validate(initial_model, fed_dataset.get_valid_loader(), 'cpu')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c12ca93f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Validating trained model\n",
    "validate(best_model, fed_dataset.get_valid_loader(), 'cpu')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a137d70d",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "fets",
   "language": "python",
   "name": "fets"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}

{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "liquid-jacket",
   "metadata": {},
   "source": [
    "# Federated PyTorch UNET Tutorial\n",
    "## Using low-level Python API"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "alike-sharing",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: torchvision in /home/davidyuk/.virtualenvs/openfl/lib/python3.8/site-packages (0.9.1)\n",
      "Requirement already satisfied: torch==1.8.1 in /home/davidyuk/.virtualenvs/openfl/lib/python3.8/site-packages (from torchvision) (1.8.1)\n",
      "Requirement already satisfied: numpy in /home/davidyuk/.virtualenvs/openfl/lib/python3.8/site-packages (from torchvision) (1.20.3)\n",
      "Requirement already satisfied: pillow>=4.1.1 in /home/davidyuk/.virtualenvs/openfl/lib/python3.8/site-packages (from torchvision) (8.2.0)\n",
      "Requirement already satisfied: typing-extensions in /home/davidyuk/.virtualenvs/openfl/lib/python3.8/site-packages (from torch==1.8.1->torchvision) (3.10.0.0)\n",
      "\u001b[33mWARNING: You are using pip version 21.1; however, version 21.1.2 is available.\n",
      "You should consider upgrading via the '/home/davidyuk/.virtualenvs/openfl/bin/python -m pip install --upgrade pip' command.\u001b[0m\n"
     ]
    }
   ],
   "source": [
    "# Install dependencies if not already installed\n",
    "!pip install torchvision"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "caring-distinction",
   "metadata": {},
   "source": [
    "### Describe the model and optimizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "visible-victor",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "foreign-gospel",
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "UNet model definition\n",
    "\"\"\"\n",
    "from layers import soft_dice_coef, soft_dice_loss, double_conv, down, up\n",
    "\n",
    "class UNet(nn.Module):\n",
    "    def __init__(self, n_channels=3, n_classes=1):\n",
    "        super().__init__()\n",
    "        self.inc = double_conv(n_channels, 64)\n",
    "        self.down1 = down(64, 128)\n",
    "        self.down2 = down(128, 256)\n",
    "        self.down3 = down(256, 512)\n",
    "        self.down4 = down(512, 1024)\n",
    "        self.up1 = up(1024, 512)\n",
    "        self.up2 = up(512, 256)\n",
    "        self.up3 = up(256, 128)\n",
    "        self.up4 = up(128, 64)\n",
    "        self.outc = nn.Conv2d(64, n_classes, 1)\n",
    "\n",
    "    def forward(self, x):\n",
    "        x1 = self.inc(x)\n",
    "        x2 = self.down1(x1)\n",
    "        x3 = self.down2(x2)\n",
    "        x4 = self.down3(x3)\n",
    "        x5 = self.down4(x4)\n",
    "        x = self.up1(x5, x4)\n",
    "        x = self.up2(x, x3)\n",
    "        x = self.up3(x, x2)\n",
    "        x = self.up4(x, x1)\n",
    "        x = self.outc(x)\n",
    "        x = torch.sigmoid(x)\n",
    "        return x\n",
    "    \n",
    "    \n",
    "model_unet = UNet()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "24e5cf06",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "class UNet(nn.Module):\n",
      "    def __init__(self, n_channels=3, n_classes=1):\n",
      "        super().__init__()\n",
      "        self.inc = double_conv(n_channels, 64)\n",
      "        self.down1 = down(64, 128)\n",
      "        self.down2 = down(128, 256)\n",
      "        self.down3 = down(256, 512)\n",
      "        self.down4 = down(512, 1024)\n",
      "        self.up1 = up(1024, 512)\n",
      "        self.up2 = up(512, 256)\n",
      "        self.up3 = up(256, 128)\n",
      "        self.up4 = up(128, 64)\n",
      "        self.outc = nn.Conv2d(64, n_classes, 1)\n",
      "\n",
      "    def forward(self, x):\n",
      "        x1 = self.inc(x)\n",
      "        x2 = self.down1(x1)\n",
      "        x3 = self.down2(x2)\n",
      "        x4 = self.down3(x3)\n",
      "        x5 = self.down4(x4)\n",
      "        x = self.up1(x5, x4)\n",
      "        x = self.up2(x, x3)\n",
      "        x = self.up3(x, x2)\n",
      "        x = self.up4(x, x1)\n",
      "        x = self.outc(x)\n",
      "        x = torch.sigmoid(x)\n",
      "        return x\n",
      "\n"
     ]
    }
   ],
   "source": [
    "import inspect\n",
    "from IPython.core.magics.code import extract_symbols\n",
    "\n",
    "import sys\n",
    "\n",
    "def new_getfile(object, _old_getfile=inspect.getfile):\n",
    "    if not inspect.isclass(object):\n",
    "        return _old_getfile(object)\n",
    "    \n",
    "    # Lookup by parent module (as in current inspect)\n",
    "    if hasattr(object, '__module__'):\n",
    "        object_ = sys.modules.get(object.__module__)\n",
    "        if hasattr(object_, '__file__'):\n",
    "            return object_.__file__\n",
    "    \n",
    "    # If parent module is __main__, lookup by methods (NEW)\n",
    "    for name, member in inspect.getmembers(object):\n",
    "        if inspect.isfunction(member) and object.__qualname__ + '.' + member.__name__ == member.__qualname__:\n",
    "            return inspect.getfile(member)\n",
    "    else:\n",
    "        raise TypeError('Source for {!r} not found'.format(object))\n",
    "inspect.getfile = new_getfile\n",
    "\n",
    "obj = UNet\n",
    "cell_code = \"\".join(inspect.linecache.getlines(new_getfile(obj)))\n",
    "class_code = extract_symbols(cell_code, obj.__name__)[0][0]\n",
    "print(class_code)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "greater-activation",
   "metadata": {},
   "outputs": [],
   "source": [
    "optimizer_adam = optim.Adam(model_unet.parameters(), lr=1e-4)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "obvious-tyler",
   "metadata": {},
   "source": [
    "## Describing FL experiment"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "rubber-address",
   "metadata": {},
   "outputs": [],
   "source": [
    "from openfl.interface.interactive_api.experiment import TaskInterface, DataInterface, ModelInterface, FLExperiment"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "caroline-passion",
   "metadata": {},
   "source": [
    "### Register model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "handled-teens",
   "metadata": {},
   "outputs": [],
   "source": [
    "from copy import deepcopy\n",
    "\n",
    "framework_adapter = 'openfl.plugins.frameworks_adapters.pytorch_adapter.FrameworkAdapterPlugin'\n",
    "MI = ModelInterface(model=model_unet, optimizer=optimizer_adam, framework_plugin=framework_adapter)\n",
    "\n",
    "# Save the initial model state\n",
    "initial_model = deepcopy(model_unet)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "portuguese-groove",
   "metadata": {},
   "source": [
    "### Define Federated Learning tasks"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "secure-protest",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "sustainable-public",
   "metadata": {},
   "source": [
    "### Register dataset"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "unlike-texas",
   "metadata": {},
   "source": [
    "We extract User dataset class implementation.\n",
    "Is it convinient?\n",
    "What if the dataset is not a class?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "verbal-newton",
   "metadata": {},
   "outputs": [],
   "source": [
    "# class FedDataset(DataInterface):\n",
    "#     def __init__(self, UserDatasetClass, **kwargs):\n",
    "#         self.UserDatasetClass = UserDatasetClass\n",
    "#         self.kwargs = kwargs\n",
    "    \n",
    "#     def _delayed_init(self, data_path='1,1'):\n",
    "#         # With the next command the local dataset will be loaded on the collaborator node\n",
    "#         # For this example we have the same dataset on the same path, and we will shard it\n",
    "#         # So we use `data_path` information for this purpose.\n",
    "#         self.rank, self.world_size = [int(part) for part in data_path.split(',')]\n",
    "        \n",
    "#         validation_fraction=1/8\n",
    "#         self.train_set = self.UserDatasetClass(validation_fraction=validation_fraction, is_validation=False)\n",
    "#         self.valid_set = self.UserDatasetClass(validation_fraction=validation_fraction, is_validation=True)\n",
    "        \n",
    "#         # Do the actual sharding\n",
    "#         self._do_sharding( self.rank, self.world_size)\n",
    "        \n",
    "#     def _do_sharding(self, rank, world_size):\n",
    "#         # This method relies on the dataset's implementation\n",
    "#         # i.e. coupled in a bad way\n",
    "#         self.train_set.images_names = self.train_set.images_names[ rank-1 :: world_size ]\n",
    "\n",
    "#     def get_train_loader(self, **kwargs):\n",
    "#         \"\"\"\n",
    "#         Output of this method will be provided to tasks with optimizer in contract\n",
    "#         \"\"\"\n",
    "#         return DataLoader(\n",
    "#             self.train_set, num_workers=8, batch_size=self.kwargs['train_bs'], shuffle=True\n",
    "#             )\n",
    "\n",
    "#     def get_valid_loader(self, **kwargs):\n",
    "#         \"\"\"\n",
    "#         Output of this method will be provided to tasks without optimizer in contract\n",
    "#         \"\"\"\n",
    "#         return DataLoader(self.valid_set, num_workers=8, batch_size=self.kwargs['valid_bs'])\n",
    "\n",
    "#     def get_train_data_size(self):\n",
    "#         \"\"\"\n",
    "#         Information for aggregation\n",
    "#         \"\"\"\n",
    "#         return len(self.train_set)\n",
    "\n",
    "#     def get_valid_data_size(self):\n",
    "#         \"\"\"\n",
    "#         Information for aggregation\n",
    "#         \"\"\"\n",
    "#         return len(self.valid_set)\n",
    "    \n",
    "# fed_dataset = FedDataset(KvasirDataset, train_bs=8, valid_bs=8)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "marine-kansas",
   "metadata": {},
   "source": [
    "### Register tasks"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "increasing-builder",
   "metadata": {},
   "outputs": [],
   "source": [
    "TI = TaskInterface()\n",
    "import torch\n",
    "\n",
    "import tqdm\n",
    "\n",
    "# The Interactive API supports registering functions definied in main module or imported.\n",
    "def function_defined_in_notebook(some_parameter):\n",
    "    print(f'Also I accept a parameter and it is {some_parameter}')\n",
    "\n",
    "# Task interface currently supports only standalone functions.\n",
    "@TI.add_kwargs(**{'some_parameter': 42})\n",
    "@TI.register_fl_task(model='unet_model', data_loader='train_loader', \\\n",
    "                     device='device', optimizer='optimizer')     \n",
    "def train(unet_model, train_loader, optimizer, device, loss_fn=soft_dice_loss, some_parameter=None):\n",
    "    if not torch.cuda.is_available():\n",
    "        device = 'cpu'\n",
    "    \n",
    "    function_defined_in_notebook(some_parameter)\n",
    "    \n",
    "    train_loader = tqdm.tqdm(train_loader, desc=\"train\")\n",
    "    \n",
    "    unet_model.train()\n",
    "    unet_model.to(device)\n",
    "\n",
    "    losses = []\n",
    "\n",
    "    for data, target in train_loader:\n",
    "        data, target = torch.tensor(data).to(device), torch.tensor(\n",
    "            target).to(device, dtype=torch.float32)\n",
    "        optimizer.zero_grad()\n",
    "        output = unet_model(data)\n",
    "        loss = loss_fn(output=output, target=target)\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        losses.append(loss.detach().cpu().numpy())\n",
    "        \n",
    "    return {'train_loss': np.mean(losses),}\n",
    "\n",
    "\n",
    "@TI.register_fl_task(model='unet_model', data_loader='val_loader', device='device')     \n",
    "def validate(unet_model, val_loader, device):\n",
    "    unet_model.eval()\n",
    "    unet_model.to(device)\n",
    "    \n",
    "    val_loader = tqdm.tqdm(val_loader, desc=\"validate\")\n",
    "\n",
    "    val_score = 0\n",
    "    total_samples = 0\n",
    "\n",
    "    with torch.no_grad():\n",
    "        for data, target in val_loader:\n",
    "            samples = target.shape[0]\n",
    "            total_samples += samples\n",
    "            data, target = torch.tensor(data).to(device), \\\n",
    "                torch.tensor(target).to(device, dtype=torch.int64)\n",
    "            output = unet_model(data)\n",
    "            val = soft_dice_coef(output, target)\n",
    "            val_score += val.sum().cpu().numpy()\n",
    "            \n",
    "    return {'dice_coef': val_score / total_samples,}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b50a39c7",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "derived-bride",
   "metadata": {},
   "source": [
    "## Time to start a federated learning experiment"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "4485ac79",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create a federation\n",
    "from openfl.interface.interactive_api.federation import Federation\n",
    "\n",
    "# 1) Run with API layer - Director mTLS \n",
    "# If the user wants to enable mTLS their must provide CA root chain, and signed key pair to the federation interface\n",
    "cert_chain = 'cert/cert_chain.crt'\n",
    "API_certificate = 'cert/API_certificate.crt'\n",
    "API_private_key = 'cert/API_private.key'\n",
    "\n",
    "# federation = Federation(director_node_fqdn='some.fqdn', disable_tls=False,\n",
    "#                        cert_chain=cert_chain, API_cert=API_certificate, API_private_key=API_private_key)\n",
    "\n",
    "# --------------------------------------------------------------------------------------------------------------------\n",
    "\n",
    "# 2) Run with TLS disabled (trusted environment)\n",
    "# Federation can also determine local fqdn automatically\n",
    "federation = Federation(director_node_fqdn='localhost:50051', disable_tls=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "e35802d5",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[node_info {\n",
       "  name: \"1,2\"\n",
       "}\n",
       "shard_description: \"Kvasir dataset, shard number 1 out of 2\"\n",
       "n_samples: 500\n",
       "sample_shape: \"529\"\n",
       "sample_shape: \"622\"\n",
       "sample_shape: \"3\"\n",
       "target_shape: \"529\"\n",
       "target_shape: \"622\"\n",
       "]"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "shard_registry = federation.dir_client.request_shard_registry()\n",
    "shard_registry"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "67ae50de",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['529', '622']"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "federation.target_shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "920216d3",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# First, request a dummy_shard_desc that holds information about the federated dataset \n",
    "dummy_shard_desc = federation.get_dummy_shard_descriptor(size=10)\n",
    "sample, target = dummy_shard_desc[0]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "insured-metallic",
   "metadata": {},
   "source": [
    "### Prepare data"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "essential-remainder",
   "metadata": {},
   "source": [
    "We ask user to keep all the test data in `data/` folder under the workspace as it will not be sent to collaborators"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "blind-circuit",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import PIL\n",
    "import numpy as np\n",
    "from torch.utils.data import Dataset, DataLoader, SubsetRandomSampler\n",
    "from torchvision import transforms as tsf"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "64f37dcf",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# Now you can implement you data loaders using dummy_shard_desc\n",
    "class KvasirSD(DataInterface, Dataset):\n",
    "\n",
    "    def __init__(self, validation_fraction=1/8, **kwargs):\n",
    "        super().__init__(**kwargs)\n",
    "        \n",
    "        self.validation_fraction = validation_fraction\n",
    "        \n",
    "        # Prepare transforms\n",
    "        self.img_trans = tsf.Compose([\n",
    "            tsf.ToPILImage(),\n",
    "            tsf.Resize((332, 332)),\n",
    "            tsf.ToTensor(),\n",
    "            tsf.Normalize(mean=[0.5, 0.5, 0.5], std=[0.5, 0.5, 0.5])])\n",
    "        self.mask_trans = tsf.Compose([\n",
    "            tsf.ToPILImage(),\n",
    "            tsf.Resize((332, 332), interpolation=PIL.Image.NEAREST),\n",
    "            tsf.ToTensor()])\n",
    "        \n",
    "    @property\n",
    "    def shard_descriptor(self):\n",
    "        return self._shard_descriptor\n",
    "        \n",
    "    @shard_descriptor.setter\n",
    "    def shard_descriptor(self, shard_descriptor):\n",
    "        \"\"\"\n",
    "        Describe per-collaborator procedures or sharding.\n",
    "\n",
    "        This method will be called during a collaborator initialization.\n",
    "        Local shard_descriptor  will be set by Envoy.\n",
    "        \"\"\"\n",
    "        self._shard_descriptor = shard_descriptor\n",
    "        \n",
    "        validation_size = max(1, int(len(self.shard_descriptor) * self.validation_fraction))\n",
    "        \n",
    "        self.train_indeces = np.arange(len(self.shard_descriptor) - validation_size)\n",
    "        self.val_indeces = np.arange(len(self.shard_descriptor) - validation_size, len(self.shard_descriptor))\n",
    "        \n",
    "\n",
    "    def __getitem__(self, index):\n",
    "        img, mask = self.shard_descriptor[index]\n",
    "        img = self.img_trans(img).numpy()\n",
    "        mask = self.mask_trans(mask).numpy()\n",
    "        return img, mask\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.shard_descriptor)\n",
    "    \n",
    "    \n",
    "    def get_train_loader(self, **kwargs):\n",
    "        \"\"\"\n",
    "        Output of this method will be provided to tasks with optimizer in contract\n",
    "        \"\"\"\n",
    "        train_sampler = SubsetRandomSampler(self.train_indeces)\n",
    "        return DataLoader(\n",
    "            self, num_workers=8, batch_size=self.kwargs['train_bs'], sampler=train_sampler\n",
    "            )\n",
    "\n",
    "    def get_valid_loader(self, **kwargs):\n",
    "        \"\"\"\n",
    "        Output of this method will be provided to tasks without optimizer in contract\n",
    "        \"\"\"\n",
    "        val_sampler = SubsetRandomSampler(self.val_indeces)\n",
    "        return DataLoader(self, num_workers=8, batch_size=self.kwargs['valid_bs'], sampler=val_sampler)\n",
    "\n",
    "    def get_train_data_size(self):\n",
    "        \"\"\"\n",
    "        Information for aggregation\n",
    "        \"\"\"\n",
    "        return len(self.train_indeces)\n",
    "\n",
    "    def get_valid_data_size(self):\n",
    "        \"\"\"\n",
    "        Information for aggregation\n",
    "        \"\"\"\n",
    "        return len(self.val_indeces)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "d8df35f5",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/davidyuk/.virtualenvs/openfl/lib/python3.8/site-packages/torchvision/transforms/transforms.py:257: UserWarning: Argument interpolation should be of type InterpolationMode instead of int. Please, use InterpolationMode enum.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([8, 3, 332, 332])\n",
      "torch.Size([1, 3, 332, 332])\n"
     ]
    }
   ],
   "source": [
    "fed_dataset = KvasirSD(train_bs=8, valid_bs=8)\n",
    "fed_dataset.shard_descriptor = dummy_shard_desc\n",
    "for i, (sample, target) in enumerate(fed_dataset.get_train_loader()):\n",
    "    print(sample.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "af0579f8",
   "metadata": {},
   "source": [
    "# Long-Living entities update\n",
    "\n",
    "* We now may have director running on another machine.\n",
    "* We use Federation API to communicate with Director.\n",
    "* Federation object should hold a Director's client (for user service)\n",
    "* Keeping in mind that several API instances may be connacted to one Director.\n",
    "\n",
    "\n",
    "* We do not think for now how we start a Director.\n",
    "* But it knows the data shape and target shape for the DataScience problem in the Federation.\n",
    "* Director holds the list of connected envoys, we do not need to specify it anymore.\n",
    "* Director and Envoys are responsible for encrypting connections, we do not need to worry about certs.\n",
    "\n",
    "\n",
    "* Yet we MUST have a cert to communicate to the Director.\n",
    "* We MUST know the FQDN of a Director.\n",
    "* Director communicates data and target shape to the Federation interface object.\n",
    "\n",
    "\n",
    "* Experiment API may use this info to construct a dummy dataset and a `shard descriptor` stub."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "adad7744",
   "metadata": {},
   "source": [
    "#### Certification of an aggregator\n",
    "* fx workspace certify: creates cert folder and CA as well as cert_chain\n",
    "* fx aggregator generate-cert-request --fqdn `FQDN`: you can pass a specific aggregator FQDN if you want\n",
    "* fx aggregator certify --fqdn `FQDN` --silent: signes aggregators cert\n",
    "<br> After that just pass the paths to required certs to the Federation API\n",
    "\n",
    "#### Certification of a collaborator\n",
    "just follow the usual procedure: <br>\n",
    "fx collaborator generate-cert-request -d {DATA_PATH} -n {COL} \n",
    "\n",
    "fx collaborator certify --request-pkg {COL_DIRECTORY}/{FED_WORKSPACE}/col_{COL}_to_agg_cert_request.zip\n",
    "\n",
    "fx collaborator certify --import {FED_DIRECTORY}/agg_to_col_{COL}_signed_cert.zip"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "12f5663f",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "2"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Interface generator\n",
    "class IFO:\n",
    "    def __init__(self):\n",
    "        self.a = 1\n",
    "    \n",
    "class EXP:\n",
    "    def __init__(self):\n",
    "        self.ifo = IFO()\n",
    "        \n",
    "    def get_ifo(self):\n",
    "        return self.ifo\n",
    "        \n",
    "exp = EXP()\n",
    "ifo = exp.get_ifo()\n",
    "ifo.a = 2\n",
    "\n",
    "# print(exp.ifo.a)\n",
    "exp2 = exp.__class__()\n",
    "exp.ifo.a"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "mature-renewal",
   "metadata": {},
   "outputs": [],
   "source": [
    "# create an experimnet in federation\n",
    "fl_experiment = FLExperiment(federation=federation,)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "lightweight-causing",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "tried to remove tensor: __opt_state_needed not present in the tensor dict\n",
      "tried to remove tensor: __opt_state_needed not present in the tensor dict\n",
      "INFO:openfl.transport.grpc.director_client:SetNewExperiment\n",
      "WARNING:openfl.transport.grpc.server:gRPC is running on insecure channel with TLS disabled.\n",
      "INFO:openfl.transport.grpc.server:Starting Aggregator gRPC Server\n",
      "INFO:openfl.component.aggregator.aggregator:Sending tasks to collaborator 1,2 for round 0\n"
     ]
    }
   ],
   "source": [
    "# If I use autoreload I got a pickling error\n",
    "\n",
    "# The following command zips the workspace and python requirements to be transfered to collaborator nodes\n",
    "fl_experiment.prepare_workspace_distribution(model_provider=MI, \n",
    "                        task_keeper=TI, data_loader=fed_dataset, rounds_to_train=7, opt_treatment='CONTINUE_GLOBAL')\n",
    "\n",
    "\n",
    "# This command starts the aggregator server\n",
    "fl_experiment.start_experiment(model_provider=MI)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8c30b301",
   "metadata": {},
   "source": [
    "## Now we validate the best model!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "55acff59",
   "metadata": {},
   "outputs": [
    {
     "ename": "KeyError",
     "evalue": "'inc.conv.0.weight'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyError\u001b[0m                                  Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-20-a539666cced0>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mbest_model\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mfl_experiment\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mget_best_model\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;32m~/openfl/openfl/interface/interactive_api/experiment.py\u001b[0m in \u001b[0;36mget_best_model\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m     41\u001b[0m         \u001b[0;31m# Next line relies on aggregator inner field where model dicts are stored\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     42\u001b[0m         \u001b[0mbest_tensor_dict\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mserver\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0maggregator\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbest_tensor_dict\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 43\u001b[0;31m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtask_runner_stub\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrebuild_model\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mbest_tensor_dict\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mvalidation\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mTrue\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdevice\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m'cpu'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     44\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtask_runner_stub\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmodel\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     45\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/openfl/openfl/federated/task/task_runner.py\u001b[0m in \u001b[0;36mrebuild_model\u001b[0;34m(self, input_tensor_dict, validation, device)\u001b[0m\n\u001b[1;32m    220\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mopt_treatment\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;34m'RESET'\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    221\u001b[0m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mreset_opt_vars\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 222\u001b[0;31m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mset_tensor_dict\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minput_tensor_dict\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mwith_opt_vars\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mFalse\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdevice\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mdevice\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    223\u001b[0m         elif (self.training_round_completed\n\u001b[1;32m    224\u001b[0m               and self.opt_treatment == 'CONTINUE_GLOBAL' and not validation):\n",
      "\u001b[0;32m~/openfl/openfl/federated/task/task_runner.py\u001b[0m in \u001b[0;36mset_tensor_dict\u001b[0;34m(self, tensor_dict, with_opt_vars, device)\u001b[0m\n\u001b[1;32m    373\u001b[0m         \u001b[0mkwargs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m{\u001b[0m\u001b[0;34m'device'\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mdevice\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m}\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    374\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 375\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mframework_adapter\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mset_tensor_dict\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;32m~/openfl/openfl/plugins/frameworks_adapters/pytorch_adapter.py\u001b[0m in \u001b[0;36mset_tensor_dict\u001b[0;34m(model, tensor_dict, optimizer, device)\u001b[0m\n\u001b[1;32m     43\u001b[0m         \u001b[0;31m# everything\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     44\u001b[0m         \u001b[0;32mfor\u001b[0m \u001b[0mk\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mmodel\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstate_dict\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 45\u001b[0;31m             \u001b[0mnew_state\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mk\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mpt\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfrom_numpy\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtensor_dict\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpop\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mk\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mto\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdevice\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     46\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     47\u001b[0m         \u001b[0;31m# set model state\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mKeyError\u001b[0m: 'inc.conv.0.weight'"
     ]
    }
   ],
   "source": [
    "best_model = fl_experiment.get_best_model()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9fb5718d",
   "metadata": {},
   "outputs": [],
   "source": [
    "fed_dataset._delayed_init()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a2acb7e6",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Validating initial model\n",
    "validate(initial_model, fed_dataset.get_valid_loader(), 'cpu')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c12ca93f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Validating trained model\n",
    "validate(best_model, fed_dataset.get_valid_loader(), 'cpu')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1e6734f6",
   "metadata": {},
   "source": [
    "## We can tune model further!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3940e75e",
   "metadata": {},
   "outputs": [],
   "source": [
    "MI = ModelInterface(model=best_model, optimizer=optimizer_adam, framework_plugin=framework_adapter)\n",
    "fl_experiment.start_experiment(model_provider=MI, task_keeper=TI, data_loader=fed_dataset, rounds_to_train=4, \\\n",
    "                              opt_treatment='CONTINUE_GLOBAL')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1bd786d2",
   "metadata": {},
   "outputs": [],
   "source": [
    "best_model = fl_experiment.get_best_model()\n",
    "# Validating trained model\n",
    "validate(best_model, fed_dataset.get_valid_loader(), 'cpu')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2dcdbfeb",
   "metadata": {},
   "outputs": [],
   "source": [
    "a = (np.zeros((2,4)), np.ones(2,), 2*np.ones(2,))\n",
    "a = [elem for elem in a if elem.shape==2 else elem.newaxis()]\n",
    "np.concatenate(a)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "70863215",
   "metadata": {},
   "outputs": [],
   "source": [
    "class A:\n",
    "    def __init__(self, **kwargs):\n",
    "        print(\"Class A\", kwargs)\n",
    "        \n",
    "class B:\n",
    "    def __init__(self, **kwargs):\n",
    "        super().__init__()\n",
    "        print(\"Class B\", kwargs)\n",
    "        \n",
    "class C(B, A):\n",
    "    def __init__(self, **kwargs):\n",
    "        super().__init__()\n",
    "        print(\"Class C\", kwargs)\n",
    "        \n",
    "# class A:\n",
    "#     def __init__(self, **kwargs):\n",
    "#         print(\"Class A\", kwargs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4ba44919",
   "metadata": {},
   "outputs": [],
   "source": [
    "c = C(x=1, z=5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0637f26f",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
